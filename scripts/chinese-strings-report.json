{
  "generatedAt": "2025-12-25T03:32:10.297Z",
  "totalFiles": 54,
  "totalStrings": 529,
  "files": {
    "src\\index.tsx": ["æœªæ‰¾åˆ° root æŒ‚è½½å…ƒç´ "],
    "src\\types\\settings.ts": ["é€šç”¨", "åŠ¨æ¼«", "ç”µå½±/å‰§é›†", "æ–°é—»", "ç§‘æŠ€"],
    "src\\services\\utils\\url.ts": ["è¯·è¾“å…¥è§†é¢‘é“¾æ¥"],
    "src\\services\\subtitle\\parser.ts": [
      "ç¬‘",
      "ç¬‘ã„",
      "ç¬‘ã„å£°",
      "éŸ³æ¥½",
      "éŸ³æ¥½å†ç”Ÿ",
      "æ‹æ‰‹",
      "å’³",
      "å’³æ‰•ã„",
      "ãŸã‚æ¯",
      "ãƒ†ãƒ¼ãƒæ›²",
      "ç¬‘å£°",
      "æŒå£°",
      "éŸ³ä¹",
      "ç‰‡å°¾æ›²",
      "ç‰‡å¤´æ›²",
      "ä¸»é¢˜æ›²"
    ],
    "src\\services\\glossary\\manager.ts": ["æœ¯è¯­è¡¨æ ¼å¼æ— æ•ˆ", "æœ¯è¯­è¡¨æ–‡ä»¶è§£æå¤±è´¥"],
    "src\\services\\glossary\\autoConfirm.ts": [
      "è‡ªåŠ¨æå–æœ¯è¯­",
      ");\r\n    return { terms: fallbackTerms, newTermsCount: 0, glossaryId: null };\r\n  }\r\n\r\n  \r\n  const currentGlossaries = settings.glossaries || [];\r\n  let targetGlossaryId = overrideTargetId || settings.activeGlossaryId;\r\n  let updatedGlossaries = [...currentGlossaries];\r\n\r\n  if (!targetGlossaryId || !currentGlossaries.find((g) => g.id === targetGlossaryId)) {\r\n    const newGlossary = createGlossary('è‡ªåŠ¨æå–æœ¯è¯­');\r\n    newGlossary.terms = [];\r\n    updatedGlossaries = [...currentGlossaries, newGlossary];\r\n    targetGlossaryId = newGlossary.id;\r\n    logger.info("
    ],
    "src\\services\\generation\\pipeline\\translation.ts": [
      "(\r\n          translatedData.map((t: any) => [String(t.id), t.text_translated as string])\r\n        );\r\n\r\n        return { transMap, batch };\r\n      },\r\n      \r\n      createTranslationPostProcessor(\r\n        ai,\r\n        systemInstruction,\r\n        onStatusUpdate,\r\n        signal,\r\n        onUsage,\r\n        timeoutMs,\r\n        useDiarization\r\n      ),\r\n      { maxRetries: 1, stepName: 'Translation' }\r\n    );\r\n\r\n    return result;\r\n  } catch (e: any) {\r\n    \r\n    logger.error('Translation batch failed', formatGeminiError(e));\r\n    const actionableMsg = getActionableErrorMessage(e);\r\n    const errorMsg = actionableMsg ? `ç¿»è¯‘å¤±è´¥ï¼š${actionableMsg}` : 'ç¿»è¯‘å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸæ–‡ã€‚';\r\n    onStatusUpdate?.({\r\n      toast: {\r\n        message: errorMsg,\r\n        type: 'error',\r\n      },\r\n    });\r\n    return batch.map((item) => ({ ...item, translated: item.original }));\r\n  }\r\n}\r\n\r\n\r\nexport async function translateBatch(\r\n  ai: GoogleGenAI,\r\n  items: any[],\r\n  systemInstruction: string,\r\n  concurrency: number,\r\n  batchSize: number,\r\n  onStatusUpdate?: (update: {\r\n    message?: string;\r\n    toast?: { message: string; type: 'info' | 'warning' | 'error' | 'success' };\r\n  }) => void,\r\n  signal?: AbortSignal,\r\n  onUsage?: (usage: TokenUsage) => void,\r\n  timeoutMs?: number,\r\n  useDiarization: boolean = false\r\n): Promise",
      "ç¿»è¯‘å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸæ–‡ã€‚",
      "ç¿»è¯‘å¤±è´¥ï¼š${actionableMsg}"
    ],
    "src\\services\\generation\\pipeline\\speakerAnalyzer.ts": [
      "æ­£åœ¨åˆ†æè¯´è¯äºº...",
      "è¯´è¯äººé¢„åˆ†æå¤±è´¥",
      "å·²è¯†åˆ« ${profileSet.profiles.length} ä½è¯´è¯äºº"
    ],
    "src\\services\\generation\\pipeline\\preprocessor.ts": [
      "{\r\n  \r\n  onProgress?.({ id: 'decoding', total: 1, status: 'processing', message: 'æ­£åœ¨è§£ç éŸ³é¢‘...' });\r\n\r\n  let audioBuffer: AudioBuffer;\r\n  try {\r\n    if (audioSource instanceof AudioBuffer) {\r\n      audioBuffer = audioSource;\r\n      onProgress?.({\r\n        id: 'decoding',\r\n        total: 1,\r\n        status: 'completed',\r\n        message: `ä½¿ç”¨ç¼“å­˜éŸ³é¢‘ï¼Œæ—¶é•¿: ${formatTime(audioBuffer.duration)}`,\r\n      });\r\n    } else {\r\n      audioBuffer = await decodeAudioWithRetry(audioSource);\r\n      onProgress?.({\r\n        id: 'decoding',\r\n        total: 1,\r\n        status: 'completed',\r\n        message: `è§£ç å®Œæˆï¼Œæ—¶é•¿: ${formatTime(audioBuffer.duration)}`,\r\n      });\r\n    }\r\n  } catch (e) {\r\n    logger.error('Failed to decode audio', e);\r\n    throw new Error('éŸ³é¢‘è§£ç å¤±è´¥ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„è§†é¢‘æˆ–éŸ³é¢‘æ ¼å¼ã€‚');\r\n  }\r\n\r\n  const totalDuration = audioBuffer.duration;\r\n  const chunkDuration = settings.chunkDuration || 300;\r\n  const totalChunks = Math.ceil(totalDuration / chunkDuration);\r\n\r\n  \r\n  const chunksParams: ChunkParams[] = [];\r\n  let vadSegments: { start: number; end: number }[] | undefined;\r\n\r\n  if (settings.useSmartSplit) {\r\n    onProgress?.({ id: 'segmenting', total: 1, status: 'processing', message: 'æ­£åœ¨æ™ºèƒ½åˆ†æ®µ...' });\r\n    const segmenter = new SmartSegmenter();\r\n    const result = await segmenter.segmentAudio(audioBuffer, chunkDuration, signal);\r\n    logger.info('Smart Segmentation Results', {\r\n      count: result.chunks.length,\r\n      chunks: result.chunks,\r\n    });\r\n\r\n    result.chunks.forEach((seg, i) => {\r\n      chunksParams.push({\r\n        index: i + 1,\r\n        start: seg.start,\r\n        end: seg.end,\r\n      });\r\n    });\r\n\r\n    \r\n    vadSegments = result.vadSegments;\r\n    logger.info(`Cached ${vadSegments.length} VAD segments for speaker profile extraction`);\r\n\r\n    onProgress?.({\r\n      id: 'segmenting',\r\n      total: 1,\r\n      status: 'completed',\r\n      message: `æ™ºèƒ½åˆ†æ®µå®Œæˆï¼Œå…± ${result.chunks.length} ä¸ªç‰‡æ®µã€‚`,\r\n    });\r\n  } else {\r\n    \r\n    let cursor = 0;\r\n    for (let i = 0; i",
      "æ­£åœ¨è§£ç éŸ³é¢‘...",
      "éŸ³é¢‘è§£ç å¤±è´¥ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„è§†é¢‘æˆ–éŸ³é¢‘æ ¼å¼ã€‚",
      "æ­£åœ¨æ™ºèƒ½åˆ†æ®µ...",
      "ä½¿ç”¨ç¼“å­˜éŸ³é¢‘ï¼Œæ—¶é•¿: ${formatTime(audioBuffer.duration)}",
      "è§£ç å®Œæˆï¼Œæ—¶é•¿: ${formatTime(audioBuffer.duration)}",
      "æ™ºèƒ½åˆ†æ®µå®Œæˆï¼Œå…± ${result.chunks.length} ä¸ªç‰‡æ®µã€‚"
    ],
    "src\\services\\generation\\pipeline\\postProcessors.ts": [
      "é‡è¯• ${missingItems.length} æ¡æ¼ç¿»..."
    ],
    "src\\services\\generation\\pipeline\\index.ts": [
      "=> {\r\n  const geminiKey = ENV.GEMINI_API_KEY || settings.geminiKey?.trim();\r\n  const openaiKey = ENV.OPENAI_API_KEY || settings.openaiKey?.trim();\r\n\r\n  if (!geminiKey) throw new Error('ç¼ºå°‘ Gemini API å¯†é’¥ã€‚');\r\n  if (!openaiKey && !settings.useLocalWhisper) throw new Error('ç¼ºå°‘ OpenAI API å¯†é’¥ã€‚');\r\n\r\n  const ai = new GoogleGenAI({\r\n    apiKey: geminiKey,\r\n    httpOptions: {\r\n      ...(settings.geminiEndpoint ? { baseUrl: settings.geminiEndpoint } : {}),\r\n      timeout: (settings.requestTimeout || 600) * 1000, \r\n    },\r\n  });\r\n\r\n  \r\n  const usageReporter = new UsageReporter();\r\n  const trackUsage = usageReporter.getTracker();\r\n  const isDebug = window.electronAPI?.isDebug ?? false;\r\n\r\n  const context: PipelineContext = {\r\n    ai,\r\n    settings,\r\n    signal,\r\n    trackUsage,\r\n    onProgress,\r\n    isDebug,\r\n    geminiKey,\r\n    openaiKey,\r\n  };\r\n\r\n  \r\n  const { audioBuffer, chunksParams, vadSegments, chunkDuration } = await preprocessAudio(\r\n    audioSource,\r\n    settings,\r\n    onProgress,\r\n    signal\r\n  );\r\n  const totalChunks = chunksParams.length;\r\n\r\n  \r\n  const whisperChunksMap = new Map",
      "| null = null;\r\n  let glossaryChunks: { index: number; start: number; end: number }[] | undefined;\r\n\r\n  if (isDebug && settings.debug?.mockGemini) {\r\n    logger.info('âš ï¸ [MOCK] Glossary Extraction ENABLED. Using MockFactory.');\r\n    glossaryPromise = MockFactory.getMockGlossary(0);\r\n  } else if (settings.enableAutoGlossary !== false) {\r\n    const sampleMinutes = settings.glossarySampleMinutes || 'all';\r\n    glossaryChunks = selectChunksByDuration(chunksParams, sampleMinutes, chunkDuration);\r\n\r\n    logger.info(\r\n      `Initiating parallel glossary extraction on ${glossaryChunks.length} chunks (Limit: ${sampleMinutes} min)`\r\n    );\r\n\r\n    \r\n    const glossaryConcurrency = settings.concurrencyPro || 2;\r\n\r\n    onProgress?.({\r\n      id: 'glossary',\r\n      total: glossaryChunks.length,\r\n      status: 'processing',\r\n      message: `æ­£åœ¨æå–æœ¯è¯­ (0/${glossaryChunks.length})...`,\r\n    });\r\n\r\n    glossaryPromise = extractGlossaryFromAudio(\r\n      ai,\r\n      audioBuffer,\r\n      glossaryChunks,\r\n      settings.genre,\r\n      glossaryConcurrency,\r\n      (completed, total) => {\r\n        onProgress?.({\r\n          id: 'glossary',\r\n          total: total,\r\n          status: completed === total ? 'completed' : 'processing',\r\n          message:\r\n            completed === total ? 'æœ¯è¯­æå–å®Œæˆã€‚' : `æ­£åœ¨æå–æœ¯è¯­ (${completed}/${total})...`,\r\n        });\r\n      },\r\n      signal,\r\n      trackUsage,\r\n      (settings.requestTimeout || 600) * 1000 \r\n    );\r\n  }\r\n\r\n  \r\n  const glossaryTask = GlossaryHandler.handle(\r\n    context,\r\n    glossaryPromise,\r\n    glossaryChunks,\r\n    onGlossaryReady\r\n  );\r\n\r\n  \r\n  const glossaryState = new GlossaryState(glossaryTask.then((r) => r.glossary));\r\n  logger.info('ğŸ”„ GlossaryState created - chunks can now access glossary independently');\r\n\r\n  \r\n  let speakerProfilePromise: Promise",
      "ç¼ºå°‘ Gemini API å¯†é’¥ã€‚",
      "ç¼ºå°‘ OpenAI API å¯†é’¥ã€‚",
      "æ­£åœ¨åˆ†æè¯´è¯äºº...",
      "æ­£åœ¨æå–æœ¯è¯­ (0/${glossaryChunks.length})...",
      "æ­£åœ¨æå–æœ¯è¯­ (${completed}/${total})..."
    ],
    "src\\services\\generation\\pipeline\\glossaryHandler.ts": [
      "æ­£åœ¨æå–æœ¯è¯­...",
      "ç­‰å¾…ç”¨æˆ·ç¡®è®¤...",
      "æœ¯è¯­è¡¨å·²åº”ç”¨ã€‚",
      "æœªå‘ç°æœ¯è¯­ã€‚",
      "å·²å–æ¶ˆ",
      "æœ¯è¯­æå–å¤±è´¥"
    ],
    "src\\services\\generation\\pipeline\\chunkProcessor.ts": [
      "{\r\n    const { index, start, end } = chunk;\r\n    const { ai, settings, signal, trackUsage, onProgress, isDebug, openaiKey } = context;\r\n    const {\r\n      glossaryState,\r\n      speakerProfilePromise,\r\n      transcriptionSemaphore,\r\n      refinementSemaphore,\r\n      audioBuffer,\r\n      chunkDuration,\r\n      totalChunks,\r\n    } = deps;\r\n\r\n    try {\r\n      \r\n      onProgress?.({\r\n        id: index,\r\n        total: totalChunks,\r\n        status: 'processing',\r\n        stage: 'transcribing',\r\n        message: 'ç­‰å¾…è½¬å½•...',\r\n      });\r\n\r\n      let rawSegments: SubtitleItem[] = [];\r\n\r\n      \r\n      await transcriptionSemaphore.acquire();\r\n      try {\r\n        if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n\r\n        onProgress?.({\r\n          id: index,\r\n          total: totalChunks,\r\n          status: 'processing',\r\n          stage: 'transcribing',\r\n          message: 'æ­£åœ¨è½¬å½•...',\r\n        });\r\n        logger.debug(`[Chunk ${index}] Starting transcription...`);\r\n\r\n        const shouldMockTranscription =\r\n          isDebug &&\r\n          (settings.useLocalWhisper\r\n            ? settings.debug?.mockLocalWhisper\r\n            : settings.debug?.mockOpenAI);\r\n\r\n        if (shouldMockTranscription) {\r\n          rawSegments = await MockFactory.getMockTranscription(index, start, end);\r\n        } else {\r\n          const wavBlob = await sliceAudioBuffer(audioBuffer, start, end);\r\n          rawSegments = await transcribeAudio(\r\n            wavBlob,\r\n            openaiKey,\r\n            settings.transcriptionModel,\r\n            settings.openaiEndpoint,\r\n            (settings.requestTimeout || 600) * 1000,\r\n            settings.useLocalWhisper,\r\n            settings.whisperModelPath,\r\n            settings.whisperThreads,\r\n            signal,\r\n            settings.debug?.whisperPath\r\n          );\r\n        }\r\n      } finally {\r\n        transcriptionSemaphore.release();\r\n      }\r\n\r\n      logger.debug(`[Chunk ${index}] Transcription complete. Segments: ${rawSegments.length}`);\r\n\r\n      \r\n      rawSegments = rawSegments\r\n        .map((seg) => ({\r\n          ...seg,\r\n          original: cleanNonSpeechAnnotations(seg.original),\r\n        }))\r\n        .filter((seg) => seg.original.length > 0);\r\n\r\n      ArtifactSaver.saveChunkArtifact(index, 'whisper', rawSegments, settings);\r\n\r\n      \r\n      if (rawSegments.length === 0) {\r\n        logger.warn(`[Chunk ${index}] No speech detected, skipping`);\r\n        onProgress?.({\r\n          id: index,\r\n          total: totalChunks,\r\n          status: 'completed',\r\n          message: 'å®Œæˆï¼ˆæ— å†…å®¹ï¼‰',\r\n        });\r\n        return { whisper: [], refined: [], translated: [], final: [] };\r\n      }\r\n\r\n      \r\n      onProgress?.({\r\n        id: index,\r\n        total: totalChunks,\r\n        status: 'processing',\r\n        stage: 'waiting_glossary',\r\n        message: 'ç­‰å¾…æœ¯è¯­è¡¨...',\r\n      });\r\n\r\n      if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n\r\n      logger.debug(`[Chunk ${index}] Waiting for glossary confirmation...`);\r\n      const finalGlossary = await glossaryState.get();\r\n\r\n      if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n\r\n      const chunkSettings = { ...settings, glossary: finalGlossary };\r\n\r\n      logger.debug(\r\n        `[Chunk ${index}] Glossary ready (${finalGlossary.length} terms), proceeding to refinement`\r\n      );\r\n\r\n      \r\n      let speakerProfiles: SpeakerProfile[] | undefined;\r\n      if (speakerProfilePromise !== null) {\r\n        onProgress?.({\r\n          id: index,\r\n          total: totalChunks,\r\n          status: 'processing',\r\n          stage: 'waiting_speakers',\r\n          message: 'ç­‰å¾…è¯´è¯äººé¢„åˆ†æ...',\r\n        });\r\n        try {\r\n          if (signal) {\r\n            speakerProfiles = await Promise.race([\r\n              speakerProfilePromise,\r\n              new Promise",
      "ç­‰å¾…è½¬å½•...",
      "æ“ä½œå·²å–æ¶ˆ",
      "æ­£åœ¨è½¬å½•...",
      "å®Œæˆï¼ˆæ— å†…å®¹ï¼‰",
      "ç­‰å¾…æœ¯è¯­è¡¨...",
      "ç­‰å¾…è¯´è¯äººé¢„åˆ†æ...",
      "æ­£åœ¨æ ¡å¯¹æ—¶é—´è½´...",
      "æ­£åœ¨ç¿»è¯‘...",
      "å®Œæˆ",
      "ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡",
      "å¤±è´¥",
      ");\r\n        onProgress?.({\r\n          id: index,\r\n          total: totalChunks,\r\n          status: 'completed',\r\n          message: 'å®Œæˆï¼ˆæ— å†…å®¹ï¼‰',\r\n        });\r\n        return { whisper: [], refined: [], translated: [], final: [] };\r\n      }\r\n\r\n      \r\n      onProgress?.({\r\n        id: index,\r\n        total: totalChunks,\r\n        status: 'processing',\r\n        stage: 'waiting_glossary',\r\n        message: 'ç­‰å¾…æœ¯è¯­è¡¨...',\r\n      });\r\n\r\n      if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n\r\n      logger.debug(",
      ");\r\n      const finalGlossary = await glossaryState.get();\r\n\r\n      if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n\r\n      const chunkSettings = { ...settings, glossary: finalGlossary };\r\n\r\n      logger.debug(",
      ");\r\n\r\n      \r\n      let speakerProfiles: SpeakerProfile[] | undefined;\r\n      if (speakerProfilePromise !== null) {\r\n        onProgress?.({\r\n          id: index,\r\n          total: totalChunks,\r\n          status: 'processing',\r\n          stage: 'waiting_speakers',\r\n          message: 'ç­‰å¾…è¯´è¯äººé¢„åˆ†æ...',\r\n        });\r\n        try {\r\n          if (signal) {\r\n            speakerProfiles = await Promise.race([\r\n              speakerProfilePromise,\r\n              new Promise<never>((_, reject) => {\r\n                if (signal.aborted) reject(new Error('Operation cancelled'));\r\n                else\r\n                  signal.addEventListener('abort', () => reject(new Error('Operation cancelled')));\r\n              }),\r\n            ]);\r\n          } else {\r\n            speakerProfiles = await speakerProfilePromise;\r\n          }\r\n        } catch (e) {\r\n          if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n          logger.warn('Failed to get speaker profiles, proceeding without them', e);\r\n        }\r\n      }\r\n\r\n      \r\n      await refinementSemaphore.acquire();\r\n      let refinedSegments: SubtitleItem[] = [];\r\n      let finalChunkSubs: SubtitleItem[] = [];\r\n\r\n      try {\r\n        if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n\r\n        const refineWavBlob = await sliceAudioBuffer(audioBuffer, start, end);\r\n        const base64Audio = await blobToBase64(refineWavBlob);\r\n\r\n        onProgress?.({\r\n          id: index,\r\n          total: totalChunks,\r\n          status: 'processing',\r\n          stage: 'refining',\r\n          message: 'æ­£åœ¨æ ¡å¯¹æ—¶é—´è½´...',\r\n        });\r\n\r\n        const refineSystemInstruction = getSystemInstructionWithDiarization(\r\n          chunkSettings.genre,\r\n          undefined,\r\n          'refinement',\r\n          chunkSettings.glossary,\r\n          chunkSettings.enableDiarization,\r\n          speakerProfiles,\r\n          chunkSettings.minSpeakers,\r\n          chunkSettings.maxSpeakers\r\n        );\r\n\r\n        const glossaryInfo =\r\n          chunkSettings.glossary && chunkSettings.glossary.length > 0\r\n            ?",
      "åˆ†æ®µ ${index} æ—¶é—´è½´å¤±è´¥ï¼Œå°†å›é€€åˆ°åŸå§‹ç»“æœã€‚",
      ");\r\n              }\r\n              finalChunkSubs = items.map((item) => ({\r\n                id: item.id,\r\n                startTime: formatTime(timeToSeconds(item.start) + start),\r\n                endTime: formatTime(timeToSeconds(item.end) + start),\r\n                original: item.original,\r\n                translated: item.translated,\r\n                ...(chunkSettings.enableDiarization && item.speaker\r\n                  ? { speaker: item.speaker }\r\n                  : {}),\r\n              }));\r\n            }\r\n\r\n            ArtifactSaver.saveChunkArtifact(index, 'translation', finalChunkSubs, settings);\r\n\r\n            onProgress?.({ id: index, total: totalChunks, status: 'completed', message: 'å®Œæˆ' });\r\n          } catch (e: any) {\r\n            logger.error(",
      ", formatGeminiError(e));\r\n            onProgress?.({\r\n              id: index,\r\n              total: totalChunks,\r\n              status: 'processing', \r\n              message: 'ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡',\r\n            });\r\n            \r\n            \r\n          }\r\n        }\r\n      } finally {\r\n        refinementSemaphore.release();\r\n      }\r\n\r\n      \r\n      const refinedGlobal = refinedSegments.map((seg) => ({\r\n        ...seg,\r\n        startTime: formatTime(timeToSeconds(seg.startTime) + start),\r\n        endTime: formatTime(timeToSeconds(seg.endTime) + start),\r\n      }));\r\n\r\n      return {\r\n        whisper: rawSegments.map((seg) => ({\r\n          ...seg,\r\n          startTime: formatTime(timeToSeconds(seg.startTime) + start),\r\n          endTime: formatTime(timeToSeconds(seg.endTime) + start),\r\n        })),\r\n        refined: refinedGlobal,\r\n        translated: finalChunkSubs, \r\n        final:\r\n          finalChunkSubs.length > 0\r\n            ? finalChunkSubs\r\n            : refinedGlobal.length > 0\r\n              ? refinedGlobal\r\n              : [],\r\n      };\r\n    } catch (e: any) {\r\n      logger.error("
    ],
    "src\\services\\generation\\extractors\\glossary.ts": [
      "setTimeout(r, delay));\r\n        return extractSingleChunk(chunk, attemptNumber + 1);\r\n      } else {\r\n        \r\n        \r\n        const actionableMsg = getActionableErrorMessage(e);\r\n        const reason = isRetryable ? `åœ¨ ${attemptNumber} æ¬¡å°è¯•å` : '(ä¸å¯é‡è¯•çš„é”™è¯¯)';\r\n        logger.error(`[åˆ†æ®µ ${index}] æå–å¤±è´¥ ${reason}`, { error: e.message, status: e.status });\r\n\r\n        \r\n        if (actionableMsg) {\r\n          const enhancedError = new Error(actionableMsg);\r\n          (enhancedError as any).status = e.status;\r\n          (enhancedError as any).originalError = e;\r\n          throw enhancedError;\r\n        }\r\n        throw e;\r\n      }\r\n    }\r\n  };\r\n\r\n  \r\n  const results = await mapInParallel(\r\n    chunks,\r\n    concurrency,\r\n    async (chunk) => {\r\n      try {\r\n        const result = await extractSingleChunk(chunk, 1);\r\n        completed++;\r\n        onProgress?.(completed, chunks.length);\r\n        return result;\r\n      } catch (e) {\r\n        \r\n        failedChunks.push(chunk);\r\n        \r\n        \r\n        \r\n\r\n        return {\r\n          terms: [],\r\n          source: 'chunk',\r\n          chunkIndex: chunk.index,\r\n          confidence: 'low',\r\n        } as GlossaryExtractionResult;\r\n      }\r\n    },\r\n    signal\r\n  );\r\n\r\n  \r\n  if (failedChunks.length > 0) {\r\n    if (signal?.aborted) {\r\n      logger.info('Glossary extraction cancelled before retry pass');\r\n      \r\n      return results;\r\n    }\r\n\r\n    logger.warn(\r\n      `First pass complete. ${failedChunks.length}/${chunks.length} chunks failed. Starting aggregated retry pass...`\r\n    );\r\n\r\n    \r\n    const retryConcurrency = Math.max(1, Math.floor(concurrency / 2));\r\n\r\n    await mapInParallel(\r\n      failedChunks,\r\n      retryConcurrency,\r\n      async (failedChunk) => {\r\n        try {\r\n          logger.info(`[Chunk ${failedChunk.index}] Retry attempt (aggregated pass)`);\r\n          const result = await extractSingleChunk(failedChunk, 1);\r\n\r\n          \r\n          const resultIndex = results.findIndex((r) => r.chunkIndex === failedChunk.index);\r\n          if (resultIndex !== -1) {\r\n            results[resultIndex] = result;\r\n          }\r\n          logger.info(`[Chunk ${failedChunk.index}] Aggregated retry succeeded!`);\r\n        } catch (e: any) {\r\n          logger.error(`[åˆ†å— ${failedChunk.index}] èšåˆé‡è¯•å¤±è´¥`, {\r\n            error: e.message,\r\n            status: e.status,\r\n          });\r\n        } finally {\r\n          \r\n          completed++;\r\n          onProgress?.(completed, chunks.length);\r\n        }\r\n      },\r\n      signal\r\n    );\r\n  }\r\n\r\n  \r\n  const successCount = results.filter((r) => r.confidence === 'high').length;\r\n  const failCount = results.filter((r) => r.confidence === 'low' && r.terms.length === 0).length;\r\n  const totalTerms = results.reduce((sum, r) => sum + r.terms.length, 0);\r\n\r\n  logger.info(\r\n    `Glossary extraction complete. Success: ${successCount}/${chunks.length}, Failed: ${failCount}/${chunks.length}, Total terms: ${totalTerms}`\r\n  );\r\n\r\n  return results;\r\n};\r\n\r\nexport const retryGlossaryExtraction = async (\r\n  apiKey: string,\r\n  audioBuffer: AudioBuffer,\r\n  chunks: { index: number; start: number; end: number }[],\r\n  genre: string,\r\n  concurrency: number,\r\n  endpoint?: string,\r\n  timeout?: number\r\n): Promise",
      "(ä¸å¯é‡è¯•çš„é”™è¯¯)",
      "åœ¨ ${attemptNumber} æ¬¡å°è¯•å",
      "[åˆ†æ®µ ${index}] æå–å¤±è´¥ ${reason}",
      "[åˆ†å— ${failedChunk.index}] èšåˆé‡è¯•å¤±è´¥"
    ],
    "src\\services\\generation\\debug\\mockFactory.ts": [
      "{\r\n    const mockGlossary = [\r\n      {\r\n        chunkIndex,\r\n        terms: [\r\n          {\r\n            term: 'Mock Term',\r\n            translation: 'æ¨¡æ‹Ÿæœ¯è¯­',\r\n            notes: 'Mock notes for validation',\r\n          } as any,\r\n        ],\r\n        confidence: 'high' as const,\r\n        source: 'chunk' as const,\r\n      },\r\n    ];\r\n    logger.info('âš ï¸ [MOCK] Glossary Extraction ENABLED. Returning mock data:', mockGlossary);\r\n    return mockGlossary;\r\n  }\r\n\r\n  static async getMockSpeakerProfiles(): Promise",
      "æ¨¡æ‹Ÿæœ¯è¯­"
    ],
    "src\\services\\generation\\batch\\operations.ts": [
      "=> {\r\n  const geminiKey = ENV.GEMINI_API_KEY || settings.geminiKey?.trim();\r\n  if (!geminiKey) throw new Error('ç¼ºå°‘ API å¯†é’¥ã€‚');\r\n  const ai = new GoogleGenAI({\r\n    apiKey: geminiKey,\r\n    httpOptions: {\r\n      ...(settings.geminiEndpoint ? { baseUrl: settings.geminiEndpoint } : {}),\r\n      timeout: (settings.requestTimeout || 600) * 1000,\r\n    },\r\n  });\r\n\r\n  let audioBuffer: AudioBuffer | null = null;\r\n  \r\n  if (file) {\r\n    onProgress?.({ id: 'init', total: 0, status: 'processing', message: 'åŠ è½½éŸ³é¢‘ä¸­...' });\r\n    try {\r\n      audioBuffer = await decodeAudio(file);\r\n    } catch (e) {\r\n      logger.warn('Audio decode failed, proceeding with text-only mode.', e);\r\n    }\r\n  } else {\r\n    \r\n    logger.info('No media file provided, running in text-only context.');\r\n  }\r\n\r\n  const systemInstruction = getSystemInstructionWithDiarization(\r\n    settings.genre,\r\n    mode === 'proofread' ? settings.customProofreadingPrompt : settings.customTranslationPrompt,\r\n    mode,\r\n    getActiveGlossaryTerms(settings),\r\n    settings.enableDiarization, \r\n    speakerProfiles,\r\n    settings.minSpeakers,\r\n    settings.maxSpeakers\r\n  );\r\n\r\n  const currentSubtitles = [...allSubtitles];\r\n  const chunks: SubtitleItem[][] = [];\r\n  const batchSize = settings.proofreadBatchSize || PROOFREAD_BATCH_SIZE;\r\n  for (let i = 0; i",
      "0) {\r\n            const rangeLabel = `[IDs ${batch[0].id}-${batch[batch.length - 1].id}]`;\r\n            mergedComment += (mergedComment ? ' | ' : '') + `${rangeLabel}: ${batchComments[idx]}`;\r\n          }\r\n        }\r\n      });\r\n\r\n      \r\n      let lastEndTime = '00:00:00,000';\r\n      if (firstBatchIdx > 0) {\r\n        const prevChunk = chunks[firstBatchIdx - 1];\r\n        if (prevChunk.length > 0) {\r\n          lastEndTime = prevChunk[prevChunk.length - 1].endTime;\r\n        }\r\n      }\r\n\r\n      let actionLabel = '';\r\n      if (mode === 'proofread') actionLabel = 'æ¶¦è‰²ä¸­';\r\n      else if (mode === 'fix_timestamps') actionLabel = 'æ ¡å¯¹æ—¶é—´è½´ä¸­';\r\n      else actionLabel = 'ç¿»è¯‘ä¸­';\r\n\r\n      const groupLabel =\r\n        group.length > 1\r\n          ? `${group[0] + 1}-${group[group.length - 1] + 1}`\r\n          : `${firstBatchIdx + 1}`;\r\n      onProgress?.({\r\n        id: groupLabel,\r\n        total: groups.length,\r\n        status: 'processing',\r\n        message: `${actionLabel}...`,\r\n      });\r\n      logger.debug(\r\n        `[Batch ${groupLabel}] Starting ${mode} operation. Merged items: ${mergedBatch.length}`\r\n      );\r\n\r\n      try {\r\n        const processed = await processBatch(\r\n          ai,\r\n          mergedBatch,\r\n          audioBuffer,\r\n          lastEndTime,\r\n          settings,\r\n          systemInstruction,\r\n          groupLabel,\r\n          audioBuffer?.duration,\r\n          mode,\r\n          mergedComment,\r\n          signal,\r\n          trackUsage\r\n        );\r\n\r\n        \r\n        \r\n        \r\n\r\n        \r\n        const firstOriginalId = mergedBatch[0]?.id;\r\n        const lastOriginalId = mergedBatch[mergedBatch.length - 1]?.id;\r\n\r\n        if (firstOriginalId && lastOriginalId) {\r\n          const startIdx = currentSubtitles.findIndex((s) => s.id === firstOriginalId);\r\n          const endIdx = currentSubtitles.findIndex((s) => s.id === lastOriginalId);\r\n\r\n          if (startIdx !== -1 && endIdx !== -1 && startIdx",
      "ç¼ºå°‘ API å¯†é’¥ã€‚",
      "åŠ è½½éŸ³é¢‘ä¸­...",
      "æ¶¦è‰²ä¸­",
      "æ ¡å¯¹æ—¶é—´è½´ä¸­",
      "ç¿»è¯‘ä¸­",
      "å®Œæˆ",
      "å¤±è´¥",
      "è‡ªåŠ¨ç¿»è¯‘å®Œæˆ",
      "è‡ªåŠ¨ç¿»è¯‘å¤±è´¥",
      ", e);\r\n  }\r\n  \r\n  return batch;\r\n}\r\n\r\nimport { ENV } from '@/config';\r\n\r\nexport const runBatchOperation = async (\r\n  file: File | null,\r\n  allSubtitles: SubtitleItem[],\r\n  batchIndices: number[], \r\n  settings: AppSettings,\r\n  mode: BatchOperationMode,\r\n  batchComments: Record<string, string> = {}, \r\n  onProgress?: (update: ChunkStatus) => void,\r\n  signal?: AbortSignal,\r\n  speakerProfiles?: SpeakerProfile[]\r\n): Promise<SubtitleItem[]> => {\r\n  const geminiKey = ENV.GEMINI_API_KEY || settings.geminiKey?.trim();\r\n  if (!geminiKey) throw new Error('ç¼ºå°‘ API å¯†é’¥ã€‚');\r\n  const ai = new GoogleGenAI({\r\n    apiKey: geminiKey,\r\n    httpOptions: {\r\n      ...(settings.geminiEndpoint ? { baseUrl: settings.geminiEndpoint } : {}),\r\n      timeout: (settings.requestTimeout || 600) * 1000,\r\n    },\r\n  });\r\n\r\n  let audioBuffer: AudioBuffer | null = null;\r\n  \r\n  if (file) {\r\n    onProgress?.({ id: 'init', total: 0, status: 'processing', message: 'åŠ è½½éŸ³é¢‘ä¸­...' });\r\n    try {\r\n      audioBuffer = await decodeAudio(file);\r\n    } catch (e) {\r\n      logger.warn('Audio decode failed, proceeding with text-only mode.', e);\r\n    }\r\n  } else {\r\n    \r\n    logger.info('No media file provided, running in text-only context.');\r\n  }\r\n\r\n  const systemInstruction = getSystemInstructionWithDiarization(\r\n    settings.genre,\r\n    mode === 'proofread' ? settings.customProofreadingPrompt : settings.customTranslationPrompt,\r\n    mode,\r\n    getActiveGlossaryTerms(settings),\r\n    settings.enableDiarization, \r\n    speakerProfiles,\r\n    settings.minSpeakers,\r\n    settings.maxSpeakers\r\n  );\r\n\r\n  const currentSubtitles = [...allSubtitles];\r\n  const chunks: SubtitleItem[][] = [];\r\n  const batchSize = settings.proofreadBatchSize || PROOFREAD_BATCH_SIZE;\r\n  for (let i = 0; i < currentSubtitles.length; i += batchSize) {\r\n    chunks.push(currentSubtitles.slice(i, i + batchSize));\r\n  }\r\n\r\n  const sortedIndices = [...batchIndices].sort((a, b) => a - b);\r\n\r\n  \r\n  const groups: number[][] = [];\r\n\r\n  \r\n  \r\n  const isSelectAll = sortedIndices.length === chunks.length;\r\n\r\n  if (sortedIndices.length > 0) {\r\n    if (isSelectAll) {\r\n      \r\n      sortedIndices.forEach((idx) => groups.push([idx]));\r\n    } else {\r\n      \r\n      let currentGroup = [sortedIndices[0]];\r\n      for (let i = 1; i < sortedIndices.length; i++) {\r\n        if (sortedIndices[i] === sortedIndices[i - 1] + 1) {\r\n          currentGroup.push(sortedIndices[i]);\r\n        } else {\r\n          groups.push(currentGroup);\r\n          currentGroup = [sortedIndices[i]];\r\n        }\r\n      }\r\n      groups.push(currentGroup);\r\n    }\r\n  }\r\n\r\n  \r\n  \r\n  \r\n  const concurrency =\r\n    mode === 'proofread' ? settings.concurrencyPro || 2 : settings.concurrencyFlash || 5;\r\n\r\n  \r\n  const usageReporter = new UsageReporter();\r\n  const trackUsage = usageReporter.getTracker();\r\n\r\n  await mapInParallel(\r\n    groups,\r\n    concurrency,\r\n    async (group, i) => {\r\n      const firstBatchIdx = group[0];\r\n\r\n      \r\n      let mergedBatch: SubtitleItem[] = [];\r\n      let mergedComment = '';\r\n\r\n      group.forEach((idx) => {\r\n        if (idx < chunks.length) {\r\n          const batch = chunks[idx];\r\n          mergedBatch = [...mergedBatch, ...batch];\r\n\r\n          if (batchComments[idx] && batch.length > 0) {\r\n            const rangeLabel =",
      ";\r\n          }\r\n        }\r\n      });\r\n\r\n      \r\n      let lastEndTime = '00:00:00,000';\r\n      if (firstBatchIdx > 0) {\r\n        const prevChunk = chunks[firstBatchIdx - 1];\r\n        if (prevChunk.length > 0) {\r\n          lastEndTime = prevChunk[prevChunk.length - 1].endTime;\r\n        }\r\n      }\r\n\r\n      let actionLabel = '';\r\n      if (mode === 'proofread') actionLabel = 'æ¶¦è‰²ä¸­';\r\n      else if (mode === 'fix_timestamps') actionLabel = 'æ ¡å¯¹æ—¶é—´è½´ä¸­';\r\n      else actionLabel = 'ç¿»è¯‘ä¸­';\r\n\r\n      const groupLabel =\r\n        group.length > 1\r\n          ?",
      ");\r\n          }\r\n        }\r\n\r\n        onProgress?.({\r\n          id: groupLabel,\r\n          total: groups.length,\r\n          status: 'completed',\r\n          message: 'å®Œæˆ',\r\n        });\r\n      } catch (e) {\r\n        logger.error(",
      ", e);\r\n        onProgress?.({ id: groupLabel, total: groups.length, status: 'error', message: 'å¤±è´¥' });\r\n        throw e; \r\n      }\r\n    },\r\n    signal\r\n  );\r\n\r\n  \r\n  if (mode === 'fix_timestamps') {\r\n    const emptyTranslationItems = currentSubtitles.filter(\r\n      (s) => !s.translated || s.translated.trim() === ''\r\n    );\r\n\r\n    if (emptyTranslationItems.length > 0) {\r\n      logger.info(",
      "æ­£åœ¨ç¿»è¯‘ ${emptyTranslationItems.length} æ¡æ–°å¢å­—å¹•..."
    ],
    "src\\services\\audio\\segmenter.ts": [
      "{\r\n    logger.debug('analyzeAudio called', {\r\n      duration: audioBuffer.duration,\r\n      sampleRate: audioBuffer.sampleRate,\r\n      numberOfChannels: audioBuffer.numberOfChannels,\r\n      options,\r\n    });\r\n\r\n    \r\n    const audioData = audioBuffer.getChannelData(0);\r\n\r\n    try {\r\n      logger.debug('Initializing VAD Worker...');\r\n      const startTime = performance.now();\r\n\r\n      if (!this.worker) {\r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n\r\n        this.worker = new Worker(new URL('../../workers/vad.worker.ts', import.meta.url), {\r\n          type: 'classic', \r\n        });\r\n\r\n        this.workerReadyPromise = new Promise((resolve, reject) => {\r\n          if (!this.worker) return reject('Worker not created');\r\n\r\n          const handleInitMessage = (e: MessageEvent) => {\r\n            if (e.data.type === 'ready') {\r\n              this.worker?.removeEventListener('message', handleInitMessage);\r\n              resolve();\r\n            } else if (e.data.type === 'error') {\r\n              this.worker?.removeEventListener('message', handleInitMessage);\r\n              const errorDetails = {\r\n                message: e.data.message,\r\n                stack: e.data.stack,\r\n                details: e.data.details,\r\n              };\r\n              logger.error('VAD Worker initialization error details:', errorDetails);\r\n              reject(new Error(e.data.message));\r\n            }\r\n          };\r\n          this.worker.addEventListener('message', handleInitMessage);\r\n\r\n          \r\n          \r\n          let baseUrl: string;\r\n\r\n          \r\n          if (window.electronAPI) {\r\n            \r\n            baseUrl = window.location.href.split('?')[0].split('#')[0];\r\n            if (!baseUrl.endsWith('/')) {\r\n              baseUrl = baseUrl.substring(0, baseUrl.lastIndexOf('/') + 1);\r\n            }\r\n\r\n            \r\n            \r\n            \r\n            if (baseUrl.includes('app.asar')) {\r\n              baseUrl = baseUrl.replace('app.asar', 'app.asar.unpacked');\r\n              logger.debug('Adjusted base URL for unpacked ASAR resources:', baseUrl);\r\n            }\r\n\r\n            logger.debug('Electron environment detected, using base URL:', baseUrl);\r\n          } else {\r\n            \r\n            baseUrl = window.location.href.substring(0, window.location.href.lastIndexOf('/') + 1);\r\n            logger.debug('Web environment detected, using base URL:', baseUrl);\r\n          }\r\n\r\n          this.worker.postMessage({\r\n            command: 'init',\r\n            base: baseUrl,\r\n            options: {\r\n              positiveSpeechThreshold: 0.6,\r\n              negativeSpeechThreshold: 0.4,\r\n              minSpeechFrames: 4,\r\n              redemptionFrames: 8, \r\n              preSpeechPadFrames: 1,\r\n              \r\n              \r\n              modelURL: new URL('silero_vad_legacy.onnx', baseUrl).href,\r\n              workletURL: new URL('vad.worklet.bundle.min.js', baseUrl).href,\r\n            },\r\n          });\r\n        });\r\n      }\r\n\r\n      await this.workerReadyPromise;\r\n      const initTime = performance.now() - startTime;\r\n      logger.debug(`VAD Worker initialized in ${initTime.toFixed(2)}ms`);\r\n\r\n      \r\n      return new Promise((resolve, reject) => {\r\n        if (!this.worker) return reject('Worker not available');\r\n        if (options.signal?.aborted) return reject(new Error('æ“ä½œå·²å–æ¶ˆ'));\r\n\r\n        const handleProcessMessage = (e: MessageEvent) => {\r\n          const msg = e.data;\r\n          if (msg.type === 'result') {\r\n            cleanup();\r\n            const runTime = performance.now() - startTime - initTime;\r\n            logger.debug(\r\n              `VAD execution complete in ${runTime.toFixed(2)}ms. Found ${msg.segments.length} speech segments.`\r\n            );\r\n            resolve(msg.segments);\r\n          } else if (msg.type === 'progress') {\r\n            logger.debug(\r\n              `VAD Progress: ${msg.processed} segments (Latest: ${msg.latestTime.toFixed(2)}s)`\r\n            );\r\n          } else if (msg.type === 'error') {\r\n            cleanup();\r\n            const errorDetails = {\r\n              message: msg.message,\r\n              stack: msg.stack,\r\n              details: msg.details,\r\n            };\r\n            logger.error('VAD Worker processing error details:', errorDetails);\r\n            reject(new Error(msg.message));\r\n          }\r\n        };\r\n\r\n        const onAbort = () => {\r\n          cleanup();\r\n          reject(new Error('æ“ä½œå·²å–æ¶ˆ'));\r\n        };\r\n\r\n        const cleanup = () => {\r\n          this.worker?.removeEventListener('message', handleProcessMessage);\r\n          options.signal?.removeEventListener('abort', onAbort);\r\n        };\r\n\r\n        this.worker.addEventListener('message', handleProcessMessage);\r\n        options.signal?.addEventListener('abort', onAbort);\r\n\r\n        \r\n        this.worker.postMessage({\r\n          command: 'process',\r\n          audioData: audioData,\r\n          sampleRate: audioBuffer.sampleRate,\r\n        });\r\n      });\r\n    } catch (e) {\r\n      logger.error('Silero VAD failed initialization or execution', {\r\n        error: e,\r\n        message: e instanceof Error ? e.message : String(e),\r\n        stack: e instanceof Error ? e.stack : undefined,\r\n      });\r\n      logger.warn('Falling back to energy-based segmentation due to VAD error');\r\n      return this.energyBasedSegmentation(\r\n        audioData,\r\n        audioBuffer.sampleRate,\r\n        options.minDurationMs || 1000\r\n      );\r\n    }\r\n  }\r\n\r\n  private energyBasedSegmentation(\r\n    data: Float32Array,\r\n    sampleRate: number,\r\n    minDurationMs: number\r\n  ): { start: number; end: number }[] {\r\n    const segments: { start: number; end: number }[] = [];\r\n    const frameSize = Math.floor(sampleRate * 0.02); \r\n\r\n    \r\n    const energies: number[] = [];\r\n    let maxEnergy = 0;\r\n\r\n    for (let i = 0; i",
      "æ“ä½œå·²å–æ¶ˆ",
      ");\r\n\r\n      \r\n      return new Promise((resolve, reject) => {\r\n        if (!this.worker) return reject('Worker not available');\r\n        if (options.signal?.aborted) return reject(new Error('æ“ä½œå·²å–æ¶ˆ'));\r\n\r\n        const handleProcessMessage = (e: MessageEvent) => {\r\n          const msg = e.data;\r\n          if (msg.type === 'result') {\r\n            cleanup();\r\n            const runTime = performance.now() - startTime - initTime;\r\n            logger.debug("
    ],
    "src\\services\\audio\\ffmpegExtractor.ts": [
      "{\r\n  if (!isElectron()) {\r\n    throw new Error('FFmpeg æå–ä»…åœ¨ Electron ç¯å¢ƒä¸­å¯ç”¨');\r\n  }\r\n\r\n  \r\n  const filePath = (file as any).path || window.electronAPI.getFilePath(file);\r\n  if (!filePath) {\r\n    throw new Error('FFmpeg æå–éœ€è¦æ–‡ä»¶è·¯å¾„');\r\n  }\r\n\r\n  let extractedAudioPath: string | undefined;\r\n\r\n  try {\r\n    \r\n    logger.info('Getting audio info...');\r\n    const infoResult = await window.electronAPI.getAudioInfo(filePath);\r\n    if (!infoResult.success || !infoResult.info) {\r\n      throw new Error(infoResult.error || 'è·å–éŸ³é¢‘ä¿¡æ¯å¤±è´¥');\r\n    }\r\n    logger.info('Audio info:', infoResult.info);\r\n\r\n    \r\n    if (onProgress) {\r\n      window.electronAPI.onAudioExtractionProgress((progress: AudioExtractionProgress) => {\r\n        onProgress({\r\n          stage: 'extracting',\r\n          percent: progress.percent,\r\n        });\r\n      });\r\n    }\r\n\r\n    \r\n    logger.info('Extracting audio with FFmpeg...');\r\n\r\n    \r\n    let customFfmpegPath: string | undefined;\r\n    try {\r\n      const settings = await window.electronAPI.storage.getSettings();\r\n      if (settings?.debug?.ffmpegPath) {\r\n        customFfmpegPath = settings.debug.ffmpegPath;\r\n        logger.info('Using custom FFmpeg path from settings:', customFfmpegPath);\r\n      }\r\n    } catch (e) {\r\n      logger.warn('Failed to read settings for FFmpeg path:', e);\r\n    }\r\n\r\n    const options: AudioExtractionOptions = {\r\n      format: 'wav',\r\n      sampleRate: 16000, \r\n      channels: 1, \r\n      customFfmpegPath,\r\n    };\r\n\r\n    const extractResult = await window.electronAPI.extractAudioFFmpeg(filePath, options);\r\n    if (!extractResult.success || !extractResult.audioPath) {\r\n      throw new Error(extractResult.error || 'FFmpeg æå–å¤±è´¥');\r\n    }\r\n    extractedAudioPath = extractResult.audioPath;\r\n    logger.info('Audio extracted to:', extractedAudioPath);\r\n\r\n    \r\n    if (onProgress) {\r\n      onProgress({ stage: 'decoding', percent: 100 });\r\n    }\r\n    const arrayBuffer = await window.electronAPI.readExtractedAudio(extractedAudioPath);\r\n\r\n    \r\n    const AudioContext = window.AudioContext || (window as any).webkitAudioContext;\r\n    if (!AudioContext) {\r\n      throw new Error('ä¸æ”¯æŒ Web Audio API');\r\n    }\r\n    const ctx = new AudioContext();\r\n    const audioBuffer = await ctx.decodeAudioData(arrayBuffer);\r\n\r\n    logger.info('Audio decoded successfully');\r\n    return audioBuffer;\r\n  } finally {\r\n    \r\n    if (extractedAudioPath) {\r\n      try {\r\n        await window.electronAPI.cleanupTempAudio(extractedAudioPath);\r\n        logger.info('Temp audio cleaned up');\r\n      } catch (err) {\r\n        logger.warn('Failed to cleanup temp audio:', err);\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n\r\nexport async function smartDecodeAudio(\r\n  file: File,\r\n  onProgress?: (progress: { stage: string; percent: number }) => void\r\n): Promise",
      "FFmpeg æå–ä»…åœ¨ Electron ç¯å¢ƒä¸­å¯ç”¨",
      "FFmpeg æå–éœ€è¦æ–‡ä»¶è·¯å¾„",
      "è·å–éŸ³é¢‘ä¿¡æ¯å¤±è´¥",
      "FFmpeg æå–å¤±è´¥",
      "ä¸æ”¯æŒ Web Audio API"
    ],
    "src\\services\\audio\\decoder.ts": [
      "=> {\r\n  \r\n  const filePath = isElectron()\r\n    ? (file as any).path || window.electronAPI.getFilePath(file) || undefined\r\n    : undefined;\r\n\r\n  \r\n  logger.info('Audio decoding environment check:', {\r\n    isElectron: isElectron(),\r\n    hasFilePath: !!filePath,\r\n    filePath: filePath,\r\n  });\r\n\r\n  \r\n  if (isElectron() && filePath) {\r\n    try {\r\n      logger.info('Starting audio decoding using FFmpeg...');\r\n      return await smartDecodeAudio(file, onProgress);\r\n    } catch (err: any) {\r\n      logger.warn('FFmpeg failed, using Web Audio API fallback:', err.message);\r\n      \r\n    }\r\n  }\r\n\r\n  logger.info('Starting audio decoding using Web Audio API...');\r\n  let arrayBuffer: ArrayBuffer;\r\n\r\n  \r\n  if (isElectron() && filePath) {\r\n    arrayBuffer = await (window as any).electronAPI.readAudioFile(filePath);\r\n  } else {\r\n    \r\n    arrayBuffer = await file.arrayBuffer();\r\n  }\r\n\r\n  const AudioContext = window.AudioContext || (window as any).webkitAudioContext;\r\n  if (!AudioContext) throw new Error('ä¸æ”¯æŒ Web Audio API');\r\n  const ctx = new AudioContext();\r\n\r\n  try {\r\n    return await ctx.decodeAudioData(arrayBuffer);\r\n  } catch (e: any) {\r\n    \r\n    if (e.name === 'EncodingError' || e.message?.includes('Unable to decode')) {\r\n      throw new Error('ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼ï¼Œè¯·å°è¯•è½¬æ¢ä¸º WAV æˆ– MP3 æ ¼å¼åé‡è¯•');\r\n    }\r\n    if (e.name === 'NotSupportedError') {\r\n      throw new Error('æµè§ˆå™¨ä¸æ”¯æŒæ­¤éŸ³é¢‘ç¼–ç ï¼Œè¯·å°è¯•å…¶ä»–æ ¼å¼');\r\n    }\r\n    throw new Error(`éŸ³é¢‘è§£ç å¤±è´¥: ${e.message || 'æœªçŸ¥é”™è¯¯'}`);\r\n  }\r\n};\r\n\r\n\r\nexport async function decodeAudioWithRetry(\r\n  file: File,\r\n  retries = 3,\r\n  onProgress?: (progress: { stage: string; percent: number }) => void\r\n): Promise",
      "ä¸æ”¯æŒ Web Audio API",
      "ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼ï¼Œè¯·å°è¯•è½¬æ¢ä¸º WAV æˆ– MP3 æ ¼å¼åé‡è¯•",
      "æµè§ˆå™¨ä¸æ”¯æŒæ­¤éŸ³é¢‘ç¼–ç ï¼Œè¯·å°è¯•å…¶ä»–æ ¼å¼",
      "æœªçŸ¥é”™è¯¯",
      "éŸ³é¢‘è§£ç é‡è¯•åä»ç„¶å¤±è´¥ã€‚",
      "éŸ³é¢‘è§£ç å¤±è´¥: ${e.message || 'æœªçŸ¥é”™è¯¯'}"
    ],
    "src\\services\\api\\whisper-local\\transcribe.ts": [
      "=> {\r\n  logger.info(\r\n    `[LocalWhisper] Processing request - blob size: ${(audioBlob.size / 1024 / 1024).toFixed(2)}MB, Threads: ${threads}`\r\n  );\r\n\r\n  try {\r\n    \r\n    if (!window.electronAPI) {\r\n      throw new WhisperLocalError('NOT_ELECTRON', 'æœ¬åœ° Whisper ä»…åœ¨æ¡Œé¢åº”ç”¨ä¸­å¯ç”¨');\r\n    }\r\n\r\n    \r\n    const arrayBuffer = await audioBlob.arrayBuffer();\r\n\r\n    if (signal?.aborted) {\r\n      throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n    }\r\n\r\n    if (signal?.aborted) {\r\n      throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n    }\r\n\r\n    \r\n    logger.info(`[LocalWhisper] Sending request to main process. Model: ${modelPath}`);\r\n\r\n    \r\n    const transcriptionPromise = window.electronAPI.transcribeLocal({\r\n      audioData: arrayBuffer,\r\n      modelPath,\r\n      language,\r\n      threads,\r\n      customBinaryPath,\r\n    });\r\n\r\n    const cancelPromise = new Promise",
      "æœ¬åœ° Whisper ä»…åœ¨æ¡Œé¢åº”ç”¨ä¸­å¯ç”¨",
      "æ“ä½œå·²å–æ¶ˆ",
      "è½¬å½•å¤±è´¥",
      ");\r\n\r\n  try {\r\n    \r\n    if (!window.electronAPI) {\r\n      throw new WhisperLocalError('NOT_ELECTRON', 'æœ¬åœ° Whisper ä»…åœ¨æ¡Œé¢åº”ç”¨ä¸­å¯ç”¨');\r\n    }\r\n\r\n    \r\n    const arrayBuffer = await audioBlob.arrayBuffer();\r\n\r\n    if (signal?.aborted) {\r\n      throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n    }\r\n\r\n    if (signal?.aborted) {\r\n      throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n    }\r\n\r\n    \r\n    logger.info(",
      ");\r\n\r\n    \r\n    const transcriptionPromise = window.electronAPI.transcribeLocal({\r\n      audioData: arrayBuffer,\r\n      modelPath,\r\n      language,\r\n      threads,\r\n      customBinaryPath,\r\n    });\r\n\r\n    const cancelPromise = new Promise<never>((_, reject) => {\r\n      if (signal) {\r\n        signal.addEventListener('abort', () => {\r\n          logger.info('[LocalWhisper] Transcription cancelled by user');\r\n          reject(new Error('æ“ä½œå·²å–æ¶ˆ'));\r\n        });\r\n      }\r\n    });\r\n\r\n    const result = await Promise.race([transcriptionPromise, cancelPromise]);\r\n\r\n    if (!result.success) {\r\n      throw new WhisperLocalError('TRANSCRIPTION_FAILED', result.error || 'è½¬å½•å¤±è´¥');\r\n    }\r\n\r\n    logger.info("
    ],
    "src\\services\\api\\openai\\whisper.ts": [
      "OpenAI API å¯†é’¥æ— æ•ˆï¼è¯·æ£€æŸ¥æ‚¨çš„ API å¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®ã€‚",
      "OpenAI API è®¿é—®è¢«æ‹’ç» (403)ï¼è¯·æ£€æŸ¥ API å¯†é’¥æƒé™æˆ–è´¦æˆ·çŠ¶æ€ã€‚",
      "OpenAI API é…é¢å·²ç”¨å°½æˆ–è¯·æ±‚è¿‡äºé¢‘ç¹ (429)ï¼è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥æ‚¨çš„é…é¢é™åˆ¶ã€‚",
      "OpenAI è´¦æˆ·ä½™é¢ä¸è¶³æˆ–æœªé…ç½®ä»˜æ¬¾æ–¹å¼ï¼è¯·æ£€æŸ¥æ‚¨çš„è´¦æˆ·è®¡è´¹è®¾ç½®ã€‚",
      "è¯·æ±‚çš„ Whisper æ¨¡å‹ä¸å­˜åœ¨ (404)ï¼è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚",
      "éŸ³é¢‘æ–‡ä»¶æ ¼å¼é”™è¯¯ (400)ï¼è¯·ç¡®ä¿æ–‡ä»¶æ ¼å¼å—æ”¯æŒã€‚",
      "æ“ä½œå·²å–æ¶ˆ",
      ".toLowerCase();\r\n\r\n  \r\n  if (\r\n    httpStatus === 401 ||\r\n    errorCode === 'invalid_api_key' ||\r\n    errorType === 'authentication_error' ||\r\n    combined.includes('unauthorized') ||\r\n    combined.includes('invalid api key') ||\r\n    combined.includes('incorrect api key')\r\n  ) {\r\n    return 'OpenAI API å¯†é’¥æ— æ•ˆï¼è¯·æ£€æŸ¥æ‚¨çš„ API å¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®ã€‚';\r\n  }\r\n\r\n  \r\n  if (\r\n    httpStatus === 403 ||\r\n    errorType === 'permission_denied_error' ||\r\n    combined.includes('forbidden') ||\r\n    combined.includes('permission denied')\r\n  ) {\r\n    return 'OpenAI API è®¿é—®è¢«æ‹’ç» (403)ï¼è¯·æ£€æŸ¥ API å¯†é’¥æƒé™æˆ–è´¦æˆ·çŠ¶æ€ã€‚';\r\n  }\r\n\r\n  \r\n  if (\r\n    httpStatus === 429 ||\r\n    errorCode === 'insufficient_quota' ||\r\n    errorCode === 'rate_limit_exceeded' ||\r\n    errorType === 'rate_limit_error' ||\r\n    combined.includes('quota') ||\r\n    combined.includes('rate limit') ||\r\n    combined.includes('too many requests')\r\n  ) {\r\n    return 'OpenAI API é…é¢å·²ç”¨å°½æˆ–è¯·æ±‚è¿‡äºé¢‘ç¹ (429)ï¼è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥æ‚¨çš„é…é¢é™åˆ¶ã€‚';\r\n  }\r\n\r\n  \r\n  if (\r\n    errorCode === 'billing_hard_limit_reached' ||\r\n    combined.includes('insufficient') ||\r\n    combined.includes('billing') ||\r\n    combined.includes('payment') ||\r\n    combined.includes('balance')\r\n  ) {\r\n    return 'OpenAI è´¦æˆ·ä½™é¢ä¸è¶³æˆ–æœªé…ç½®ä»˜æ¬¾æ–¹å¼ï¼è¯·æ£€æŸ¥æ‚¨çš„è´¦æˆ·è®¡è´¹è®¾ç½®ã€‚';\r\n  }\r\n\r\n  \r\n  if (\r\n    httpStatus === 404 ||\r\n    errorType === 'not_found_error' ||\r\n    combined.includes('model not found') ||\r\n    combined.includes('not found')\r\n  ) {\r\n    return 'è¯·æ±‚çš„ Whisper æ¨¡å‹ä¸å­˜åœ¨ (404)ï¼è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚';\r\n  }\r\n\r\n  \r\n  if (httpStatus === 400 && !combined.includes('api key')) {\r\n    if (combined.includes('audio') || combined.includes('file')) {\r\n      return 'éŸ³é¢‘æ–‡ä»¶æ ¼å¼é”™è¯¯ (400)ï¼è¯·ç¡®ä¿æ–‡ä»¶æ ¼å¼å—æ”¯æŒã€‚';\r\n    }\r\n  }\r\n\r\n  return undefined;\r\n}\r\n\r\nexport const transcribeWithWhisper = async (\r\n  audioBlob: Blob,\r\n  apiKey: string,\r\n  model: string,\r\n  endpoint?: string,\r\n  timeout?: number,\r\n  signal?: AbortSignal\r\n): Promise<SubtitleItem[]> => {\r\n  const formData = new FormData();\r\n  formData.append('file', audioBlob, 'audio.wav');\r\n  formData.append('model', model); \r\n  formData.append('response_format', 'verbose_json');\r\n\r\n  \r\n  formData.append('vad_filter', 'true'); \r\n  formData.append('no_speech_threshold', '0.6'); \r\n\r\n  let attempt = 0;\r\n  const maxRetries = 3;\r\n  let lastError: any;\r\n\r\n  const baseUrl = endpoint || 'https:\r\n\r\n  while (attempt < maxRetries) {\r\n    try {\r\n      \r\n      if (signal?.aborted) {\r\n        throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n      }\r\n\r\n      const controller = new AbortController();\r\n      const timeoutId = setTimeout(() => controller.abort(), timeout || 600000); \r\n\r\n      \r\n      if (signal) {\r\n        signal.addEventListener('abort', () => controller.abort());\r\n      }\r\n\r\n      const response = await fetch("
    ],
    "src\\services\\api\\openai\\transcribe.ts": [
      "æ“ä½œå·²å–æ¶ˆ",
      "å·²å¯ç”¨æœ¬åœ° Whisper ä½†æœªæä¾›æ¨¡å‹è·¯å¾„",
      "æœ¬åœ°è½¬å½•å¤±è´¥ï¼Œå·²åˆ‡æ¢åˆ°åœ¨çº¿ API",
      "æœ¬åœ°è½¬å½•å¤±è´¥ï¼š${error.message}"
    ],
    "src\\services\\api\\openai\\chat.ts": [
      "æ“ä½œå·²å–æ¶ˆ",
      ");\r\n  const base64Audio = await blobToBase64(audioBlob);\r\n\r\n  const requestBody = {\r\n    model: model, \r\n    modalities: ['text'],\r\n    messages: [\r\n      {\r\n        role: 'user',\r\n        content: [\r\n          {\r\n            type: 'text',\r\n            text: \"Transcribe the following audio. Return ONLY a JSON object with a 'segments' array. Each segment must have 'start' (number, seconds), 'end' (number, seconds), and 'text' (string). Do not include any other markdown.\",\r\n          },\r\n          {\r\n            type: 'input_audio',\r\n            input_audio: {\r\n              data: base64Audio,\r\n              format: 'wav',\r\n            },\r\n          },\r\n        ],\r\n      },\r\n    ],\r\n  };\r\n\r\n  const baseUrl = endpoint || 'https:\r\n\r\n  try {\r\n    \r\n    if (signal?.aborted) {\r\n      throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n    }\r\n\r\n    const controller = new AbortController();\r\n    const timeoutId = setTimeout(() => controller.abort(), timeout || 600000); \r\n\r\n    \r\n    if (signal) {\r\n      signal.addEventListener('abort', () => controller.abort());\r\n    }\r\n\r\n    const response = await fetch(",
      "GPT-4o è½¬å½•é”™è¯¯ï¼š${err.error?.message || response.statusText}",
      "GPT-4o éŸ³é¢‘è½¬å½•å¤±è´¥ï¼š${e.message}"
    ],
    "src\\services\\api\\gemini\\core\\prompts.ts": [
      "${MAX_SEGMENT_CHARACTERS} ${charType}`;\r\n}\r\n\r\n\r\nfunction getTimestampSplittingInstructions(): string {\r\n  return `\r\n**IF SPLITTING IS NEEDED**, follow these rules exactly:\r\nâ†’ **PRESERVE ORIGINAL BOUNDARIES**: The FIRST split segment starts at original start time, the LAST split segment ends at original end time\r\nâ†’ **NO GAPS, NO OVERLAPS**: Split segments must be perfectly continuous (segment N end = segment N+1 start)\r\nâ†’ **LISTEN TO AUDIO FOR SPLIT TIMING**: Do NOT allocate time proportionally by text length. Instead:\r\n   1. Listen to the actual audio to hear when each phrase/sentence is spoken\r\n   2. Identify natural pauses, breath breaks, or sentence transitions in the audio\r\n   3. Set split timestamps based on ACTUAL speech timing in the audio\r\n   Example (English): Original segment 00:00:00,000 â†’ 00:00:06,000, text \"Hello world. How are you today?\"\r\n   - If speaker says \"Hello world.\" quickly (1.5s) then pauses, then speaks slowly:\r\n   - Split 1: \"Hello world.\" â†’ 00:00:00,000 â†’ 00:00:01,500 (based on actual audio)\r\n   - Split 2: \"How are you today?\" â†’ 00:00:01,500 â†’ 00:00:06,000 (based on actual audio)\r\n   Example (Japanese): Original segment 00:00:00,000 â†’ 00:00:05,000, text \"ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚æ•£æ­©ã«è¡Œãã¾ã—ã‚‡ã†ã€‚\"\r\n   - If speaker says \"ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚\" slowly (3s), then quickly says the rest:\r\n   - Split 1: \"ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚\" â†’ 00:00:00,000 â†’ 00:00:03,000 (based on actual audio)\r\n   - Split 2: \"æ•£æ­©ã«è¡Œãã¾ã—ã‚‡ã†ã€‚\" â†’ 00:00:03,000 â†’ 00:00:05,000 (based on actual audio)\r\nâ†’ **NATURAL BREAKS ONLY**: Split at sentence boundaries, punctuation, or natural pauses - NEVER mid-word or mid-phrase\r\nâ†’ **MINIMUM DURATION**: Each split segment must be at least 0.5 seconds`;\r\n}\r\n\r\n\r\nfunction getFillerWordsRule(): string {\r\n  return `Remove filler words (${FILLER_WORDS_PROMPT})`;\r\n}\r\n\r\n\r\nfunction formatGlossaryForPrompt(\r\n  glossary: GlossaryItem[] | undefined,\r\n  mode: 'refinement' | 'translation' | 'proofread' | 'fix_timestamps'\r\n): string {\r\n  if (!glossary || glossary.length === 0) return '';\r\n\r\n  if (mode === 'refinement') {\r\n    const terms = glossary.map((g) => `- ${g.term}${g.notes ? ` (${g.notes})` : ''}`).join('\\n');\r\n    return `\\n\\nKEY TERMINOLOGY (Listen for these terms and transcribe them accurately in the ORIGINAL LANGUAGE):\\n${terms}`;\r\n  }\r\n\r\n  const terms = glossary\r\n    .map((g) => `- ${g.term}: ${g.translation} ${g.notes ? `(${g.notes})` : ''}`)\r\n    .join('\\n');\r\n  return `\\n\\nTERMINOLOGY GLOSSARY (STRICTLY FOLLOW):\\n${terms}`;\r\n}\r\n\r\n\r\nfunction getGenreSpecificGuidance(genre: string): string {\r\n  switch (genre) {\r\n    case 'anime':\r\n      return `\\nGENRE-SPECIFIC NOTES:\\n- Preserve emotional nuances and character personality\\n- Keep honorifics (-san, -kun, -chan) appropriately\\n- Use casual, emotive tone in translation`;\r\n    case 'movie':\r\n      return `\\nGENRE-SPECIFIC NOTES:\\n- Natural dialogue flow is critical\\n- Keep subtitles concise and easy to read\\n- Match the tone and pacing of the scene`;\r\n    case 'news':\r\n      return `\\nGENRE-SPECIFIC NOTES:\\n- Maintain formal, objective tone\\n- Use standard news terminology\\n- Accuracy is paramount`;\r\n    case 'tech':\r\n      return `\\nGENRE-SPECIFIC NOTES:\\n- Keep technical terms precise\\n- Preserve standard English acronyms (API, SDK, etc.)\\n- Ensure terminology consistency`;\r\n    case 'general':\r\n      return `\\nGENRE-SPECIFIC NOTES:\\n- Neutral and accurate translation\\n- Clear, accessible language`;\r\n    default:\r\n      return `\\nGENRE-SPECIFIC NOTES:\\n- Adapt tone and terminology for ${genre} content`;\r\n  }\r\n}\r\n\r\n\r\nexport const getSystemInstructionWithDiarization = (\r\n  genre: string,\r\n  customPrompt: string | undefined,\r\n  mode: 'refinement' | 'translation' | 'proofread' | 'fix_timestamps',\r\n  glossary?: GlossaryItem[],\r\n  enableDiarization?: boolean,\r\n  speakerProfiles?: SpeakerProfile[],\r\n  minSpeakers?: number,\r\n  maxSpeakers?: number\r\n): string => {\r\n  \r\n  if ((mode !== 'fix_timestamps' && mode !== 'refinement') || !enableDiarization) {\r\n    return getSystemInstruction(genre, customPrompt, mode, glossary);\r\n  }\r\n\r\n  \r\n  const glossaryText = formatGlossaryForPrompt(glossary, mode);\r\n\r\n  let diarizationSection = '';\r\n  if (enableDiarization) {\r\n    \r\n    let speakerCountHint = '';\r\n    if (minSpeakers && maxSpeakers) {\r\n      speakerCountHint = `\\nâ†’ **USER HINT - EXPECTED SPEAKER COUNT**: The user has specified there are between ${minSpeakers} and ${maxSpeakers} speakers.`;\r\n    } else if (minSpeakers) {\r\n      speakerCountHint = `\\nâ†’ **USER HINT - EXPECTED SPEAKER COUNT**: The user has specified there are at least ${minSpeakers} speakers.`;\r\n    } else if (maxSpeakers) {\r\n      speakerCountHint = `\\nâ†’ **USER HINT - EXPECTED SPEAKER COUNT**: The user has specified there are at most ${maxSpeakers} speakers.`;\r\n    }\r\n\r\n    if (speakerProfiles && speakerProfiles.length > 0) {\r\n      \r\n      diarizationSection = `\r\n[P2 - SPEAKER IDENTIFICATION] Diarization (ENABLED - WITH PROFILE DATABASE)\r\n\r\n**IMPORTANT**: A senior AI (${SENIOR_MODEL_NAME}) has pre-analyzed this audio and identified ${speakerProfiles.length} speakers.\r\nYour task is to MATCH voices to these profiles.${speakerCountHint}\r\n\r\n**KNOWN SPEAKER PROFILES**:\r\n${speakerProfiles\r\n  .map(\r\n    (p, i) => `\r\n${i + 1}. **${p.id}**\r\n   - Gender: ${p.characteristics.gender}\r\n   ${p.characteristics.name ? `- Name: ${p.characteristics.name}` : ''}\r\n   - Pitch: ${p.characteristics.pitch}\r\n   - Speed: ${p.characteristics.speed}\r\n   - Accent: ${p.characteristics.accent}\r\n   - Tone: ${p.characteristics.tone}\r\n   ${p.inferredIdentity ? `- Role: ${p.inferredIdentity}` : ''}\r\n   ${p.speakingStyle ? `- Speaking Style: ${p.speakingStyle.formality || ''} ${p.speakingStyle.vocabulary ? `(${p.speakingStyle.vocabulary})` : ''}` : ''}\r\n   ${p.emotionalTone ? `- Emotional Tone: ${p.emotionalTone}` : ''}\r\n   ${p.catchphrases && p.catchphrases.length > 0 ? `- Catchphrases: ${p.catchphrases.map((c) => `\"${c}\"`).join(', ')}` : ''}\r\n   ${p.speakingContext && p.speakingContext.length > 0 ? `- Speaking Context: ${p.speakingContext.join(', ')}` : ''}\r\n   - Sample Quotes: ${p.sampleQuotes.map((q) => `\"${q}\"`).join(', ')}\r\n   - Confidence: ${(p.confidence * 100).toFixed(0)}%\r\n`\r\n  )\r\n  .join('\\n')}\r\n\r\n**MATCHING STRATEGY** (Priority Order):\r\n\r\n1. **PRIMARY: Content & Style Matching** (Most Reliable)\r\n   - **Catchphrase Detection**: If the speaker uses any catchphrase from a profile â†’ VERY STRONG match\r\n   - **Sample Quote Similarity**: Compare what is SAID with sample quotes in each profile\r\n   - **Vocabulary Style**: Match vocabulary level (technical/colloquial/formal) with profile's speakingStyle\r\n   - **Topic Alignment**: If speaker discusses topics in a profile's speakingContext â†’ Strong match\r\n   - **Identity Clues**: If content relates to inferredIdentity (e.g., medical terms â†’ Doctor profile)\r\n\r\n2. **SECONDARY: Dialogue Context** (Very Helpful)\r\n   - **Conversation Flow**: If previous subtitle was Speaker A asking question â†’ this might be Speaker B answering\r\n   - **Alternation Pattern**: Speakers usually alternate (Aâ†’Bâ†’Aâ†’B)\r\n   - **Continuity**: Multiple consecutive lines with same tone/topic â†’ likely same speaker\r\n\r\n3. **TERTIARY: Voice & Emotion** (Use as Confirmation)\r\n   - **Emotional Consistency**: Match emotional tone (enthusiastic/calm) with profile\r\n   - **Gender**: Usually reliable if clearly male/female\r\n   - **Pitch/Speed/Accent**: HINTS only - don't rely heavily\r\n\r\n4. **CRITICAL RULE: Consistency**\r\n   - Once assigned, maintain same speaker ID for similar content/style\r\n   - Same voice + same topic + same catchphrases = same speaker\r\n\r\n**MATCHING PROCESS**:\r\nFor each subtitle line:\r\nâ†’ Step 1: Check for catchphrases from any profile (HIGHEST priority)\r\nâ†’ Step 2: Compare content with sample quotes (exact or similar phrases)\r\nâ†’ Step 3: Check if topic relates to any profile's speakingContext or inferredIdentity\r\nâ†’ Step 4: Consider dialogue context (response pattern, continuity)\r\nâ†’ Step 5: Verify emotional tone and speaking style consistency\r\nâ†’ Step 6: Use voice characteristics as final confirmation\r\nâ†’ Step 7: Assign the best match from the ${speakerProfiles.length} profiles\r\n\r\n**EDGE CASE - NEW SPEAKER DISCOVERY**:\r\nIf you encounter a voice that does NOT match ANY profile AND you are >90% confident it's a NEW speaker:\r\n- Assign a new ID: \"Speaker ${speakerProfiles.length + 1}\", \"Speaker ${speakerProfiles.length + 2}\", etc.\r\n- Add brief characteristics in a comment field (you can use \"comment\" for this)\r\n- This should be RARE - Gemini 3 Pro is very thorough\r\n\r\n**QUALITY VERIFICATION**:\r\nâœ“ Every subtitle has a \"speaker\" field\r\nâœ“ Speaker IDs match profile list (or are justified new additions)\r\nâœ“ Voice changes are detected and speaker switches occur appropriately\r\nâœ“ Consistency maintained across the batch\r\n`;\r\n    } else {\r\n      \r\n      diarizationSection = `\r\n[P2 - SPEAKER IDENTIFICATION] Audio Diarization\r\nâ†’ **CRITICAL TASK**: Identify and label DISTINCT SPEAKERS in the audio\r\nâ†’ **OUTPUT FORMAT**: Add \"speaker\" field to EVERY subtitle entry\r\nâ†’ **LABELING**: Use \"Speaker 1\", \"Speaker 2\", \"Speaker 3\", etc.${speakerCountHint}\r\n\r\n**VOICE CHARACTERISTICS TO ANALYZE**:\r\nâ†’ Pitch: Fundamental frequency and tonal range\r\nâ†’ Timbre: Voice quality and texture\r\nâ†’ Speaking rate: Words per minute and rhythm\r\nâ†’ Accent or dialect: Regional or linguistic markers\r\nâ†’ Gender: If clearly distinguishable from vocal characteristics\r\n\r\n**DIARIZATION RULES**:\r\nâ†’ SAME voice = SAME speaker ID (consistency is critical)\r\nâ†’ If a speaker change occurs mid-segment: SPLIT the segment\r\nâ†’ Single speaker audio: Still label as \"Speaker 1\"\r\nâ†’ Overlapping speech: Assign to the PRIMARY/LOUDER speaker\r\nâ†’ Background voices: IGNORE unless they are part of main dialogue\r\nâ†’ Narrator vs. character dialogue: Treat as DIFFERENT speakers\r\n\r\n**EDGE CASES**:\r\nâ†’ Similar voices: Err on the side of maintaining previous assignment\r\nâ†’ Short interjections: May be same speaker as previous/next segment\r\nâ†’ Phone calls/filtered audio: Use contextual clues and voice patterns\r\n\r\n**QUALITY VERIFICATION**:\r\nBefore returning, confirm:\r\nâœ“ Every segment has a \"speaker\" field\r\nâœ“ Speaker IDs remain consistent throughout\r\nâœ“ No speaker changes within a single segment\r\nâœ“ At least one speaker identified (minimum \"Speaker 1\")\r\n`;\r\n    }\r\n  }\r\n\r\n  if (mode === 'refinement') {\r\n    return `You are a professional Subtitle QA Specialist. \r\n    You will receive an audio chunk and a raw JSON transcription.\r\n    \r\n    YOUR TASKS:\r\n    1. Listen to the audio to verify the transcription.\r\n    2. **CHECK FOR MISSED SPEECH**: If there is CLEAR, MEANINGFUL speech in the audio that is MISSING from the transcription, you MUST ADD IT.\r\n       â†’ Do NOT add: background noise, music lyrics, ambient sounds, or unintelligible mumbling\r\n    3. **ALIGN TIMESTAMPS**: Listen to audio and adjust start/end times to match actual speech boundaries.\r\n       â†’ Whisper timestamps may drift, especially in long audio - correct any misalignment you detect\r\n       â†’ **Timestamps MUST be strictly within the provided audio duration.**\r\n       â†’ **NEVER MERGE** multiple segments into one - only SPLIT long segments when needed\r\n    4. FIX TRANSCRIPTION: Correct mishearings, typos, and proper nouns (names, terminology).\r\n    5. IGNORE FILLERS: Do not transcribe stuttering or meaningless filler words (${FILLER_WORDS_PROMPT}).\r\n    6. SPLIT LINES: STRICT RULE. ${getSegmentSplittingRule()}, YOU MUST SPLIT IT into shorter, natural segments.\r\n    ${getTimestampSplittingInstructions()}\r\n    7. **LANGUAGE RULE**: Keep the transcription in the ORIGINAL LANGUAGE spoken in the audio. DO NOT translate to any other language.\r\n    8. FORMAT: Return a valid JSON array.\r\n\r\n    ${diarizationSection}\r\n\r\n    9. FINAL CHECK: Before outputting, strictly verify that ALL previous rules have been perfectly followed. Correct any remaining errors.\r\n    \r\n    Genre Context: ${genre}${glossaryText}`;\r\n  }\r\n\r\n  return `You are a professional Subtitle Timing and Synchronization Specialist.\r\nYour PRIMARY GOAL is to perfect timestamp alignment and segment timing for ${genre} content.\r\n\r\nTASK RULES (Priority Order):\r\n\r\n[P0 - HIGHEST] User Directives\r\nâ†’ If a subtitle has a \"comment\" field, follow that instruction exactly\r\nâ†’ User corrections override all other rules\r\n\r\n[P1 - PRIMARY FOCUS] Timestamp Alignment\r\nâ†’ Listen to audio and align start/end times to actual speech boundaries\r\nâ†’ Whisper timestamps may drift, especially in long audio - correct any misalignment you detect\r\nâ†’ Ensure timestamps are strictly within the provided audio duration\r\nâ†’ Timestamps must be relative to provided audio file (starting at 00:00:00)\r\nâ†’ **NEVER MERGE** multiple segments into one - only SPLIT long segments when needed\r\n\r\n${diarizationSection}\r\n\r\n[P3 - READABILITY] Segment Splitting\r\nâ†’ ${getSegmentSplittingRule('translation')}\r\n${getTimestampSplittingInstructions()}\r\n\r\n[P4 - CONTENT ACCURACY] Audio Content Verification\r\nâ†’ If you hear CLEAR, MEANINGFUL speech NOT in subtitles â†’ ADD new subtitle entries\r\nâ†’ Do NOT add: background noise, music lyrics, ambient sounds, or unintelligible speech\r\nâ†’ ${getFillerWordsRule()} from 'text_original'\r\n\r\n[P5 - ABSOLUTE RULE] Translation Preservation\r\nâ†’ DO NOT modify 'text_translated' field under ANY circumstances\r\nâ†’ Even if translation is incorrect â†’ LEAVE IT UNCHANGED\r\nâ†’ Your job is TIMING and SPEAKER IDENTIFICATION, not translation\r\nâ†’ Translation fixes belong in the Proofread function\r\n\r\nOUTPUT REQUIREMENTS:\r\nâœ“ Valid JSON matching input structure\r\nâœ“ Preserve all IDs (assign new IDs only for inserted/split segments)\r\nâœ“ All timestamps in HH:MM:SS,mmm format\r\nâœ“ Ensure start",
      "{\r\n  \r\n  const glossaryText = formatGlossaryForPrompt(glossary, mode);\r\n\r\n  \r\n\r\n  if (mode === 'refinement') {\r\n    return `You are a professional Subtitle QA Specialist. \r\n    You will receive an audio chunk and a raw JSON transcription.\r\n    \r\n    YOUR TASKS:\r\n    1. Listen to the audio to verify the transcription.\r\n    2. **CHECK FOR MISSED SPEECH**: If there is CLEAR, MEANINGFUL speech in the audio that is MISSING from the transcription, you MUST ADD IT.\r\n       â†’ Do NOT add: background noise, music lyrics, ambient sounds, or unintelligible mumbling\r\n    3. **ALIGN TIMESTAMPS**: Listen to audio and adjust start/end times to match actual speech boundaries.\r\n       â†’ Whisper timestamps may drift, especially in long audio - correct any misalignment you detect\r\n       â†’ **Timestamps MUST be strictly within the provided audio duration.**\r\n       â†’ **NEVER MERGE** multiple segments into one - only SPLIT long segments when needed\r\n    4. FIX TRANSCRIPTION: Correct mishearings, typos, and proper nouns (names, terminology).\r\n    5. IGNORE FILLERS: Do not transcribe stuttering or meaningless filler words (${FILLER_WORDS_PROMPT}).\r\n    6. SPLIT LINES: STRICT RULE. ${getSegmentSplittingRule()}, YOU MUST SPLIT IT into shorter, natural segments.\r\n    ${getTimestampSplittingInstructions()}\r\n    7. **LANGUAGE RULE**: Keep the transcription in the ORIGINAL LANGUAGE spoken in the audio. DO NOT translate to any other language.\r\n    8. FORMAT: Return a valid JSON array.\r\n    9. FINAL CHECK: Before outputting, strictly verify that ALL previous rules (1-8) have been perfectly followed. Correct any remaining errors.\r\n    \r\n    \r\n    Genre Context: ${genre}${glossaryText}`;\r\n  }\r\n\r\n  \r\n  if (mode === 'translation') {\r\n    let genreContext = '';\r\n    switch (genre) {\r\n      case 'anime':\r\n        genreContext = 'Genre: Anime. Use casual, emotive tone. Preserve honorifics nuances.';\r\n        break;\r\n      case 'movie':\r\n        genreContext = 'Genre: Movie/TV. Natural dialogue, concise, easy to read.';\r\n        break;\r\n      case 'news':\r\n        genreContext = 'Genre: News. Formal, objective, standard terminology.';\r\n        break;\r\n      case 'tech':\r\n        genreContext = 'Genre: Tech. Precise terminology. Keep standard English acronyms.';\r\n        break;\r\n      case 'general':\r\n        genreContext = 'Genre: General. Neutral and accurate.';\r\n        break;\r\n      default:\r\n        genreContext = `Context: ${genre}. Translate using tone/terminology appropriate for this context.`;\r\n        break;\r\n    }\r\n\r\n    return `You are an expert Subtitle Translator specializing in ${genre} content.\r\n    Your GOAL is to provide fluent, natural Simplified Chinese (zh-CN) translations while strictly preserving the subtitle structure.\r\n\r\n    TASK RULES (Strict Priority):\r\n\r\n    [P0 - STRUCTURAL INTEGRITY]\r\n    â†’ **ONE INPUT = ONE OUTPUT**: You must return exactly one translated subtitle for every input subtitle.\r\n    â†’ **ID PRESERVATION**: Maintain the \"id\" field exactly as provided.\r\n    â†’ **NO MERGING/SPLITTING**: Do not combine multiple lines or split a single line.\r\n    â†’ **TIMESTAMPS**: Do not modify timestamps.\r\n\r\n    [P1 - TRANSLATION QUALITY]\r\n    â†’ **FLUENCY**: Translate into natural, written Chinese, not \"translationese\".\r\n    â†’ **CONTEXT AWARENESS**: Use the provided genre context to determine tone and style.\r\n    â†’ **COMPLETENESS**: Ensure every meaningful part of the original text is represented.\r\n    â†’ **NO HALLUCINATIONS**: Do not invent information not present in the source.\r\n    â†’ **MULTI-LINE CONTEXT**: Read the previous and next 1-2 lines to understand context. This helps with:\r\n       - Resolving ambiguous pronouns (e.g., \"it\", \"that\")\r\n       - Understanding incomplete sentences split across lines\r\n       - Maintaining consistent tone and terminology across related lines\r\n    â†’ **STRICT BOUNDARY RULE**: Use this context for **UNDERSTANDING** only. **NEVER** merge segments or move text between lines.\r\n    â†’ **PARTIAL SENTENCES**: If a sentence is split across lines, translate ONLY the specific fragment in the current line. Do not \"complete\" it using text from the next line.${getSearchEnhancedTranslationPrompt('translation')}\r\n\r\n    [P2 - CLEANUP & REFINEMENT]\r\n    â†’ **REMOVE FILLERS**: Ignore stuttering, hesitation, and meaningless fillers (e.g., \"uh\", \"um\", \"ah\", \"eto\", \"ano\", \"å‘ƒ\", \"é‚£ä¸ª\").\r\n    â†’ **CONCISENESS**: Keep subtitles concise and easy to read quickly.\r\n\r\n    [P3 - TERMINOLOGY]\r\n    â†’ **GLOSSARY**: Strictly follow the provided glossary for specific terms.\r\n    â†’ **CONSISTENCY**: Maintain consistent terminology for names and places.\r\n\r\n    OUTPUT REQUIREMENTS:\r\n    âœ“ Valid JSON matching input structure\r\n    âœ“ Output count MUST match input count exactly\r\n    âœ“ All 'text_translated' fields must be Simplified Chinese\r\n\r\n    FINAL QUALITY CHECK:\r\n    Before returning, verify:\r\n    âœ“ Did I return the exact same number of items as the input?\r\n    âœ“ Are all IDs preserved?\r\n    âœ“ Is the Chinese fluent and natural?\r\n    âœ“ Did I remove all filler words?\r\n\r\n    ${genreContext}${customPrompt ? `\\n    ${customPrompt}\\n` : ''}${glossaryText}${\r\n      speakerProfiles && speakerProfiles.length > 0\r\n        ? `\r\n\r\n**SPEAKER PROFILES**:\r\n${speakerProfiles\r\n  .map(\r\n    (p) => `\r\n[${p.id}]\r\n- Gender: ${p.characteristics.gender}\r\n- Role: ${p.inferredIdentity || 'unknown'}\r\n- Style: ${p.speakingStyle?.formality || 'normal'}\r\n- Vocabulary: ${p.speakingStyle?.vocabulary || 'standard'}\r\n`\r\n  )\r\n  .join('')}\r\n**TRANSLATION STYLE EXAMPLES**:\r\n- formal style â†’ polite/literary expressions\r\n- casual style â†’ colloquial expressions\r\n- technical vocabulary â†’ preserve domain terms\r\n\r\nExample:\r\nSpeaker (casual): \"ã™ã”ã„ï¼\" â†’ \"å¤ªæ£’äº†ï¼\"\r\nSpeaker (formal): \"ã™ã”ã„ã§ã™ã­\" â†’ \"çœŸæ˜¯ä»¤äººå°è±¡æ·±åˆ»ã€‚\"\r\n`\r\n        : ''\r\n    }`;\r\n  }\r\n\r\n  \r\n  if (mode === 'fix_timestamps') {\r\n    return `You are a Subtitle Timing and Synchronization Specialist.\r\n      Your PRIMARY GOAL is to perfect timestamp alignment and segment timing for ${genre} content.\r\n      \r\n      TASK RULES (Strict Priority):\r\n      \r\n      [P0 - HIGHEST] User Directives\r\n      â†’ If a subtitle has a \"comment\" field, follow that instruction exactly\r\n      â†’ User corrections override all other rules\r\n      \r\n      [P1 - PRIMARY FOCUS] Timestamp Alignment\r\n      â†’ Listen to audio and align start/end times to actual speech boundaries\r\n      â†’ Whisper timestamps may drift, especially in long audio - correct any misalignment you detect\r\n      â†’ Ensure timestamps are strictly within the provided audio duration\r\n      â†’ Timestamps must be relative to provided audio file (starting at 00:00:00)\r\n      â†’ **NEVER MERGE** multiple segments into one - only SPLIT long segments when needed\r\n      \r\n      [P2 - READABILITY] Segment Splitting\r\n      â†’ ${getSegmentSplittingRule('translation')}\r\n      ${getTimestampSplittingInstructions()}\r\n      \r\n      [P3 - CONTENT ACCURACY] Audio Content Verification\r\n      â†’ If you hear CLEAR, MEANINGFUL speech NOT in subtitles â†’ ADD new subtitle entries\r\n      â†’ Do NOT add: background noise, music lyrics, ambient sounds, or unintelligible speech\r\n      â†’ ${getFillerWordsRule()} from 'text_original'\r\n      \r\n      [P4 - ABSOLUTE RULE] Translation Preservation\r\n      â†’ DO NOT modify 'text_translated' field under ANY circumstances\r\n      â†’ Even if the translation is wrong, in English, or nonsensical â†’ LEAVE IT\r\n      â†’ Your job is TIMING, not translation quality\r\n      â†’ Translation fixes belong in the Proofread function\r\n      \r\n      OUTPUT REQUIREMENTS:\r\n      âœ“ Valid JSON matching input structure\r\n      âœ“ Preserve all IDs (assign new IDs only for inserted/split segments)\r\n      âœ“ All timestamps in HH:MM:SS,mmm format\r\n      âœ“ Ensure start",
      "`\r\nTERMINOLOGY EXTRACTION TASK\r\nGenre Context: ${genre}\r\n\r\nTASK: Extract key terminology from the audio that requires consistent translation across subtitles.\r\n\r\nRULES (Priority Order):\r\n\r\n[P0 - CRITICAL] Language Detection and Matching\r\nâ†’ **FIRST**: Detect the PRIMARY LANGUAGE spoken in this audio segment\r\nâ†’ **SECOND**: Extract ONLY terms that are spoken in that detected language\r\nâ†’ **ABSOLUTE RULE**: DO NOT extract terms in other languages\r\nâ†’ Examples for Japanese audio:\r\n  â€¢ If speaker says \"é‡œå±±\" (Busan), extract \"é‡œå±±\" NOT \"Busan\"\r\n  â€¢ If speaker says \"ã‚¹ã‚«ã‚¤ã‚¹ã‚­ãƒ£ãƒŠãƒ¼\", extract \"ã‚¹ã‚«ã‚¤ã‚¹ã‚­ãƒ£ãƒŠãƒ¼\" NOT \"Skyscanner\"\r\nâ†’ Examples for English audio:\r\n  â€¢ If speaker says \"Tokyo\", extract \"Tokyo\" NOT \"æ±äº¬\"\r\n  â€¢ If speaker says \"Microsoft\", extract \"Microsoft\" NOT \"å¾®è½¯\"\r\nâ†’ **CRITICAL**: Listen to what is ACTUALLY SAID, not what you think the translation equivalent is\r\n\r\n[P1 - EXTRACTION] Identify Key Terms in Audio Language\r\nâ†’ Listen carefully to the audio\r\nâ†’ Extract names (people, characters, organizations) AS SPOKEN\r\nâ†’ Extract places (locations, venues, regions) AS SPOKEN\r\nâ†’ Extract specialized terms (technical jargon, domain-specific vocabulary) AS SPOKEN\r\nâ†’ Extract recurring phrases that need consistent translation\r\nâ†’ ONLY include terms that ACTUALLY APPEAR in this audio segment\r\n\r\n[P2 - TRANSLATION] Provide Accurate Translations\r\nâ†’ Translate all terms to Simplified Chinese\r\nâ†’ For names: Use standard transliterations (e.g., \"Alice\" â†’ \"è‰¾ä¸½ä¸\", \"é‡œå±±\" â†’ \"é‡œå±±\")\r\nâ†’ For technical terms: Use established industry translations\r\nâ†’ Use Google Search to verify standard translations when uncertain\r\nâ†’ Ensure translations are appropriate for ${genre} content\r\n\r\n[P3 - ANNOTATION] Add Context When Needed\r\nâ†’ Add notes for ambiguous terms or pronunciation guidance\r\nâ†’ Include context that helps maintain translation consistency\r\nâ†’ Note any special handling requirements\r\n\r\n[P4 - QUALITY] Focus on Consistency-Critical Terms\r\nâ†’ Prioritize terms that will appear multiple times\r\nâ†’ Skip common words that don't need special handling\r\nâ†’ Focus on terms where inconsistent translation would be problematic\r\n\r\nOUTPUT FORMAT:\r\nâ†’ JSON array of objects\r\nâ†’ Each object: {term: string, translation: string, notes?: string}\r\nâ†’ \"term\" field MUST be in the ORIGINAL LANGUAGE spoken in audio\r\nâ†’ Return empty array [] if no significant terms found\r\n\r\nFINAL VERIFICATION:\r\nâœ“ Detected audio language correctly\r\nâœ“ All \"term\" values are in the SAME LANGUAGE as the audio (NOT English unless audio is English)\r\nâœ“ All extracted terms actually appear in the audio AS SPOKEN\r\nâœ“ All translations are in Simplified Chinese\r\nâœ“ Translations verified with search when needed\r\nâœ“ Only included terms that need consistent translation\r\nâœ“ Notes added where helpful for consistency\r\n`;\r\n\r\nexport const getSpeakerProfileExtractionPrompt = (\r\n  genre: string,\r\n  minSpeakers?: number,\r\n  maxSpeakers?: number\r\n) => {\r\n  \r\n  let speakerCountHint = '';\r\n  if (minSpeakers && maxSpeakers) {\r\n    speakerCountHint = `\\n- **USER HINT - EXPECTED SPEAKER COUNT**: Between ${minSpeakers} and ${maxSpeakers} speakers`;\r\n  } else if (minSpeakers) {\r\n    speakerCountHint = `\\n- **USER HINT - EXPECTED SPEAKER COUNT**: At least ${minSpeakers} speakers`;\r\n  } else if (maxSpeakers) {\r\n    speakerCountHint = `\\n- **USER HINT - EXPECTED SPEAKER COUNT**: At most ${maxSpeakers} speakers`;\r\n  }\r\n\r\n  return `\r\n**TASK**: Extract comprehensive speaker profiles from audio samples for downstream voice matching.\r\n\r\n**CONTEXT**:\r\n- Genre: ${genre}\r\n- Audio: Representative samples from different time periods\r\n- Purpose: Create voice fingerprint database for Gemini Flash to identify speakers\r\n- **Tools Available**: Google Search (use to verify public figures if names are mentioned)${speakerCountHint}\r\n\r\n**SPEAKER PROFILE EXTRACTION**:\r\n1. Identify ALL distinct speakers (missing a speaker is critical failure)\r\n2. For each speaker, document:\r\n   - **Voice characteristics**: gender, name (if mentioned), pitch, speed, accent, tone\r\n   - **Inferred identity/role**: occupation, character role, identity clues (if mentioned in dialogue)\r\n   - **Speaking style**: \r\n     * Formality: formal/casual/mixed (level of formality)\r\n     * Vocabulary: technical/colloquial/poetic/etc. (vocabulary style)\r\n     * Sentence structure: complex/simple/fragmented (sentence patterns)\r\n   - **Emotional tone**: enthusiastic/calm/nervous/authoritative/etc. (emotional baseline)\r\n   - **Catchphrases**: verbal tics, repeated phrases, language habits (if clearly identifiable)\r\n   - **Speaking context**: speaking scenarios or topics discussed (helps with matching)\r\n   - **6-8 representative quotes**: original quotes extracted from different parts of the audio\r\n   - **Confidence score**: 0.0-1.0\r\n\r\n**OUTPUT FORMAT** (JSON):\r\n\\`\\`\\`json\r\n{\r\n  \"speakerCount\":",
      "å‘ƒ",
      "å—¯",
      "é‚£ä¸ª",
      "å°±æ˜¯",
      "ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚æ•£æ­©ã«è¡Œãã¾ã—ã‚‡ã†ã€‚",
      "å¤ªæ£’äº†ï¼",
      "çœŸæ˜¯ä»¤äººå°è±¡æ·±åˆ»ã€‚",
      "æ±äº¬",
      "å¾®è½¯",
      "è‰¾ä¸½ä¸",
      "é‡œå±±",
      "ç”°ä¸­ç¾å’²",
      "ã“ã‚“ã«ã¡ã¯ï¼ç”°ä¸­ã§ã™ï¼",
      "å¤§é˜ªã®é£Ÿã¹ç‰©ã¯æœ€é«˜ã§ã™ï¼",
      "ã‚ã£ã¡ã‚ƒç¾å‘³ã—ã„ã‚“ã§ã™ã‚ˆã€‚",
      "ç§ã‚‚åŒã˜ã“ã¨æ€ã£ã¦ã¾ã—ãŸã€‚",
      "**IF SPLITTING IS NEEDED**, follow these rules exactly:\r\nâ†’ **PRESERVE ORIGINAL BOUNDARIES**: The FIRST split segment starts at original start time, the LAST split segment ends at original end time\r\nâ†’ **NO GAPS, NO OVERLAPS**: Split segments must be perfectly continuous (segment N end = segment N+1 start)\r\nâ†’ **LISTEN TO AUDIO FOR SPLIT TIMING**: Do NOT allocate time proportionally by text length. Instead:\r\n   1. Listen to the actual audio to hear when each phrase/sentence is spoken\r\n   2. Identify natural pauses, breath breaks, or sentence transitions in the audio\r\n   3. Set split timestamps based on ACTUAL speech timing in the audio\r\n   Example (English): Original segment 00:00:00,000 â†’ 00:00:06,000, text \"Hello world. How are you today?\"\r\n   - If speaker says \"Hello world.\" quickly (1.5s) then pauses, then speaks slowly:\r\n   - Split 1: \"Hello world.\" â†’ 00:00:00,000 â†’ 00:00:01,500 (based on actual audio)\r\n   - Split 2: \"How are you today?\" â†’ 00:00:01,500 â†’ 00:00:06,000 (based on actual audio)\r\n   Example (Japanese): Original segment 00:00:00,000 â†’ 00:00:05,000, text \"ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚æ•£æ­©ã«è¡Œãã¾ã—ã‚‡ã†ã€‚\"\r\n   - If speaker says \"ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚\" slowly (3s), then quickly says the rest:\r\n   - Split 1: \"ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚\" â†’ 00:00:00,000 â†’ 00:00:03,000 (based on actual audio)\r\n   - Split 2: \"æ•£æ­©ã«è¡Œãã¾ã—ã‚‡ã†ã€‚\" â†’ 00:00:03,000 â†’ 00:00:05,000 (based on actual audio)\r\nâ†’ **NATURAL BREAKS ONLY**: Split at sentence boundaries, punctuation, or natural pauses - NEVER mid-word or mid-phrase\r\nâ†’ **MINIMUM DURATION**: Each split segment must be at least 0.5 seconds",
      "You are an expert Subtitle Translator specializing in ${genre} content.\r\n    Your GOAL is to provide fluent, natural Simplified Chinese (zh-CN) translations while strictly preserving the subtitle structure.\r\n\r\n    TASK RULES (Strict Priority):\r\n\r\n    [P0 - STRUCTURAL INTEGRITY]\r\n    â†’ **ONE INPUT = ONE OUTPUT**: You must return exactly one translated subtitle for every input subtitle.\r\n    â†’ **ID PRESERVATION**: Maintain the \"id\" field exactly as provided.\r\n    â†’ **NO MERGING/SPLITTING**: Do not combine multiple lines or split a single line.\r\n    â†’ **TIMESTAMPS**: Do not modify timestamps.\r\n\r\n    [P1 - TRANSLATION QUALITY]\r\n    â†’ **FLUENCY**: Translate into natural, written Chinese, not \"translationese\".\r\n    â†’ **CONTEXT AWARENESS**: Use the provided genre context to determine tone and style.\r\n    â†’ **COMPLETENESS**: Ensure every meaningful part of the original text is represented.\r\n    â†’ **NO HALLUCINATIONS**: Do not invent information not present in the source.\r\n    â†’ **MULTI-LINE CONTEXT**: Read the previous and next 1-2 lines to understand context. This helps with:\r\n       - Resolving ambiguous pronouns (e.g., \"it\", \"that\")\r\n       - Understanding incomplete sentences split across lines\r\n       - Maintaining consistent tone and terminology across related lines\r\n    â†’ **STRICT BOUNDARY RULE**: Use this context for **UNDERSTANDING** only. **NEVER** merge segments or move text between lines.\r\n    â†’ **PARTIAL SENTENCES**: If a sentence is split across lines, translate ONLY the specific fragment in the current line. Do not \"complete\" it using text from the next line.${getSearchEnhancedTranslationPrompt('translation')}\r\n\r\n    [P2 - CLEANUP & REFINEMENT]\r\n    â†’ **REMOVE FILLERS**: Ignore stuttering, hesitation, and meaningless fillers (e.g., \"uh\", \"um\", \"ah\", \"eto\", \"ano\", \"å‘ƒ\", \"é‚£ä¸ª\").\r\n    â†’ **CONCISENESS**: Keep subtitles concise and easy to read quickly.\r\n\r\n    [P3 - TERMINOLOGY]\r\n    â†’ **GLOSSARY**: Strictly follow the provided glossary for specific terms.\r\n    â†’ **CONSISTENCY**: Maintain consistent terminology for names and places.\r\n\r\n    OUTPUT REQUIREMENTS:\r\n    âœ“ Valid JSON matching input structure\r\n    âœ“ Output count MUST match input count exactly\r\n    âœ“ All 'text_translated' fields must be Simplified Chinese\r\n\r\n    FINAL QUALITY CHECK:\r\n    Before returning, verify:\r\n    âœ“ Did I return the exact same number of items as the input?\r\n    âœ“ Are all IDs preserved?\r\n    âœ“ Is the Chinese fluent and natural?\r\n    âœ“ Did I remove all filler words?\r\n\r\n    ${genreContext}${customPrompt ?",
      ")\r\n  .join('')}\r\n**TRANSLATION STYLE EXAMPLES**:\r\n- formal style â†’ polite/literary expressions\r\n- casual style â†’ colloquial expressions\r\n- technical vocabulary â†’ preserve domain terms\r\n\r\nExample:\r\nSpeaker (casual): \"ã™ã”ã„ï¼\" â†’ \"å¤ªæ£’äº†ï¼\"\r\nSpeaker (formal): \"ã™ã”ã„ã§ã™ã­\" â†’ \"çœŸæ˜¯ä»¤äººå°è±¡æ·±åˆ»ã€‚\"",
      "TERMINOLOGY EXTRACTION TASK\r\nGenre Context: ${genre}\r\n\r\nTASK: Extract key terminology from the audio that requires consistent translation across subtitles.\r\n\r\nRULES (Priority Order):\r\n\r\n[P0 - CRITICAL] Language Detection and Matching\r\nâ†’ **FIRST**: Detect the PRIMARY LANGUAGE spoken in this audio segment\r\nâ†’ **SECOND**: Extract ONLY terms that are spoken in that detected language\r\nâ†’ **ABSOLUTE RULE**: DO NOT extract terms in other languages\r\nâ†’ Examples for Japanese audio:\r\n  â€¢ If speaker says \"é‡œå±±\" (Busan), extract \"é‡œå±±\" NOT \"Busan\"\r\n  â€¢ If speaker says \"ã‚¹ã‚«ã‚¤ã‚¹ã‚­ãƒ£ãƒŠãƒ¼\", extract \"ã‚¹ã‚«ã‚¤ã‚¹ã‚­ãƒ£ãƒŠãƒ¼\" NOT \"Skyscanner\"\r\nâ†’ Examples for English audio:\r\n  â€¢ If speaker says \"Tokyo\", extract \"Tokyo\" NOT \"æ±äº¬\"\r\n  â€¢ If speaker says \"Microsoft\", extract \"Microsoft\" NOT \"å¾®è½¯\"\r\nâ†’ **CRITICAL**: Listen to what is ACTUALLY SAID, not what you think the translation equivalent is\r\n\r\n[P1 - EXTRACTION] Identify Key Terms in Audio Language\r\nâ†’ Listen carefully to the audio\r\nâ†’ Extract names (people, characters, organizations) AS SPOKEN\r\nâ†’ Extract places (locations, venues, regions) AS SPOKEN\r\nâ†’ Extract specialized terms (technical jargon, domain-specific vocabulary) AS SPOKEN\r\nâ†’ Extract recurring phrases that need consistent translation\r\nâ†’ ONLY include terms that ACTUALLY APPEAR in this audio segment\r\n\r\n[P2 - TRANSLATION] Provide Accurate Translations\r\nâ†’ Translate all terms to Simplified Chinese\r\nâ†’ For names: Use standard transliterations (e.g., \"Alice\" â†’ \"è‰¾ä¸½ä¸\", \"é‡œå±±\" â†’ \"é‡œå±±\")\r\nâ†’ For technical terms: Use established industry translations\r\nâ†’ Use Google Search to verify standard translations when uncertain\r\nâ†’ Ensure translations are appropriate for ${genre} content\r\n\r\n[P3 - ANNOTATION] Add Context When Needed\r\nâ†’ Add notes for ambiguous terms or pronunciation guidance\r\nâ†’ Include context that helps maintain translation consistency\r\nâ†’ Note any special handling requirements\r\n\r\n[P4 - QUALITY] Focus on Consistency-Critical Terms\r\nâ†’ Prioritize terms that will appear multiple times\r\nâ†’ Skip common words that don't need special handling\r\nâ†’ Focus on terms where inconsistent translation would be problematic\r\n\r\nOUTPUT FORMAT:\r\nâ†’ JSON array of objects\r\nâ†’ Each object: {term: string, translation: string, notes?: string}\r\nâ†’ \"term\" field MUST be in the ORIGINAL LANGUAGE spoken in audio\r\nâ†’ Return empty array [] if no significant terms found\r\n\r\nFINAL VERIFICATION:\r\nâœ“ Detected audio language correctly\r\nâœ“ All \"term\" values are in the SAME LANGUAGE as the audio (NOT English unless audio is English)\r\nâœ“ All extracted terms actually appear in the audio AS SPOKEN\r\nâœ“ All translations are in Simplified Chinese\r\nâœ“ Translations verified with search when needed\r\nâœ“ Only included terms that need consistent translation\r\nâœ“ Notes added where helpful for consistency",
      "json\r\n{\r\n  \"speakerCount\": <integer>,\r\n  \"profiles\": [\r\n    {\r\n      \"id\": \"Speaker 1\",\r\n      \"characteristics\": {\r\n        \"name\": \"<name if mentioned, in source language (e.g., 'ç”°ä¸­' not 'Tanaka')>\",\r\n        \"gender\": \"male\" | \"female\" | \"unknown\",\r\n        \"pitch\": \"low\" | \"medium\" | \"high\",\r\n        \"speed\": \"slow\" | \"normal\" | \"fast\",\r\n        \"accent\": \"<English description>\",\r\n        \"tone\": \"<English description, e.g., calm, energetic>\"\r\n      },\r\n      \"inferredIdentity\": \"<role/name if identifiable>\",\r\n      \"speakingStyle\": {\r\n        \"formality\": \"formal\" | \"casual\" | \"mixed\",\r\n        \"vocabulary\": \"<description, e.g., technical, colloquial>\",\r\n        \"sentenceStructure\": \"<description, e.g., complex, simple>\"\r\n      },\r\n      \"emotionalTone\": \"<description, e.g., enthusiastic, calm>\",\r\n      \"catchphrases\": [\"<phrase 1>\", \"<phrase 2>\"],\r\n      \"speakingContext\": [\"<context 1>\", \"<context 2>\"],\r\n      \"sampleQuotes\": [\"<quote 1>\", \"<quote 2>\", ..., \"<quote 6-8>\"],\r\n      \"confidence\": <0.0-1.0>\r\n    }\r\n  ]\r\n}\r\n\\",
      "json\r\n{\r\n  \"speakerCount\": 2,\r\n  \"profiles\": [\r\n    {\r\n      \"id\": \"Speaker 1\",\r\n      \"characteristics\": {\r\n        \"name\": \"John\",\r\n        \"gender\": \"male\",\r\n        \"pitch\": \"low\",\r\n        \"speed\": \"fast\",\r\n        \"accent\": \"American English\",\r\n        \"tone\": \"Professional, Authoritative\"\r\n      },\r\n      \"inferredIdentity\": \"News Anchor / Host\",\r\n      \"speakingStyle\": {\r\n        \"formality\": \"formal\",\r\n        \"vocabulary\": \"professional broadcast terminology\",\r\n        \"sentenceStructure\": \"complex, well-structured\"\r\n      },\r\n      \"emotionalTone\": \"calm, confident, authoritative\",\r\n      \"catchphrases\": [\"Welcome to the show\", \"Let's dive into\"],\r\n      \"speakingContext\": [\"introducing topics\", \"asking questions\", \"transitioning segments\"],\r\n      \"sampleQuotes\": [\"Welcome to the show tonight.\", \"Let's bring in our guest.\", \"That's an excellent point.\", \"We'll be right back after this.\", \"Thank you for watching.\", \"Let's dive into today's headlines.\"],\r\n      \"confidence\": 0.95\r\n    },\r\n    {\r\n      \"id\": \"Speaker 2\",\r\n      \"characteristics\": {\r\n        \"name\": \"ç”°ä¸­ç¾å’²\",\r\n        \"gender\": \"female\",\r\n        \"pitch\": \"high\",\r\n        \"speed\": \"normal\",\r\n        \"accent\": \"Japanese (Kansai dialect)\",\r\n        \"tone\": \"Energetic, Friendly\"\r\n      },\r\n      \"inferredIdentity\": \"Guest / Expert\",\r\n      \"speakingStyle\": {\r\n        \"formality\": \"casual\",\r\n        \"vocabulary\": \"colloquial, regional expressions\",\r\n        \"sentenceStructure\": \"simple, conversational\"\r\n      },\r\n      \"emotionalTone\": \"enthusiastic, warm, expressive\",\r\n      \"catchphrases\": [\"ã»ã‚“ã¾ã«\", \"ã‚ã£ã¡ã‚ƒ\"],\r\n      \"speakingContext\": [\"discussing food culture\", \"sharing personal experiences\", \"responding to questions\"],\r\n      \"sampleQuotes\": [\"ã“ã‚“ã«ã¡ã¯ï¼ç”°ä¸­ã§ã™ï¼\", \"å¤§é˜ªã®é£Ÿã¹ç‰©ã¯æœ€é«˜ã§ã™ï¼\", \"ã»ã‚“ã¾ã«ãã†ã§ã™ã­ï¼\", \"ã‚ã£ã¡ã‚ƒç¾å‘³ã—ã„ã‚“ã§ã™ã‚ˆã€‚\", \"ç§ã‚‚åŒã˜ã“ã¨æ€ã£ã¦ã¾ã—ãŸã€‚\", \"ã“ã‚Œã¯ãœã²è©¦ã—ã¦ã»ã—ã„ï¼\"],\r\n      \"confidence\": 0.88\r\n    }\r\n  ]\r\n}\r\n\\"
    ],
    "src\\services\\api\\gemini\\core\\client.ts": [
      "[] = [\r\n            ai.models.generateContent(params).then((res) => {\r\n              if (timeoutHandle) clearTimeout(timeoutHandle);\r\n              if (abortHandler && signal) signal.removeEventListener('abort', abortHandler);\r\n              return res;\r\n            }),\r\n          ];\r\n\r\n          \r\n          promises.push(\r\n            new Promise((_, reject) => {\r\n              timeoutHandle = setTimeout(\r\n                () => reject(new Error(`è¯·æ±‚è¶…æ—¶ (${Math.round(timeoutMs / 1000)}ç§’)`)),\r\n                timeoutMs\r\n              );\r\n            })\r\n          );\r\n\r\n          \r\n          if (signal) {\r\n            promises.push(\r\n              new Promise((_, reject) => {\r\n                if (signal.aborted) {\r\n                  reject(new Error('Operation cancelled'));\r\n                } else {\r\n                  abortHandler = () => reject(new Error('Operation cancelled'));\r\n                  signal.addEventListener('abort', abortHandler);\r\n                }\r\n              })\r\n            );\r\n          }\r\n\r\n          result = await Promise.race(promises);\r\n        } catch (error) {\r\n          if (timeoutHandle) clearTimeout(timeoutHandle);\r\n          if (abortHandler && signal) signal.removeEventListener('abort', abortHandler);\r\n          throw error;\r\n        }\r\n      } else {\r\n        result = await ai.models.generateContent(params);\r\n      }\r\n\r\n      const candidates = (result as any).candidates;\r\n\r\n      \r\n      if ((result as any).usageMetadata) {\r\n        const usageMeta = (result as any).usageMetadata;\r\n\r\n        \r\n        if (onUsage) {\r\n          \r\n          let textInputTokens = 0;\r\n          let audioInputTokens = 0;\r\n          let cachedTokens = usageMeta.cachedContentTokenCount || 0;\r\n          const thoughtsTokens = usageMeta.thoughtsTokenCount || 0;\r\n\r\n          if (usageMeta.promptTokensDetails && Array.isArray(usageMeta.promptTokensDetails)) {\r\n            for (const detail of usageMeta.promptTokensDetails) {\r\n              if (detail.modality === 'TEXT') {\r\n                textInputTokens = detail.tokenCount || 0;\r\n              } else if (detail.modality === 'AUDIO') {\r\n                audioInputTokens = detail.tokenCount || 0;\r\n              }\r\n            }\r\n          }\r\n\r\n          onUsage({\r\n            promptTokens: usageMeta.promptTokenCount || 0,\r\n            candidatesTokens: usageMeta.candidatesTokenCount || 0,\r\n            totalTokens: usageMeta.totalTokenCount || 0,\r\n            modelName: params.model || 'unknown-model',\r\n            textInputTokens,\r\n            audioInputTokens,\r\n            thoughtsTokens,\r\n            cachedTokens,\r\n          });\r\n        }\r\n\r\n        \r\n        const sanitizeValue = (value: any): any => {\r\n          if (!value) return value;\r\n          if (Array.isArray(value)) return value.map(sanitizeValue);\r\n          if (typeof value === 'object') {\r\n            \r\n            if ('inlineData' in value && value.inlineData?.data) {\r\n              return {\r\n                ...value,\r\n                inlineData: {\r\n                  ...value.inlineData,\r\n                  data: '",
      "setTimeout(r, delay));\r\n      } else {\r\n        throw e;\r\n      }\r\n    }\r\n  }\r\n  throw new Error('Gemini API è¯·æ±‚é‡è¯•åä»ç„¶å¤±è´¥ã€‚');\r\n}\r\n\r\nexport async function generateContentWithLongOutput(\r\n  ai: GoogleGenAI,\r\n  modelName: string,\r\n  systemInstruction: string,\r\n  parts: Part[],\r\n  schema: any,\r\n  tools?: any[],\r\n  signal?: AbortSignal,\r\n  onUsage?: (usage: TokenUsage) => void,\r\n  timeoutMs?: number \r\n): Promise",
      "API å¯†é’¥æ— æ•ˆï¼è¯·æ£€æŸ¥æ‚¨çš„ API å¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®ã€‚",
      "å¯ç”¨ç»“ç®—",
      "å…è´¹å±‚çº§",
      "Gemini API å…è´¹å±‚çº§ä¸å¯ç”¨ï¼è¯·åœ¨ Google AI Studio ä¸­ä¸ºæ‚¨çš„é¡¹ç›®å¯ç”¨ç»“ç®—åŠŸèƒ½ã€‚",
      "API è®¿é—®è¢«æ‹’ç» (403)ï¼è¯·æ£€æŸ¥ï¼š1) API å¯†é’¥æƒé™è®¾ç½® 2) API æ˜¯å¦ä¸ºå½“å‰åœ°åŒºå¯ç”¨ 3) æ˜¯å¦éœ€è¦å¼€å¯è®¡è´¹",
      "API é…é¢å·²ç”¨å°½æˆ–è¯·æ±‚è¿‡äºé¢‘ç¹ (429)ï¼è¯·ç¨åé‡è¯•ï¼Œæˆ–æ£€æŸ¥æ‚¨çš„ API é…é¢é™åˆ¶ã€‚",
      "è¯·æ±‚çš„æ¨¡å‹ä¸å­˜åœ¨ (404)ï¼è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚",
      "è¯·æ±‚çš„èµ„æºä¸å­˜åœ¨ (404)ï¼è¯·æ£€æŸ¥è¯·æ±‚å‚æ•°ã€‚",
      "å›½å®¶/åœ°åŒº",
      "API åœ¨å½“å‰åœ°åŒºä¸å¯ç”¨ï¼è¯·æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨ä»£ç†æˆ–æ›´æ¢ API ç«¯ç‚¹ã€‚",
      "æ“ä½œå·²å–æ¶ˆ",
      "Gemini API è¯·æ±‚é‡è¯•åä»ç„¶å¤±è´¥ã€‚",
      ";\r\n\r\n  \r\n  \r\n  if (\r\n    reason === 'api_key_invalid' ||\r\n    combinedMsg.includes('api key not valid') ||\r\n    combinedMsg.includes('invalid api key') ||\r\n    httpStatus === 401 ||\r\n    combinedMsg.includes('unauthorized')\r\n  ) {\r\n    return 'API å¯†é’¥æ— æ•ˆï¼è¯·æ£€æŸ¥æ‚¨çš„ API å¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®ã€‚';\r\n  }\r\n\r\n  \r\n  \r\n  if (\r\n    errorStatus === 'failed_precondition' ||\r\n    combinedMsg.includes('enable billing') ||\r\n    combinedMsg.includes('å¯ç”¨ç»“ç®—') ||\r\n    combinedMsg.includes('free tier') ||\r\n    combinedMsg.includes('å…è´¹å±‚çº§')\r\n  ) {\r\n    return 'Gemini API å…è´¹å±‚çº§ä¸å¯ç”¨ï¼è¯·åœ¨ Google AI Studio ä¸­ä¸ºæ‚¨çš„é¡¹ç›®å¯ç”¨ç»“ç®—åŠŸèƒ½ã€‚';\r\n  }\r\n\r\n  \r\n  if (\r\n    httpStatus === 403 ||\r\n    errorStatus === 'permission_denied' ||\r\n    combinedMsg.includes('permission denied') ||\r\n    combinedMsg.includes('forbidden') ||\r\n    combinedMsg.includes('access denied')\r\n  ) {\r\n    return 'API è®¿é—®è¢«æ‹’ç» (403)ï¼è¯·æ£€æŸ¥ï¼š1) API å¯†é’¥æƒé™è®¾ç½® 2) API æ˜¯å¦ä¸ºå½“å‰åœ°åŒºå¯ç”¨ 3) æ˜¯å¦éœ€è¦å¼€å¯è®¡è´¹';\r\n  }\r\n\r\n  \r\n  if (\r\n    httpStatus === 429 ||\r\n    errorStatus === 'resource_exhausted' ||\r\n    combinedMsg.includes('quota') ||\r\n    combinedMsg.includes('rate limit') ||\r\n    combinedMsg.includes('too many requests')\r\n  ) {\r\n    return 'API é…é¢å·²ç”¨å°½æˆ–è¯·æ±‚è¿‡äºé¢‘ç¹ (429)ï¼è¯·ç¨åé‡è¯•ï¼Œæˆ–æ£€æŸ¥æ‚¨çš„ API é…é¢é™åˆ¶ã€‚';\r\n  }\r\n\r\n  \r\n  if (httpStatus === 404 || errorStatus === 'not_found') {\r\n    if (combinedMsg.includes('model')) {\r\n      return 'è¯·æ±‚çš„æ¨¡å‹ä¸å­˜åœ¨ (404)ï¼è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚';\r\n    }\r\n    return 'è¯·æ±‚çš„èµ„æºä¸å­˜åœ¨ (404)ï¼è¯·æ£€æŸ¥è¯·æ±‚å‚æ•°ã€‚';\r\n  }\r\n\r\n  \r\n  if (\r\n    combinedMsg.includes('region') ||\r\n    combinedMsg.includes('location') ||\r\n    combinedMsg.includes('not available in') ||\r\n    combinedMsg.includes('å›½å®¶/åœ°åŒº')\r\n  ) {\r\n    return 'API åœ¨å½“å‰åœ°åŒºä¸å¯ç”¨ï¼è¯·æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨ä»£ç†æˆ–æ›´æ¢ API ç«¯ç‚¹ã€‚';\r\n  }\r\n\r\n  \r\n  return undefined;\r\n}\r\n\r\nexport function isRetryableError(error: any): boolean {\r\n  if (!error) return false;\r\n\r\n  const status = error.status || error.response?.status;\r\n  const msg = error.message || '';\r\n  const code = error.code || '';\r\n\r\n  \r\n  if (\r\n    code === 'ETIMEDOUT' ||\r\n    code === 'ECONNABORTED' ||\r\n    code === 'ENOTFOUND' || \r\n    msg.includes('timeout') ||\r\n    msg.includes('timed out') ||\r\n    msg.toLowerCase().includes('timeout')\r\n  ) {\r\n    return true;\r\n  }\r\n\r\n  \r\n  if (status === 429 || msg.includes('429') || msg.includes('Resource has been exhausted')) {\r\n    return true;\r\n  }\r\n\r\n  \r\n  if (status === 503 || status === 500 || msg.includes('503') || msg.includes('Overloaded')) {\r\n    return true;\r\n  }\r\n\r\n  \r\n  const msgLower = msg.toLowerCase();\r\n  if (\r\n    msgLower.includes('fetch failed') ||\r\n    msgLower.includes('failed to fetch') ||\r\n    msgLower.includes('network') ||\r\n    msgLower.includes('econnrefused') ||\r\n    msgLower.includes('econnreset') ||\r\n    msgLower.includes('err_network')\r\n  ) {\r\n    return true;\r\n  }\r\n\r\n  \r\n  if (msg.includes('JSON') || msg.includes('SyntaxError')) {\r\n    return true;\r\n  }\r\n\r\n  return false;\r\n}\r\n\r\nexport async function generateContentWithRetry<T = any>(\r\n  ai: GoogleGenAI,\r\n  params: any,\r\n  retries = 3,\r\n  signal?: AbortSignal,\r\n  onUsage?: (usage: TokenUsage) => void,\r\n  timeoutMs?: number, \r\n  parseJson?: 'array' | 'object' | false \r\n): Promise<T> {\r\n  for (let i = 0; i < retries; i++) {\r\n    \r\n    if (signal?.aborted) {\r\n      throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n    }\r\n\r\n    try {\r\n      \r\n      let result;\r\n      if (timeoutMs && timeoutMs > 0) {\r\n        let timeoutHandle: NodeJS.Timeout | null = null;\r\n        let abortHandler: (() => void) | null = null;\r\n\r\n        try {\r\n          const promises: Promise<any>[] = [\r\n            ai.models.generateContent(params).then((res) => {\r\n              if (timeoutHandle) clearTimeout(timeoutHandle);\r\n              if (abortHandler && signal) signal.removeEventListener('abort', abortHandler);\r\n              return res;\r\n            }),\r\n          ];\r\n\r\n          \r\n          promises.push(\r\n            new Promise((_, reject) => {\r\n              timeoutHandle = setTimeout(\r\n                () => reject(new Error(",
      ", {\r\n          attempt: i + 1,\r\n          maxRetries: retries,\r\n          error: e.message,\r\n          status: e.status,\r\n        });\r\n        await new Promise((r) => setTimeout(r, delay));\r\n      } else {\r\n        throw e;\r\n      }\r\n    }\r\n  }\r\n  throw new Error('Gemini API è¯·æ±‚é‡è¯•åä»ç„¶å¤±è´¥ã€‚');\r\n}\r\n\r\nexport async function generateContentWithLongOutput(\r\n  ai: GoogleGenAI,\r\n  modelName: string,\r\n  systemInstruction: string,\r\n  parts: Part[],\r\n  schema: any,\r\n  tools?: any[],\r\n  signal?: AbortSignal,\r\n  onUsage?: (usage: TokenUsage) => void,\r\n  timeoutMs?: number \r\n): Promise<string> {\r\n  let fullText = '';\r\n\r\n  \r\n  \r\n  let messages: Content[] = [{ role: 'user', parts: parts }];\r\n\r\n  try {\r\n    \r\n    if (signal?.aborted) {\r\n      throw new Error('Operation cancelled');\r\n    }\r\n\r\n    \r\n    logger.debug(",
      ",\r\n            { error: e, partialText: fullText.slice(-500) } \r\n          );\r\n        }\r\n      }\r\n\r\n      \r\n      \r\n      \r\n      \r\n\r\n      if (signal?.aborted) {\r\n        throw new Error('æ“ä½œå·²å–æ¶ˆ');\r\n      }\r\n\r\n      messages.push({ role: 'model', parts: [{ text: text }] });\r\n      messages.push({\r\n        role: 'user',\r\n        parts: [\r\n          { text: 'The response was truncated. Please continue exactly where you left off.' },\r\n        ],\r\n      });\r\n\r\n      response = await generateContentWithRetry(\r\n        ai,\r\n        {\r\n          model: modelName,\r\n          contents: messages,\r\n          config: {\r\n            responseMimeType: 'application/json',\r\n            responseSchema: schema,\r\n            systemInstruction: systemInstruction,\r\n            safetySettings: SAFETY_SETTINGS,\r\n            maxOutputTokens: 65536,\r\n          },\r\n        },\r\n        3,\r\n        signal,\r\n        onUsage,\r\n        timeoutMs\r\n      );\r\n\r\n      text = response.text || '';\r\n      fullText += text;\r\n      attempts++;\r\n    }\r\n\r\n    \r\n    try {\r\n      const clean = cleanJsonMarkdown(fullText);\r\n      const extracted = extractJsonArray(clean);\r\n      const textToParse = extracted || clean;\r\n\r\n      JSON.parse(textToParse);\r\n      logger.debug('Final JSON validation passed');\r\n      return fullText;\r\n    } catch (e) {\r\n      logger.error('Final JSON validation failed after 3 continuation attempts', {\r\n        fullTextLength: fullText.length,\r\n        preview: fullText.substring(0, 1000), \r\n        error: e,\r\n      });\r\n      throw new Error("
    ],
    "src\\hooks\\useSnapshots.ts": ["è‡ªåŠ¨ä¿å­˜"],
    "src\\hooks\\useEndToEndSubtitleGeneration.ts": [
      "=> {\r\n      \r\n      if (isProcessingRef.current) {\r\n        logger.warn('[EndToEnd] Already processing, rejecting new request');\r\n        return { success: false, error: 'å·²æœ‰ä»»åŠ¡åœ¨å¤„ç†ä¸­', errorCode: 'BUSY' };\r\n      }\r\n\r\n      \r\n      if (!audioPath || typeof audioPath !== 'string') {\r\n        logger.error('[EndToEnd] Invalid audio path', { audioPath });\r\n        return { success: false, error: 'æ— æ•ˆçš„éŸ³é¢‘è·¯å¾„', errorCode: 'INVALID_PATH' };\r\n      }\r\n\r\n      isProcessingRef.current = true;\r\n      abortControllerRef.current = new AbortController();\r\n      const signal = abortControllerRef.current.signal;\r\n\r\n      \r\n      const timeoutMs = 30 * 60 * 1000;\r\n      const timeoutId = setTimeout(() => {\r\n        logger.warn('[EndToEnd] Generation timeout, aborting');\r\n        abortControllerRef.current?.abort();\r\n      }, timeoutMs);\r\n\r\n      try {\r\n        logger.info('[EndToEnd] Starting subtitle generation', { audioPath, config });\r\n\r\n        \r\n        const currentSettings = settingsRef.current;\r\n        const hasGeminiKey = currentSettings.geminiKey?.trim() || process.env.VITE_GEMINI_API_KEY;\r\n        const hasOpenAIKey = currentSettings.openaiKey?.trim() || process.env.VITE_OPENAI_API_KEY;\r\n\r\n        if (!hasGeminiKey) {\r\n          return {\r\n            success: false,\r\n            error: 'ç¼ºå°‘ Gemini API å¯†é’¥ï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®',\r\n            errorCode: 'MISSING_API_KEY',\r\n          };\r\n        }\r\n        if (!hasOpenAIKey && !currentSettings.useLocalWhisper) {\r\n          return {\r\n            success: false,\r\n            error: 'ç¼ºå°‘ OpenAI API å¯†é’¥æˆ–æœªé…ç½®æœ¬åœ° Whisper',\r\n            errorCode: 'MISSING_API_KEY',\r\n          };\r\n        }\r\n\r\n        \r\n        let audioFile: File;\r\n        try {\r\n          audioFile = await loadAudioFromPath(audioPath);\r\n          logger.info('[EndToEnd] Audio file loaded', { size: audioFile.size });\r\n        } catch (loadError: any) {\r\n          logger.error('[EndToEnd] Failed to load audio file', loadError);\r\n          return {\r\n            success: false,\r\n            error: `æ— æ³•è¯»å–éŸ³é¢‘æ–‡ä»¶: ${loadError.message}`,\r\n            errorCode: 'FILE_READ_ERROR',\r\n          };\r\n        }\r\n\r\n        \r\n        if (audioFile.size === 0) {\r\n          logger.error('[EndToEnd] Audio file is empty');\r\n          return { success: false, error: 'éŸ³é¢‘æ–‡ä»¶ä¸ºç©º', errorCode: 'EMPTY_FILE' };\r\n        }\r\n\r\n        \r\n        if (audioFile.size",
      "å·²æœ‰ä»»åŠ¡åœ¨å¤„ç†ä¸­",
      "æ— æ•ˆçš„éŸ³é¢‘è·¯å¾„",
      "ç¼ºå°‘ Gemini API å¯†é’¥ï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®",
      "ç¼ºå°‘ OpenAI API å¯†é’¥æˆ–æœªé…ç½®æœ¬åœ° Whisper",
      "éŸ³é¢‘æ–‡ä»¶ä¸ºç©º",
      "éŸ³é¢‘æ–‡ä»¶è¿‡å°ï¼Œå¯èƒ½å·²æŸå",
      "éŸ³é¢‘æ—¶é•¿è¿‡çŸ­ï¼ˆå°‘äº1ç§’ï¼‰",
      "éŸ³é¢‘æ—¶é•¿è¿‡é•¿ï¼ˆè¶…è¿‡6å°æ—¶ï¼‰",
      "æ“ä½œå·²å–æ¶ˆ",
      "æœªç”Ÿæˆä»»ä½•å­—å¹•ï¼ŒéŸ³é¢‘å¯èƒ½æ— è¯­éŸ³å†…å®¹",
      "å¯†é’¥",
      "API è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•",
      "è¶…æ—¶",
      "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥",
      "å­—å¹•ç”Ÿæˆå¤±è´¥",
      "æ— æ³•è¯»å–éŸ³é¢‘æ–‡ä»¶: ${loadError.message}",
      "éŸ³é¢‘è§£ç å¤±è´¥: ${decodeError.message}"
    ],
    "src\\hooks\\useEndToEnd.ts": [
      ") => {\r\n    setState((prev) => ({\r\n      ...prev,\r\n      config: { ...prev.config, ...updates },\r\n    }));\r\n  }, []);\r\n\r\n  const resetConfig = useCallback(() => {\r\n    setState({\r\n      currentStep: 'input',\r\n      config: { ...DEFAULT_CONFIG },\r\n      isParsing: false,\r\n      isExecuting: false,\r\n      videoInfo: undefined,\r\n      progress: undefined,\r\n      result: undefined,\r\n      parseError: undefined,\r\n    });\r\n  }, []);\r\n\r\n  \r\n  const parseUrl = useCallback(\r\n    async (url: string) => {\r\n      if (!isElectron || !window.electronAPI?.download?.parse) {\r\n        throw new Error('æ­¤åŠŸèƒ½ä»…åœ¨æ¡Œé¢ç‰ˆå¯ç”¨');\r\n      }\r\n\r\n      setState((prev) => ({\r\n        ...prev,\r\n        isParsing: true,\r\n        parseError: undefined,\r\n        config: { ...prev.config, url },\r\n      }));\r\n\r\n      \r\n      const PARSE_TIMEOUT_MS = 60000; \r\n\r\n      try {\r\n        const parsePromise = window.electronAPI.download.parse(url);\r\n        const timeoutPromise = new Promise",
      "((_, reject) =>\r\n          setTimeout(() => reject(new Error('è§£æè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•')), PARSE_TIMEOUT_MS)\r\n        );\r\n\r\n        const result = await Promise.race([parsePromise, timeoutPromise]);\r\n\r\n        if (result.success && result.videoInfo) {\r\n          setState((prev) => ({\r\n            ...prev,\r\n            isParsing: false,\r\n            videoInfo: result.videoInfo,\r\n            currentStep: 'config', \r\n          }));\r\n        } else {\r\n          setState((prev) => ({\r\n            ...prev,\r\n            isParsing: false,\r\n            parseError: result.error || 'è§£æè§†é¢‘é“¾æ¥å¤±è´¥',\r\n          }));\r\n        }\r\n      } catch (error: any) {\r\n        setState((prev) => ({\r\n          ...prev,\r\n          isParsing: false,\r\n          parseError: error.message || 'è§£æè§†é¢‘é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯',\r\n        }));\r\n      }\r\n    },\r\n    [isElectron]\r\n  );\r\n\r\n  \r\n  const startPipeline = useCallback(async (): Promise",
      "æ­¤åŠŸèƒ½ä»…åœ¨æ¡Œé¢ç‰ˆå¯ç”¨",
      "è§£æè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•",
      "è§£æè§†é¢‘é“¾æ¥å¤±è´¥",
      "è§£æè§†é¢‘é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯",
      "è¯·è¾“å…¥è§†é¢‘é“¾æ¥",
      "è¯·é€‰æ‹©è¾“å‡ºç›®å½•",
      "æ­£åœ¨åˆå§‹åŒ–...",
      "æ‰§è¡Œå¤±è´¥"
    ],
    "src\\hooks\\useDownload.ts": ["å°é¢ä¸‹è½½å¤±è´¥"],
    "src\\hooks\\useWorkspaceLogic\\useGeneration.ts": [
      ";\r\n}\r\n\r\n\r\nexport function useGeneration({\r\n  file,\r\n  duration,\r\n  settings,\r\n  batchComments,\r\n  setStatus,\r\n  setError,\r\n  setSubtitles,\r\n  setChunkProgress,\r\n  setStartTime,\r\n  setSelectedBatches,\r\n  setBatchComments,\r\n  abortControllerRef,\r\n  audioCacheRef,\r\n  subtitlesRef,\r\n  handleProgress,\r\n  glossaryFlow,\r\n  snapshotsValues,\r\n  addToast,\r\n  setShowSettings,\r\n  updateSetting,\r\n}: UseGenerationProps): UseGenerationReturn {\r\n  const handleGenerate = useCallback(async () => {\r\n    if (!file) {\r\n      setError('è¯·å…ˆä¸Šä¼ åª’ä½“æ–‡ä»¶ã€‚');\r\n      return;\r\n    }\r\n    const hasGemini = !!(settings.geminiKey || ENV.GEMINI_API_KEY);\r\n    const hasOpenAI = !!(settings.openaiKey || ENV.OPENAI_API_KEY);\r\n    const hasLocalWhisper = !!settings.useLocalWhisper;\r\n\r\n    if (!hasGemini || (!hasOpenAI && !hasLocalWhisper)) {\r\n      setError('API å¯†é’¥æœªé…ç½®ï¼Œè¯·åœ¨è®¾ç½®ä¸­æ·»åŠ ã€‚');\r\n      setShowSettings(true);\r\n      return;\r\n    }\r\n    setStatus(GenerationStatus.UPLOADING);\r\n    setError(null);\r\n    setSubtitles([]);\r\n    snapshotsValues.setSnapshots([]);\r\n    setBatchComments({});\r\n    setSelectedBatches(new Set());\r\n    setChunkProgress({});\r\n    setStartTime(Date.now());\r\n    logger.info('Starting subtitle generation', {\r\n      file: file.name,\r\n      duration,\r\n      settings: { ...settings, geminiKey: '***', openaiKey: '***' },\r\n    });\r\n    \r\n    abortControllerRef.current = new AbortController();\r\n    const signal = abortControllerRef.current.signal;\r\n\r\n    try {\r\n      setStatus(GenerationStatus.PROCESSING);\r\n\r\n      \r\n      const runtimeSettings = {\r\n        ...settings,\r\n        glossary: getActiveGlossaryTerms(settings),\r\n      };\r\n\r\n      \r\n      let audioBuffer: AudioBuffer;\r\n      try {\r\n        if (audioCacheRef.current && audioCacheRef.current.file === file) {\r\n          audioBuffer = audioCacheRef.current.buffer;\r\n        } else {\r\n          handleProgress({\r\n            id: 'decoding',\r\n            total: 1,\r\n            status: 'processing',\r\n            message: 'æ­£åœ¨è§£ç éŸ³é¢‘...',\r\n          });\r\n          audioBuffer = await decodeAudioWithRetry(file);\r\n          audioCacheRef.current = { file, buffer: audioBuffer };\r\n        }\r\n      } catch (e) {\r\n        logger.error('Failed to decode audio in handleGenerate', e);\r\n        throw new Error('éŸ³é¢‘è§£ç å¤±è´¥ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„è§†é¢‘æˆ–éŸ³é¢‘æ ¼å¼ã€‚');\r\n      }\r\n\r\n      const { subtitles: result } = await generateSubtitles(\r\n        audioBuffer,\r\n        duration,\r\n        runtimeSettings,\r\n        handleProgress,\r\n        (newSubs) => setSubtitles(newSubs),\r\n        \r\n        async (metadata: GlossaryExtractionMetadata) => {\r\n          logger.info('onGlossaryReady called with metadata:', metadata);\r\n\r\n          if (settings.glossaryAutoConfirm && !metadata.hasFailures) {\r\n            const result = autoConfirmGlossaryTerms({\r\n              metadata,\r\n              settings,\r\n              updateSetting,\r\n              logPrefix: '[Workspace]',\r\n            });\r\n            return result.terms;\r\n          }\r\n\r\n          \r\n          return new Promise",
      "è¯·å…ˆä¸Šä¼ åª’ä½“æ–‡ä»¶ã€‚",
      "API å¯†é’¥æœªé…ç½®ï¼Œè¯·åœ¨è®¾ç½®ä¸­æ·»åŠ ã€‚",
      "æ­£åœ¨è§£ç éŸ³é¢‘...",
      "éŸ³é¢‘è§£ç å¤±è´¥ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„è§†é¢‘æˆ–éŸ³é¢‘æ ¼å¼ã€‚",
      "æœªç”Ÿæˆä»»ä½•å­—å¹•ã€‚",
      "åˆå§‹ç”Ÿæˆ",
      "å­—å¹•ç”ŸæˆæˆåŠŸï¼",
      "éƒ¨åˆ†ç”Ÿæˆ (å·²ç»ˆæ­¢)",
      "ç”Ÿæˆå·²ç»ˆæ­¢ï¼Œä¿ç•™éƒ¨åˆ†ç»“æœ",
      "ç”Ÿæˆå·²ç»ˆæ­¢",
      "ç”Ÿæˆå¤±è´¥ï¼š${error.message}"
    ],
    "src\\hooks\\useWorkspaceLogic\\useFileOperations.ts": [
      ", _activeTab: 'new' | 'import') => {\r\n      if (e.target.files && e.target.files[0]) {\r\n        const selectedFile = e.target.files[0];\r\n\r\n        \r\n        if (file && subtitles.length > 0 && status === GenerationStatus.COMPLETED) {\r\n          showConfirm(\r\n            'ç¡®è®¤æ›¿æ¢æ–‡ä»¶',\r\n            'æ›¿æ¢æ–‡ä»¶åå°†æ¸…ç©ºå½“å‰å­—å¹•ã€‚å»ºè®®å…ˆå¯¼å‡ºå­—å¹•ï¼ˆSRT/ASSï¼‰å†æ“ä½œã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ',\r\n            async () => {\r\n              setSubtitles([]);\r\n              setStatus(GenerationStatus.IDLE);\r\n              snapshotsValues.setSnapshots([]);\r\n              setBatchComments({});\r\n              await processFileInternal(selectedFile);\r\n            },\r\n            'warning'\r\n          );\r\n        } else {\r\n          await processFileInternal(selectedFile);\r\n        }\r\n      }\r\n    },\r\n    [\r\n      file,\r\n      subtitles.length,\r\n      status,\r\n      snapshotsValues,\r\n      showConfirm,\r\n      processFileInternal,\r\n      setSubtitles,\r\n      setStatus,\r\n      setBatchComments,\r\n    ]\r\n  );\r\n\r\n  const handleFileSelectNative = useCallback(\r\n    async (fileStub: File & { path?: string; _needsRead?: boolean }) => {\r\n      \r\n      const readAndProcessFile = async () => {\r\n        const filePath = fileStub.path || window.electronAPI?.getFilePath?.(fileStub);\r\n        if (!filePath) {\r\n          \r\n          await processFileInternal(fileStub);\r\n          return;\r\n        }\r\n\r\n        const currentOpId = ++operationIdRef.current;\r\n        setIsLoadingFile(true);\r\n        \r\n        await new Promise((resolve) => setTimeout(resolve, 50));\r\n\r\n        if (currentOpId !== operationIdRef.current) return;\r\n\r\n        try {\r\n          \r\n          \r\n          \r\n          const fileObj = new File([], fileStub.name, {\r\n            type: fileStub.type || 'application/octet-stream',\r\n          });\r\n\r\n          \r\n          Object.defineProperty(fileObj, 'path', {\r\n            value: filePath,\r\n            writable: false,\r\n            enumerable: false,\r\n            configurable: false,\r\n          });\r\n          \r\n          Object.defineProperty(fileObj, 'size', {\r\n            value: fileStub.size || 0,\r\n            writable: false,\r\n            enumerable: true,\r\n            configurable: false,\r\n          });\r\n\r\n          if (currentOpId !== operationIdRef.current) return;\r\n\r\n          \r\n          await processFileInternal(fileObj);\r\n        } catch (err) {\r\n          if (currentOpId !== operationIdRef.current) return;\r\n          logger.error('Failed to process file', err);\r\n          setError('æ–‡ä»¶å¤„ç†å¤±è´¥');\r\n          setIsLoadingFile(false);\r\n        }\r\n      };\r\n\r\n      \r\n      if (file && subtitles.length > 0 && status === GenerationStatus.COMPLETED) {\r\n        showConfirm(\r\n          'ç¡®è®¤æ›¿æ¢æ–‡ä»¶',\r\n          'æ›¿æ¢æ–‡ä»¶åå°†æ¸…ç©ºå½“å‰å­—å¹•ã€‚å»ºè®®å…ˆå¯¼å‡ºå­—å¹•ï¼ˆSRT/ASSï¼‰å†æ“ä½œã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ',\r\n          async () => {\r\n            setSubtitles([]);\r\n            setStatus(GenerationStatus.IDLE);\r\n            snapshotsValues.setSnapshots([]);\r\n            setBatchComments({});\r\n            await readAndProcessFile();\r\n          },\r\n          'warning'\r\n        );\r\n      } else {\r\n        await readAndProcessFile();\r\n      }\r\n    },\r\n    [\r\n      file,\r\n      subtitles.length,\r\n      status,\r\n      snapshotsValues,\r\n      showConfirm,\r\n      processFileInternal,\r\n      setSubtitles,\r\n      setStatus,\r\n      setBatchComments,\r\n      setError,\r\n    ]\r\n  );\r\n\r\n  const handleSubtitleImport = useCallback(\r\n    async (e: React.ChangeEvent",
      "ç¡®è®¤æ›¿æ¢æ–‡ä»¶",
      "æ›¿æ¢æ–‡ä»¶åå°†æ¸…ç©ºå½“å‰å­—å¹•ã€‚å»ºè®®å…ˆå¯¼å‡ºå­—å¹•ï¼ˆSRT/ASSï¼‰å†æ“ä½œã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ",
      "æ–‡ä»¶å¤„ç†å¤±è´¥",
      "åˆå§‹å¯¼å…¥",
      "å­—å¹•è§£æå¤±è´¥: ${errorMessage}"
    ],
    "src\\hooks\\useWorkspaceLogic\\useBatchActions.ts": [
      "ç¼ºå°‘ API å¯†é’¥ã€‚",
      "æ ¡å¯¹æ—¶é—´è½´éœ€è¦æºè§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶ã€‚",
      "æ¶¦è‰²ç¿»è¯‘",
      "æ“ä½œå·²ç»ˆæ­¢ï¼Œå·²æ¢å¤åŸçŠ¶æ€",
      "æ“ä½œå·²ç»ˆæ­¢",
      "${actionName}å‰å¤‡ä»½",
      "æ‰¹é‡æ“ä½œ '${actionName}' å®Œæˆï¼",
      "æ“ä½œå¤±è´¥: ${error.message}",
      "æ“ä½œå¤±è´¥ï¼š${error.message}"
    ],
    "src\\components\\upload\\FileUploader.tsx": ["æ›´æ”¹æ–‡ä»¶"],
    "src\\components\\settings\\LanguageSwitcher.tsx": ["ç®€ä½“ä¸­æ–‡"],
    "src\\components\\modals\\SpeakerManagerModal.tsx": [
      "è¯´è¯äººæ¡£æ¡ˆ",
      "æš‚æ— è¯´è¯äºº",
      "ç”Ÿæˆå­—å¹•åä¼šè‡ªåŠ¨æ·»åŠ ",
      "handleStartEdit(profile)}\r\n                            className=\"p-1.5 text-slate-500 hover:text-white hover:bg-slate-700 rounded transition-colors\"\r\n                            title=\"é‡å‘½å\"\r\n                          >",
      "handleDeleteClick(profile.id)}\r\n                            className=\"p-1.5 text-slate-500 hover:text-red-400 hover:bg-slate-700 rounded transition-colors\"\r\n                            title=\"åˆ é™¤\"\r\n                          >",
      "setNewSpeakerName(e.target.value)}\r\n                onKeyDown={(e) => {\r\n                  if (e.key === 'Enter') handleCreateSpeaker();\r\n                  if (e.key === 'Escape') setIsCreating(false);\r\n                }}\r\n                placeholder=\"è¾“å…¥è¯´è¯äººåç§°...\"\r\n                autoFocus\r\n                className=\"flex-1 bg-slate-800 border border-slate-600 rounded px-3 py-1.5 text-sm text-white placeholder-slate-500 focus:border-indigo-500 focus:outline-none\"\r\n              />",
      "æ·»åŠ ",
      "{\r\n                  setIsCreating(false);\r\n                  setNewSpeakerName('');\r\n                }}\r\n                className=\"px-3 py-1.5 text-sm text-slate-400 hover:text-white transition-colors\"\r\n              >\r\n                å–æ¶ˆ",
      "å·²é€‰æ‹© {selectedForMerge.size} ä¸ªè¯´è¯äºº",
      "å–æ¶ˆ",
      "}\r\n                  {isProcessing ? 'åˆå¹¶ä¸­...' : 'åˆå¹¶'}",
      "æ–°å»º",
      "åˆå¹¶",
      "å®Œæˆ",
      "{\r\n          setDeleteConfirmOpen(false);\r\n          setDeleteCandidateId(null);\r\n        }}\r\n        onConfirm={confirmDelete}\r\n        title=\"åˆ é™¤è¯´è¯äºº\"\r\n        message={`ç¡®å®šè¦åˆ é™¤è¯´è¯äººã€Œ${deleteCandidateName}ã€å—ï¼Ÿè¯¥è¯´è¯äººçš„æ‰€æœ‰å­—å¹•å°†ä¸å†å…³è”ä»»ä½•è¯´è¯äººã€‚`}\r\n        confirmText=\"åˆ é™¤\"\r\n        type=\"danger\"\r\n      />",
      "é‡å‘½å",
      "åˆ é™¤",
      "è¾“å…¥è¯´è¯äººåç§°...",
      "åˆ é™¤è¯´è¯äºº",
      "ç¡®å®šè¦åˆ é™¤è¯´è¯äººã€Œ${deleteCandidateName}ã€å—ï¼Ÿè¯¥è¯´è¯äººçš„æ‰€æœ‰å­—å¹•å°†ä¸å†å…³è”ä»»ä½•è¯´è¯äººã€‚"
    ],
    "src\\components\\modals\\GlossaryImportDialog.tsx": [
      "å¯¼å…¥æœ¯è¯­è¡¨",
      "å‡†å¤‡å¯¼å…¥",
      "å·²ä»æ–‡ä»¶ä¸­è¯»å–",
      "ä¸ªæœ¯è¯­ã€‚",
      "é€‰æ‹©å¯¼å…¥æ–¹å¼",
      "åˆ›å»ºæ–°æœ¯è¯­è¡¨",
      "æœ¯è¯­è¡¨åç§°",
      "setNewName(e.target.value)}\r\n                      className=\"w-full bg-slate-950 border border-slate-700 rounded-lg px-3 py-2 text-sm text-white focus:outline-none focus:border-indigo-500\"\r\n                      placeholder=\"è¾“å…¥åç§°...\"\r\n                      autoFocus\r\n                      onClick={(e) => e.stopPropagation()}\r\n                    />",
      "åˆå¹¶åˆ°ç°æœ‰æœ¯è¯­è¡¨",
      "ç›®æ ‡æœ¯è¯­è¡¨",
      "setTargetId(val)}\r\n                        options={glossaries.map((g) => ({ value: g.id, label: g.name }))}\r\n                        className=\"w-full\"\r\n                        placeholder=\"é€‰æ‹©æœ¯è¯­è¡¨\"\r\n                      />",
      "å†²çªå¤„ç†ï¼ˆå½“æœ¯è¯­å·²å­˜åœ¨ï¼‰",
      "è·³è¿‡ (ä¿ç•™æ—§å€¼)",
      "è¦†ç›–ï¼ˆä½¿ç”¨æ–°å€¼ï¼‰",
      "å–æ¶ˆ",
      "ç¡®è®¤å¯¼å…¥",
      "è¾“å…¥åç§°...",
      "é€‰æ‹©æœ¯è¯­è¡¨"
    ],
    "src\\components\\modals\\GlossaryExtractionFailedDialog.tsx": [
      "æœ¯è¯­æå–å¤±è´¥",
      "æ— æ³•ä»éŸ³é¢‘ä¸­æå–æœ¯è¯­ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–éŸ³é¢‘è´¨é‡ä¸ä½³ã€‚",
      ")}\r\n            {isGeneratingGlossary ? 'æ­£åœ¨é‡è¯•...' : 'é‡è¯•æå–'}",
      "è·³è¿‡ (ç»§ç»­è€Œä¸ä½¿ç”¨æ–°æœ¯è¯­)",
      "é‡è¯•æå–"
    ],
    "src\\components\\modals\\GlossaryConfirmationModal.tsx": [
      "ç¡®è®¤æœ¯è¯­è¡¨",
      "æå–å®Œæˆï¼Œè¯·é€‰æ‹©è¦åº”ç”¨çš„æœ¯è¯­ã€‚",
      "å†²çª ({conflicts.length})",
      "å­˜åœ¨å¤šä¸ªç‰ˆæœ¬",
      "ä¿ç•™å½“å‰",
      "{\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, translation: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-sm text-white\"\r\n                                placeholder=\"è‡ªå®šä¹‰ç¿»è¯‘\"\r\n                                autoFocus\r\n                                onClick={(e) => e.stopPropagation()}\r\n                              />",
      "{\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, notes: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-xs text-slate-400\"\r\n                                placeholder=\"å¤‡æ³¨ (å¯é€‰)\"\r\n                                onClick={(e) => e.stopPropagation()}\r\n                              />",
      "{\r\n                                    e.stopPropagation();\r\n                                    setEditingId(null);\r\n                                  }}\r\n                                  className=\"text-xs text-slate-400 hover:text-white\"\r\n                                >\r\n                                  å–æ¶ˆ",
      "{\r\n                                    e.stopPropagation();\r\n                                    saveEdit(conflict.term);\r\n                                  }}\r\n                                  className=\"text-xs text-indigo-400 hover:text-indigo-300\"\r\n                                >\r\n                                  ä¿å­˜",
      "{customValue ? customValue.translation : 'è‡ªå®šä¹‰ç¿»è¯‘...'}",
      "ä¸ä½¿ç”¨æ­¤æœ¯è¯­",
      "æ–°æœ¯è¯­ ({unique.length})",
      "{\r\n                    if (selectedTerms.size === unique.length) setSelectedTerms(new Set());\r\n                    else setSelectedTerms(new Set(unique.map((t) => t.term)));\r\n                  }}\r\n                  className=\"text-xs text-indigo-400 hover:text-indigo-300\"\r\n                >\r\n                  {selectedTerms.size === unique.length ? 'å–æ¶ˆå…¨é€‰' : 'å…¨é€‰'}",
      "{\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, term: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-sm text-white\"\r\n                                placeholder=\"æœ¯è¯­\"\r\n                              />",
      "{\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, translation: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-sm text-white\"\r\n                                placeholder=\"ç¿»è¯‘\"\r\n                              />",
      "{\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, notes: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-xs text-slate-400\"\r\n                                placeholder=\"å¤‡æ³¨\"\r\n                              />",
      "setEditingId(null)}\r\n                                  className=\"text-xs text-slate-400 hover:text-white\"\r\n                                >\r\n                                  å–æ¶ˆ",
      "saveEdit(term.term)}\r\n                                  className=\"text-xs text-indigo-400 hover:text-indigo-300\"\r\n                                >\r\n                                  ä¿å­˜",
      "è‡ªå®šä¹‰æœ¯è¯­ ({customTerms.length})",
      "+ æ·»åŠ æœ¯è¯­",
      "{\r\n                              setEditValue((prev) =>\r\n                                prev ? { ...prev, term: e.target.value } : null\r\n                              );\r\n                            }}\r\n                            className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-sm text-white\"\r\n                            placeholder=\"æœ¯è¯­\"\r\n                          />",
      "{\r\n                              setEditValue((prev) =>\r\n                                prev ? { ...prev, translation: e.target.value } : null\r\n                              );\r\n                            }}\r\n                            className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-sm text-white\"\r\n                            placeholder=\"ç¿»è¯‘\"\r\n                          />",
      "{\r\n                              setEditValue((prev) =>\r\n                                prev ? { ...prev, notes: e.target.value } : null\r\n                              );\r\n                            }}\r\n                            className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-xs text-slate-400\"\r\n                            placeholder=\"å¤‡æ³¨\"\r\n                          />",
      "{\r\n                                setCustomTerms((prev) => prev.filter((_, i) => i !== idx));\r\n                                setEditingId(null);\r\n                              }}\r\n                              className=\"text-xs text-red-400 hover:text-red-300\"\r\n                            >\r\n                              åˆ é™¤",
      "saveEdit(term.term)}\r\n                              className=\"text-xs text-emerald-400 hover:text-emerald-300\"\r\n                            >\r\n                              ä¿å­˜",
      "å…¨éƒ¨ä¸¢å¼ƒ",
      "æ·»åŠ åˆ°:",
      "({ value: g.id, label: g.name })) || []),\r\n                  { value: 'temporary', label: 'ä¸´æ—¶ (ä»…æœ¬æ¬¡ä¼šè¯)' },\r\n                  { value: 'create-new', label: '+ æ–°å»ºæœ¯è¯­è¡¨' },\r\n                ]}\r\n                className=\"w-48\"\r\n                placeholder=\"é€‰æ‹©æœ¯è¯­è¡¨\"\r\n              />",
      "æ·»åŠ  {totalToAdd} ä¸ªæœ¯è¯­",
      "æ–°å»ºæœ¯è¯­è¡¨",
      "setNewGlossaryName(e.target.value)}\r\n              onKeyDown={(e) => e.key === 'Enter' && handleCreateNewGlossary()}\r\n              placeholder=\"è¾“å…¥æœ¯è¯­è¡¨åç§°\"\r\n              className=\"w-full bg-slate-800 border border-slate-700 rounded-lg px-4 py-2 text-white focus:outline-none focus:border-indigo-500 mb-4\"\r\n              autoFocus\r\n            />",
      "{\r\n                  setShowNewGlossaryDialog(false);\r\n                  setNewGlossaryName('');\r\n                }}\r\n                className=\"px-4 py-2 text-slate-400 hover:text-white hover:bg-slate-800 rounded-lg transition-colors\"\r\n              >\r\n                å–æ¶ˆ",
      "åˆ›å»º",
      "è‡ªå®šä¹‰ç¿»è¯‘",
      "å¤‡æ³¨ (å¯é€‰)",
      "è‡ªå®šä¹‰ç¿»è¯‘...",
      "å…¨é€‰",
      "æœ¯è¯­",
      "ç¿»è¯‘",
      "å¤‡æ³¨",
      "ä¸´æ—¶ (ä»…æœ¬æ¬¡ä¼šè¯)",
      "+ æ–°å»ºæœ¯è¯­è¡¨",
      "é€‰æ‹©æœ¯è¯­è¡¨",
      "è¾“å…¥æœ¯è¯­è¡¨åç§°",
      "), 0);\r\n  };\r\n\r\n  const totalToAdd =\r\n    selectedTerms.size +\r\n    Object.values(resolvedConflicts).filter((v) => v !== null).length +\r\n    customTerms.length;\r\n\r\n  return (\r\n    <div className=\"fixed inset-0 z-[60] flex items-center justify-center bg-black/80 backdrop-blur-sm p-4 animate-fade-in\">\r\n      <div className=\"bg-slate-900 border border-indigo-500/30 rounded-2xl w-full max-w-4xl shadow-2xl flex flex-col max-h-[90vh]\">\r\n        <div className=\"p-6 border-b border-slate-800 flex items-center justify-between bg-slate-900/50\">\r\n          <div>\r\n            <h3 className=\"text-xl font-bold text-white flex items-center gap-2\">\r\n              <Book className=\"w-5 h-5 text-indigo-400\" />\r\n              ç¡®è®¤æœ¯è¯­è¡¨\r\n            </h3>\r\n            <p className=\"text-slate-400 text-sm mt-1\">æå–å®Œæˆï¼Œè¯·é€‰æ‹©è¦åº”ç”¨çš„æœ¯è¯­ã€‚</p>\r\n          </div>\r\n          <div className=\"flex items-center gap-4\">\r\n            <button\r\n              onClick={handleDiscard}\r\n              className=\"text-slate-400 hover:text-white transition-colors\"\r\n            >\r\n              <X className=\"w-6 h-6\" />\r\n            </button>\r\n          </div>\r\n        </div>\r\n\r\n        <div className=\"flex-1 overflow-y-auto custom-scrollbar p-6 space-y-8\">\r\n          {}\r\n          {conflicts.length > 0 && (\r\n            <div className=\"space-y-4\">\r\n              <h3 className=\"text-sm font-bold text-amber-400 uppercase tracking-wider flex items-center\">\r\n                <AlertCircle className=\"w-4 h-4 mr-2\" /> å†²çª ({conflicts.length})\r\n              </h3>\r\n              <div className=\"grid gap-4\">\r\n                {conflicts.map((conflict, idx) => {\r\n                  const existingOption = conflict.options.find((o) =>\r\n                    getActiveGlossaryTerms(settings)?.some(\r\n                      (g) => g.term === o.term && g.translation === o.translation\r\n                    )\r\n                  );\r\n                  const newOptions = conflict.options.filter((o) => o !== existingOption);\r\n                  const customId =",
      ";\r\n                  const isCustomEditing = editingId === customId;\r\n                  const customValue = conflictCustomValues[conflict.term];\r\n                  const isCustomSelected = resolvedConflicts[conflict.term] === customValue;\r\n\r\n                  return (\r\n                    <div\r\n                      key={idx}\r\n                      className=\"bg-slate-800/50 border border-amber-500/20 rounded-xl p-4\"\r\n                    >\r\n                      <div className=\"flex items-center justify-between mb-3\">\r\n                        <span className=\"font-bold text-white text-lg\">{conflict.term}</span>\r\n                        <span className=\"text-xs text-amber-500 bg-amber-500/10 px-2 py-1 rounded-full border border-amber-500/20\">\r\n                          å­˜åœ¨å¤šä¸ªç‰ˆæœ¬\r\n                        </span>\r\n                      </div>\r\n                      <div className=\"space-y-2\">\r\n                        {}\r\n                        {newOptions.map((option, optIdx) => (\r\n                          <div\r\n                            key={optIdx}\r\n                            onClick={() => {\r\n                              setResolvedConflicts((prev) => ({\r\n                                ...prev,\r\n                                [conflict.term]: option,\r\n                              }));\r\n                            }}\r\n                            className={cn(\r\n                              'p-3 rounded-lg border cursor-pointer transition-all',\r\n                              resolvedConflicts[conflict.term] === option\r\n                                ? 'bg-indigo-500/20 border-indigo-500 ring-1 ring-indigo-500'\r\n                                : 'bg-slate-800 border-slate-700 hover:border-slate-600'\r\n                            )}\r\n                          >\r\n                            <div className=\"flex items-center justify-between\">\r\n                              <div>\r\n                                <div className=\"font-medium text-white flex items-center gap-2\">\r\n                                  {option.translation}\r\n                                </div>\r\n                                {option.notes && (\r\n                                  <div className=\"text-sm text-slate-400 mt-1\">{option.notes}</div>\r\n                                )}\r\n                              </div>\r\n                              {resolvedConflicts[conflict.term] === option && (\r\n                                <CheckCircle className=\"w-5 h-5 text-indigo-400\" />\r\n                              )}\r\n                            </div>\r\n                          </div>\r\n                        ))}\r\n\r\n                        {}\r\n                        {existingOption && (\r\n                          <div\r\n                            onClick={() => {\r\n                              setResolvedConflicts((prev) => ({\r\n                                ...prev,\r\n                                [conflict.term]: existingOption,\r\n                              }));\r\n                            }}\r\n                            className={cn(\r\n                              'p-3 rounded-lg border cursor-pointer transition-all',\r\n                              resolvedConflicts[conflict.term] === existingOption\r\n                                ? 'bg-indigo-500/20 border-indigo-500 ring-1 ring-indigo-500'\r\n                                : 'bg-slate-800 border-slate-700 hover:border-slate-600'\r\n                            )}\r\n                          >\r\n                            <div className=\"flex items-center justify-between\">\r\n                              <div>\r\n                                <div className=\"font-medium text-white flex items-center gap-2\">\r\n                                  {existingOption.translation}\r\n                                  <span className=\"text-xs bg-slate-700 text-slate-300 px-2 py-0.5 rounded\">\r\n                                    ä¿ç•™å½“å‰\r\n                                  </span>\r\n                                </div>\r\n                                {existingOption.notes && (\r\n                                  <div className=\"text-sm text-slate-400 mt-1\">\r\n                                    {existingOption.notes}\r\n                                  </div>\r\n                                )}\r\n                              </div>\r\n                              {resolvedConflicts[conflict.term] === existingOption && (\r\n                                <CheckCircle className=\"w-5 h-5 text-indigo-400\" />\r\n                              )}\r\n                            </div>\r\n                          </div>\r\n                        )}\r\n\r\n                        {}\r\n                        <div\r\n                          className={cn(\r\n                            'p-3 rounded-lg border cursor-pointer transition-all',\r\n                            isCustomSelected\r\n                              ? 'bg-indigo-500/20 border-indigo-500 ring-1 ring-indigo-500'\r\n                              : 'bg-slate-800 border-slate-700 hover:border-slate-600'\r\n                          )}\r\n                        >\r\n                          {isCustomEditing ? (\r\n                            <div className=\"space-y-2\">\r\n                              <input\r\n                                value={editValue?.translation || ''}\r\n                                onChange={(e) => {\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, translation: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-sm text-white\"\r\n                                placeholder=\"è‡ªå®šä¹‰ç¿»è¯‘\"\r\n                                autoFocus\r\n                                onClick={(e) => e.stopPropagation()}\r\n                              />\r\n                              <input\r\n                                value={editValue?.notes || ''}\r\n                                onChange={(e) => {\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, notes: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-xs text-slate-400\"\r\n                                placeholder=\"å¤‡æ³¨ (å¯é€‰)\"\r\n                                onClick={(e) => e.stopPropagation()}\r\n                              />\r\n                              <div className=\"flex justify-end gap-2\">\r\n                                <button\r\n                                  onClick={(e) => {\r\n                                    e.stopPropagation();\r\n                                    setEditingId(null);\r\n                                  }}\r\n                                  className=\"text-xs text-slate-400 hover:text-white\"\r\n                                >\r\n                                  å–æ¶ˆ\r\n                                </button>\r\n                                <button\r\n                                  onClick={(e) => {\r\n                                    e.stopPropagation();\r\n                                    saveEdit(conflict.term);\r\n                                  }}\r\n                                  className=\"text-xs text-indigo-400 hover:text-indigo-300\"\r\n                                >\r\n                                  ä¿å­˜\r\n                                </button>\r\n                              </div>\r\n                            </div>\r\n                          ) : (\r\n                            <div\r\n                              onClick={() => {\r\n                                if (customValue) {\r\n                                  setResolvedConflicts((prev) => ({\r\n                                    ...prev,\r\n                                    [conflict.term]: customValue,\r\n                                  }));\r\n                                } else {\r\n                                  startEditing(\r\n                                    { term: conflict.term, translation: '', notes: '' },\r\n                                    customId\r\n                                  );\r\n                                }\r\n                              }}\r\n                              className=\"flex items-center justify-between\"\r\n                            >\r\n                              <div className=\"flex items-center gap-2\">\r\n                                <Edit2 className=\"w-4 h-4 text-emerald-400\" />\r\n                                <span\r\n                                  className={\r\n                                    customValue\r\n                                      ? 'text-white font-medium'\r\n                                      : 'text-slate-400 italic text-sm'\r\n                                  }\r\n                                >\r\n                                  {customValue ? customValue.translation : 'è‡ªå®šä¹‰ç¿»è¯‘...'}\r\n                                </span>\r\n                              </div>\r\n                              {customValue && (\r\n                                <div className=\"flex items-center gap-2\">\r\n                                  <button\r\n                                    onClick={(e) => {\r\n                                      e.stopPropagation();\r\n                                      startEditing(customValue, customId);\r\n                                    }}\r\n                                    className=\"p-1 hover:bg-slate-700 rounded text-slate-400 hover:text-white\"\r\n                                  >\r\n                                    <Edit2 className=\"w-3 h-3\" />\r\n                                  </button>\r\n                                  {isCustomSelected && (\r\n                                    <CheckCircle className=\"w-5 h-5 text-indigo-400\" />\r\n                                  )}\r\n                                </div>\r\n                              )}\r\n                            </div>\r\n                          )}\r\n                        </div>\r\n\r\n                        {}\r\n                        {!existingOption && (\r\n                          <div\r\n                            onClick={() => {\r\n                              setResolvedConflicts((prev) => ({ ...prev, [conflict.term]: null }));\r\n                            }}\r\n                            className={cn(\r\n                              'p-3 rounded-lg border cursor-pointer transition-all',\r\n                              resolvedConflicts[conflict.term] === null\r\n                                ? 'bg-red-500/10 border-red-500/50 text-red-400'\r\n                                : 'bg-slate-800 border-slate-700 hover:border-slate-600 text-slate-400'\r\n                            )}\r\n                          >\r\n                            <div className=\"flex items-center gap-2\">\r\n                              <X className=\"w-4 h-4\" />\r\n                              <span>ä¸ä½¿ç”¨æ­¤æœ¯è¯­</span>\r\n                            </div>\r\n                          </div>\r\n                        )}\r\n                      </div>\r\n                    </div>\r\n                  );\r\n                })}\r\n              </div>\r\n            </div>\r\n          )}\r\n\r\n          {}\r\n          {unique.length > 0 && (\r\n            <div className=\"space-y-4\">\r\n              <div className=\"flex items-center justify-between\">\r\n                <h3 className=\"text-sm font-bold text-indigo-400 uppercase tracking-wider flex items-center\">\r\n                  <Sparkles className=\"w-4 h-4 mr-2\" /> æ–°æœ¯è¯­ ({unique.length})\r\n                </h3>\r\n                <button\r\n                  onClick={() => {\r\n                    if (selectedTerms.size === unique.length) setSelectedTerms(new Set());\r\n                    else setSelectedTerms(new Set(unique.map((t) => t.term)));\r\n                  }}\r\n                  className=\"text-xs text-indigo-400 hover:text-indigo-300\"\r\n                >\r\n                  {selectedTerms.size === unique.length ? 'å–æ¶ˆå…¨é€‰' : 'å…¨é€‰'}\r\n                </button>\r\n              </div>\r\n              <div className=\"grid grid-cols-1 md:grid-cols-2 gap-3\">\r\n                {unique.map((term, idx) => {\r\n                  const isSelected = selectedTerms.has(term.term);\r\n                  const isEditing = editingId === term.term;\r\n                  const displayTerm = overrides[term.term] || term;\r\n\r\n                  return (\r\n                    <div\r\n                      key={idx}\r\n                      className={cn(\r\n                        'p-3 rounded-xl border transition-all',\r\n                        isSelected\r\n                          ? 'bg-indigo-500/10 border-indigo-500/30'\r\n                          : 'bg-slate-800/50 border-slate-700 opacity-60'\r\n                      )}\r\n                    >\r\n                      <div className=\"flex items-start gap-3\">\r\n                        <div className=\"pt-1\">\r\n                          <div\r\n                            onClick={() => toggleTerm(term.term)}\r\n                            className={cn(\r\n                              'w-5 h-5 rounded border flex items-center justify-center cursor-pointer transition-colors',\r\n                              isSelected\r\n                                ? 'bg-indigo-500 border-indigo-500'\r\n                                : 'border-slate-600 bg-slate-700/50 hover:border-slate-500'\r\n                            )}\r\n                          >\r\n                            {isSelected && <Check className=\"w-3.5 h-3.5 text-white\" />}\r\n                          </div>\r\n                        </div>\r\n                        <div className=\"flex-1 min-w-0\">\r\n                          {isEditing ? (\r\n                            <div className=\"space-y-2\">\r\n                              <input\r\n                                value={editValue?.term}\r\n                                onChange={(e) => {\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, term: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-sm text-white\"\r\n                                placeholder=\"æœ¯è¯­\"\r\n                              />\r\n                              <input\r\n                                value={editValue?.translation}\r\n                                onChange={(e) => {\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, translation: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-sm text-white\"\r\n                                placeholder=\"ç¿»è¯‘\"\r\n                              />\r\n                              <input\r\n                                value={editValue?.notes}\r\n                                onChange={(e) => {\r\n                                  setEditValue((prev) =>\r\n                                    prev ? { ...prev, notes: e.target.value } : null\r\n                                  );\r\n                                }}\r\n                                className=\"w-full bg-slate-900 border border-slate-600 rounded px-2 py-1 text-xs text-slate-400\"\r\n                                placeholder=\"å¤‡æ³¨\"\r\n                              />\r\n                              <div className=\"flex justify-end gap-2\">\r\n                                <button\r\n                                  onClick={() => setEditingId(null)}\r\n                                  className=\"text-xs text-slate-400 hover:text-white\"\r\n                                >\r\n                                  å–æ¶ˆ\r\n                                </button>\r\n                                <button\r\n                                  onClick={() => saveEdit(term.term)}\r\n                                  className=\"text-xs text-indigo-400 hover:text-indigo-300\"\r\n                                >\r\n                                  ä¿å­˜\r\n                                </button>\r\n                              </div>\r\n                            </div>\r\n                          ) : (\r\n                            <div className=\"group relative\">\r\n                              <div className=\"flex items-center justify-between\">\r\n                                <div className=\"font-medium text-white text-base truncate pr-2\">\r\n                                  {displayTerm.term}\r\n                                </div>\r\n                                <button\r\n                                  onClick={() => startEditing(displayTerm, term.term)}\r\n                                  className=\"opacity-0 group-hover:opacity-100 p-1 hover:bg-slate-700 rounded text-slate-400 hover:text-white transition-all\"\r\n                                >\r\n                                  <Edit2 className=\"w-3 h-3\" />\r\n                                </button>\r\n                              </div>\r\n                              <div className=\"text-indigo-300 text-sm mt-0.5\">\r\n                                {displayTerm.translation}\r\n                              </div>\r\n                              {displayTerm.notes && (\r\n                                <div className=\"text-slate-500 text-xs mt-1 italic\">\r\n                                  {displayTerm.notes}\r\n                                </div>\r\n                              )}\r\n                            </div>\r\n                          )}\r\n                        </div>\r\n                      </div>\r\n                    </div>\r\n                  );\r\n                })}\r\n              </div>\r\n            </div>\r\n          )}\r\n\r\n          {}\r\n          <div className=\"space-y-4\">\r\n            <div className=\"flex items-center justify-between\">\r\n              <h3 className=\"text-sm font-bold text-emerald-400 uppercase tracking-wider flex items-center\">\r\n                <Plus className=\"w-4 h-4 mr-2\" /> è‡ªå®šä¹‰æœ¯è¯­ ({customTerms.length})\r\n              </h3>\r\n              <button\r\n                onClick={addCustomTerm}\r\n                className=\"text-xs bg-emerald-500/10 text-emerald-400 hover:bg-emerald-500/20 px-2 py-1 rounded border border-emerald-500/20 transition-colors\"\r\n              >\r\n                + æ·»åŠ æœ¯è¯­\r\n              </button>\r\n            </div>\r\n            {customTerms.length > 0 && (\r\n              <div className=\"grid grid-cols-1 md:grid-cols-2 gap-3\">\r\n                {customTerms.map((term, idx) => {\r\n                  const id ="
    ],
    "src\\components\\layout\\WorkspaceHeader.tsx": [
      "}\r\n            label=\"å†å²\"\r\n            title=\"å†å²è®°å½•\"\r\n            highlighted={hasSnapshots}\r\n          />",
      "}\r\n            label=\"æ—¥å¿—\"\r\n            title=\"æŸ¥çœ‹æ—¥å¿—\"\r\n            hoverColor=\"blue\"\r\n          />",
      "}\r\n            label=\"æœ¯è¯­è¡¨\"\r\n            title=\"æœ¯è¯­è¡¨ç®¡ç†\"\r\n            hoverColor=\"indigo\"\r\n          />",
      "}\r\n            label=\"è®¾ç½®\"\r\n            hoverColor=\"emerald\"\r\n          />",
      "å†å²",
      "å†å²è®°å½•",
      "æ—¥å¿—",
      "æŸ¥çœ‹æ—¥å¿—",
      "æœ¯è¯­è¡¨",
      "æœ¯è¯­è¡¨ç®¡ç†",
      "è®¾ç½®"
    ],
    "src\\components\\layout\\LogViewerModal.tsx": [
      "åº”ç”¨æ—¥å¿—",
      "setFilterLevel(value as LogLevel)}\r\n              options={LOG_LEVELS.map((level) => ({\r\n                value: level,\r\n                label: level === 'ALL' ? 'å…¨éƒ¨' : level,\r\n              }))}\r\n              icon={",
      "å¯¼å‡º",
      "æš‚æ— æ—¥å¿—",
      "å½“å‰çº§åˆ«æ²¡æœ‰æ—¥å¿—",
      "å¯¼å‡ºæ‰€æœ‰æ—¥å¿—"
    ],
    "src\\components\\layout\\HistoryPanel.tsx": [
      "();\r\n    sorted.forEach((snap) => {\r\n      const key = snap.fileId || 'unknown';\r\n      if (!groups.has(key)) {\r\n        groups.set(key, []);\r\n      }\r\n      groups.get(key)!.push(snap);\r\n    });\r\n\r\n    \r\n    const result: GroupedSnapshots[] = [];\r\n    groups.forEach((snaps, fileId) => {\r\n      result.push({\r\n        fileId,\r\n        fileName: snaps[0]?.fileName || 'æœªçŸ¥æ–‡ä»¶',\r\n        snapshots: snaps,\r\n      });\r\n    });\r\n\r\n    \r\n    result.sort((a, b) => {\r\n      const aLatest = parseInt(a.snapshots[0]?.id || '0');\r\n      const bLatest = parseInt(b.snapshots[0]?.id || '0');\r\n      return bLatest - aLatest;\r\n    });\r\n\r\n    return result;\r\n  }, [snapshots, searchQuery]);\r\n\r\n  const toggleGroup = (fileId: string) => {\r\n    setExpandedGroups((prev) => {\r\n      const next = new Set(prev);\r\n      if (next.has(fileId)) {\r\n        next.delete(fileId);\r\n      } else {\r\n        next.add(fileId);\r\n      }\r\n      return next;\r\n    });\r\n  };\r\n\r\n  \r\n  React.useEffect(() => {\r\n    if (groupedSnapshots.length > 0 && expandedGroups.size === 0) {\r\n      setExpandedGroups(new Set([groupedSnapshots[0].fileId]));\r\n    }\r\n    \r\n  }, [groupedSnapshots.length]);\r\n\r\n  if (!isOpen) return null;\r\n\r\n  return (",
      "å¿«ç…§è®°å½•",
      "æš‚æ— å¿«ç…§",
      "{snap.subtitles.length} è¡Œå­—å¹• Â· {snap.timestamp}",
      "åŠ è½½",
      "onDeleteSnapshot(snap.id)}\r\n                              className=\"p-1.5 text-slate-500 hover:text-red-400 hover:bg-red-500/10 rounded transition-colors\"\r\n                              title=\"åˆ é™¤\"\r\n                            >",
      "æœªçŸ¥æ–‡ä»¶",
      "æœç´¢æ–‡ä»¶åæˆ–æè¿°...",
      "åˆ é™¤"
    ],
    "src\\components\\layout\\Header.tsx": [
      "}\r\n      subtitle=\"AI å­—å¹•ç”Ÿæˆä¸ç¿»è¯‘å·¥å…·\"\r\n      icon={",
      "}\r\n              label=\"æ—¥å¿—\"\r\n              title=\"æŸ¥çœ‹æ—¥å¿—\"\r\n              hoverColor=\"blue\"\r\n            />\r\n          )}\r\n          {onShowGlossary && (",
      "}\r\n              label=\"æœ¯è¯­è¡¨\"\r\n              title=\"æœ¯è¯­è¡¨ç®¡ç†\"\r\n              hoverColor=\"indigo\"\r\n            />\r\n          )}\r\n          {onShowSettings && (",
      "}\r\n              label=\"è®¾ç½®\"\r\n              hoverColor=\"emerald\"\r\n            />\r\n          )}",
      "AI å­—å¹•ç”Ÿæˆä¸ç¿»è¯‘å·¥å…·",
      "æ—¥å¿—",
      "æŸ¥çœ‹æ—¥å¿—",
      "æœ¯è¯­è¡¨",
      "æœ¯è¯­è¡¨ç®¡ç†",
      "è®¾ç½®"
    ],
    "src\\components\\endToEnd\\EndToEndWizard.tsx": [
      "void;\r\n  onCancel: () => void;\r\n  onShowLogs?: () => void;\r\n  onShowGlossary?: () => void;\r\n  onShowSettings?: () => void;\r\n}\r\n\r\n\r\nexport function EndToEndWizard({\r\n  settings,\r\n  onComplete,\r\n  onCancel,\r\n  onShowLogs,\r\n  onShowGlossary,\r\n  onShowSettings,\r\n}: EndToEndWizardProps) {\r\n  const {\r\n    state,\r\n    setStep,\r\n    goNext,\r\n    goBack,\r\n    updateConfig,\r\n    resetConfig,\r\n    resetToConfig,\r\n    retryPipeline,\r\n    parseUrl,\r\n    videoInfo,\r\n    startPipeline,\r\n    abortPipeline,\r\n    isElectron,\r\n  } = useEndToEnd();\r\n\r\n  const steps = [\r\n    { label: 'è¾“å…¥é“¾æ¥', icon:",
      "},\r\n    { label: 'é…ç½®å‚æ•°', icon:",
      "},\r\n    { label: 'æ‰§è¡Œå¤„ç†', icon:",
      "},\r\n    { label: 'å®Œæˆ', icon:",
      "åŠŸèƒ½ä¸å¯ç”¨",
      "æ­¤åŠŸèƒ½ä»…åœ¨æ¡Œé¢ç‰ˆå¯ç”¨",
      "è¿”å›",
      "å…¨è‡ªåŠ¨æ¨¡å¼",
      "ç«¯åˆ°ç«¯æ¨¡å¼",
      "}\r\n          subtitle=\"è¾“å…¥é“¾æ¥ï¼Œè‡ªåŠ¨ç”Ÿæˆå­—å¹•è§†é¢‘\"\r\n          onBack={onCancel}\r\n          actions={",
      "}\r\n                  label=\"æ—¥å¿—\"\r\n                  title=\"æŸ¥çœ‹æ—¥å¿—\"\r\n                  hoverColor=\"blue\"\r\n                />\r\n              )}\r\n              {onShowGlossary && (",
      "}\r\n                  label=\"æœ¯è¯­è¡¨\"\r\n                  title=\"æœ¯è¯­è¡¨ç®¡ç†\"\r\n                  hoverColor=\"indigo\"\r\n                />\r\n              )}\r\n              {onShowSettings && (",
      "}\r\n                  label=\"è®¾ç½®\"\r\n                  hoverColor=\"emerald\"\r\n                />\r\n              )}",
      "{currentStepIndex > 0 ? 'ä¸Šä¸€æ­¥' : 'å–æ¶ˆ'}",
      ": undefined}\r\n              >\r\n                {state.currentStep === 'config' ? (\r\n                  'å¼€å§‹å¤„ç†'\r\n                ) : (",
      "ä¸‹ä¸€æ­¥",
      "è¾“å…¥é“¾æ¥",
      "é…ç½®å‚æ•°",
      "æ‰§è¡Œå¤„ç†",
      "å®Œæˆ",
      "è¾“å…¥é“¾æ¥ï¼Œè‡ªåŠ¨ç”Ÿæˆå­—å¹•è§†é¢‘",
      "æ—¥å¿—",
      "æŸ¥çœ‹æ—¥å¿—",
      "æœ¯è¯­è¡¨",
      "æœ¯è¯­è¡¨ç®¡ç†",
      "è®¾ç½®",
      "å–æ¶ˆ",
      "å¼€å§‹å¤„ç†"
    ],
    "src\\components\\endToEnd\\EndToEndProgress.tsx": [
      "ç”¨æ—¶: {formatDuration(elapsed)}",
      "= {\r\n  decoding: 'è§£ç éŸ³é¢‘',\r\n  segmenting: 'åˆ†æ®µå¤„ç†',\r\n  glossary: 'æå–æœ¯è¯­',\r\n  diarization: 'è¯´è¯äººé¢„åˆ†æ',\r\n};\r\n\r\n\r\nfunction TranscribeChunkList({ chunks }: { chunks: ChunkStatus[] }) {\r\n  \r\n  const sortedChunks = [...chunks].sort((a, b) => {\r\n    const systemOrder: Record",
      "å­—å¹•ç”Ÿæˆè¿›åº¦",
      "{totalCompleted}/{totalCount} å·²å®Œæˆ",
      "{CHUNK_LABELS[String(chunk.id)] || `ç‰‡æ®µ ${chunk.id}`}",
      ",\r\n    label: 'å‡†å¤‡ä¸­',\r\n    description: 'åˆå§‹åŒ–ä»»åŠ¡...',\r\n  },\r\n  downloading: {\r\n    icon:",
      ",\r\n    label: 'ä¸‹è½½è§†é¢‘',\r\n    description: 'æ­£åœ¨ä»ç½‘ç»œä¸‹è½½è§†é¢‘æ–‡ä»¶',\r\n  },\r\n  extracting_audio: {\r\n    icon:",
      ",\r\n    label: 'æå–éŸ³é¢‘',\r\n    description: 'ä»è§†é¢‘ä¸­æå–éŸ³é¢‘æµ',\r\n  },\r\n  transcribing: {\r\n    icon:",
      ",\r\n    label: 'ç”Ÿæˆå­—å¹•',\r\n    description: 'AI è½¬å½•ä¸ç¿»è¯‘',\r\n  },\r\n  extracting_glossary: {\r\n    icon:",
      ",\r\n    label: 'æå–æœ¯è¯­',\r\n    description: 'ç”Ÿæˆä¸“æœ‰è¯æ±‡è¡¨',\r\n  },\r\n  extracting_speakers: {\r\n    icon:",
      ",\r\n    label: 'è¯´è¯äººé¢„åˆ†æ',\r\n    description: 'è¯†åˆ«éŸ³é¢‘ä¸­çš„è¯´è¯äºº',\r\n  },\r\n  refining: {\r\n    icon:",
      ",\r\n    label: 'æ¶¦è‰²æ ¡å¯¹',\r\n    description: 'æ ¡å¯¹å¹¶æ¶¦è‰²å­—å¹•å†…å®¹',\r\n  },\r\n  translating: {\r\n    icon:",
      ",\r\n    label: 'ç¿»è¯‘å­—å¹•',\r\n    description: 'ç¿»è¯‘å­—å¹•å†…å®¹',\r\n  },\r\n  exporting_subtitle: {\r\n    icon:",
      ",\r\n    label: 'å¯¼å‡ºå­—å¹•',\r\n    description: 'å°†å­—å¹•å¯¼å‡ºä¸ºæ–‡ä»¶',\r\n  },\r\n  compressing: {\r\n    icon:",
      ",\r\n    label: 'å‹åˆ¶è§†é¢‘',\r\n    description: 'å‹ç¼©è§†é¢‘å¹¶åµŒå…¥å­—å¹•',\r\n  },\r\n  completed: {\r\n    icon:",
      ",\r\n    label: 'å®Œæˆ',\r\n    description: 'æ‰€æœ‰å¤„ç†å·²å®Œæˆ',\r\n  },\r\n  failed: {\r\n    icon:",
      ",\r\n    label: 'å¤±è´¥',\r\n    description: 'å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯',\r\n  },\r\n};\r\n\r\n\r\nconst processingStages: PipelineStage[] = [\r\n  'downloading',\r\n  'extracting_audio',\r\n  'transcribing',\r\n  \r\n  'compressing',\r\n];\r\n\r\n\r\nfunction StageIndicator({\r\n  stage,\r\n  status,\r\n  isCurrent,\r\n}: {\r\n  stage: PipelineStage;\r\n  status: 'pending' | 'active' | 'completed' | 'error';\r\n  isCurrent: boolean;\r\n}) {\r\n  const config = stageConfig[stage];\r\n\r\n  const statusStyles = {\r\n    pending: 'bg-white/5 border-white/10 text-white/40',\r\n    active: 'bg-blue-500/20 border-blue-500/50 text-blue-400 animate-pulse',\r\n    completed: 'bg-emerald-500/20 border-emerald-500/50 text-emerald-400',\r\n    error: 'bg-red-500/20 border-red-500/50 text-red-400',\r\n  };\r\n\r\n  return (",
      "ç¡®è®¤å–æ¶ˆ",
      "ç¡®å®šè¦å–æ¶ˆå½“å‰å¤„ç†å—ï¼Ÿå·²å®Œæˆçš„ä¸­é—´æ–‡ä»¶å°†è¢«ä¿ç•™åœ¨ä¸´æ—¶ç›®å½•ä¸­ã€‚",
      "ç»§ç»­å¤„ç†",
      "æ€»è¿›åº¦",
      "æ­£åœ¨å‹åˆ¶è§†é¢‘...",
      "é”™è¯¯è¯¦æƒ…",
      "é‡è¯•",
      "{isCompleted || isError ? 'å…³é—­' : 'å–æ¶ˆå¤„ç†'}",
      "è§£ç éŸ³é¢‘",
      "åˆ†æ®µå¤„ç†",
      "æå–æœ¯è¯­",
      "è¯´è¯äººé¢„åˆ†æ",
      "å‡†å¤‡ä¸­",
      "åˆå§‹åŒ–ä»»åŠ¡...",
      "ä¸‹è½½è§†é¢‘",
      "æ­£åœ¨ä»ç½‘ç»œä¸‹è½½è§†é¢‘æ–‡ä»¶",
      "æå–éŸ³é¢‘",
      "ä»è§†é¢‘ä¸­æå–éŸ³é¢‘æµ",
      "ç”Ÿæˆå­—å¹•",
      "AI è½¬å½•ä¸ç¿»è¯‘",
      "ç”Ÿæˆä¸“æœ‰è¯æ±‡è¡¨",
      "è¯†åˆ«éŸ³é¢‘ä¸­çš„è¯´è¯äºº",
      "æ¶¦è‰²æ ¡å¯¹",
      "æ ¡å¯¹å¹¶æ¶¦è‰²å­—å¹•å†…å®¹",
      "ç¿»è¯‘å­—å¹•",
      "ç¿»è¯‘å­—å¹•å†…å®¹",
      "å¯¼å‡ºå­—å¹•",
      "å°†å­—å¹•å¯¼å‡ºä¸ºæ–‡ä»¶",
      "å‹åˆ¶è§†é¢‘",
      "å‹ç¼©è§†é¢‘å¹¶åµŒå…¥å­—å¹•",
      "å®Œæˆ",
      "æ‰€æœ‰å¤„ç†å·²å®Œæˆ",
      "å¤±è´¥",
      "å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
      "å–æ¶ˆå¤„ç†",
      "ç‰‡æ®µ ${chunk.id}"
    ],
    "src\\components\\endToEnd\\wizard\\steps\\StepResult.tsx": [
      "{success ? 'å¤„ç†å®Œæˆ' : 'å¤„ç†å¤±è´¥'}",
      "{success\r\n            ? `è€—æ—¶ ${Math.round((result?.duration || 0) / 1000 / 60)} åˆ†é’Ÿ`\r\n            : result?.error || 'å‘ç”ŸæœªçŸ¥é”™è¯¯'}",
      "}\r\n              label=\"åŸå§‹è§†é¢‘\"\r\n              path={outputs.videoPath}\r\n              onOpen={() => handleOpenFolder(outputs.videoPath)}\r\n            />\r\n          )}\r\n          {outputs.subtitlePath && (",
      "}\r\n              label=\"å­—å¹•æ–‡ä»¶\"\r\n              path={outputs.subtitlePath}\r\n              onOpen={() => handleOpenFolder(outputs.subtitlePath)}\r\n            />\r\n          )}\r\n          {outputs.outputVideoPath && (",
      "}\r\n              label=\"å‹åˆ¶è§†é¢‘\"\r\n              path={outputs.outputVideoPath}\r\n              onOpen={() => handleOpenFolder(outputs.outputVideoPath)}\r\n              highlight\r\n            />\r\n          )}",
      "é”™è¯¯é˜¶æ®µ: {result.errorDetails.stage}",
      "å¤„ç†æ–°è§†é¢‘",
      "å®Œæˆ",
      "å¤„ç†å¤±è´¥",
      "å‘ç”ŸæœªçŸ¥é”™è¯¯",
      "åŸå§‹è§†é¢‘",
      "å­—å¹•æ–‡ä»¶",
      "å‹åˆ¶è§†é¢‘",
      "è€—æ—¶ ${Math.round((result?.duration || 0) / 1000 / 60)} åˆ†é’Ÿ"
    ],
    "src\\components\\endToEnd\\wizard\\steps\\StepInput.tsx": [
      "è¾“å…¥è§†é¢‘é“¾æ¥",
      "æ”¯æŒ YouTube å’Œ Bilibili è§†é¢‘",
      "setInputUrl(e.target.value)}\r\n            onKeyDown={(e) =>\r\n              e.key === 'Enter' && !isParsing && validationResult?.valid && handleParse()\r\n            }\r\n            placeholder=\"ç²˜è´´ YouTube / Bilibili è§†é¢‘é“¾æ¥...\"\r\n            className={cn(\r\n              'w-full px-4 py-4 bg-white/5 border rounded-xl text-white placeholder-white/40 focus:outline-none focus:ring-2 transition-all',\r\n              validationResult?.valid === false &&\r\n                'border-red-500/50 focus:border-red-500/50 focus:ring-red-500/20',\r\n              validationResult?.valid === true &&\r\n                'border-emerald-500/30 focus:border-emerald-500/50 focus:ring-emerald-500/20',\r\n              validationResult === null &&\r\n                'border-white/10 focus:border-violet-500/50 focus:ring-violet-500/20'\r\n            )}\r\n            disabled={isParsing}\r\n          />",
      "{videoInfo.title || 'æœªçŸ¥æ ‡é¢˜'}",
      "{videoInfo.uploader || 'æœªçŸ¥ä½œè€…'}",
      "{videoInfo.platform || 'è§†é¢‘'}",
      "}\r\n          size=\"lg\"\r\n          fullWidth\r\n        >\r\n          è§£æè§†é¢‘",
      "ç²˜è´´ YouTube / Bilibili è§†é¢‘é“¾æ¥...",
      "æœªçŸ¥æ ‡é¢˜",
      "æœªçŸ¥ä½œè€…",
      "è§†é¢‘",
      "æ­£åœ¨è§£æ..."
    ],
    "src\\components\\endToEnd\\wizard\\steps\\StepConfig.tsx": [
      "è¾“å‡ºç›®å½•",
      "ç”»è´¨é€‰æ‹©",
      "{[\r\n                  { value: 'best', label: 'æœ€ä½³' },\r\n                  { value: '1080p', label: '1080p' },\r\n                  { value: '720p', label: '720p' },\r\n                  { value: '480p', label: '480p' },\r\n                ].map((quality) => (",
      "å†…å®¹ç±»å‹",
      "ä½¿ç”¨æœ¯è¯­è¡¨",
      "é€‰æ‹©å·²æœ‰çš„æœ¯è¯­è¡¨è¾…åŠ©ç¿»è¯‘",
      "onConfigChange({ selectedGlossaryId: val || null })}\r\n                  options={[\r\n                    { value: '', label: '(æ— )' },\r\n                    ...(settings?.glossaries?.map((g) => ({\r\n                      value: g.id,\r\n                      label: (",
      "),\r\n                    })) || []),\r\n                  ]}\r\n                  placeholder=\"(æ— )\"\r\n                />",
      "è¯´è¯äººæ•°é‡ (å¯é€‰)",
      "æœ€å°‘è¯´è¯äºº",
      "æœ€å¤šè¯´è¯äºº",
      "ç¡¬ä»¶åŠ é€Ÿ",
      "ç¼–ç å™¨",
      "åˆ†è¾¨ç‡",
      "è´¨é‡ (CRF)",
      "èŒƒå›´ 0-51ï¼Œæ•°å€¼è¶Šå°ç”»è´¨è¶Šé«˜ã€‚æ¨èï¼šH.264 (23), H.265 (28)",
      "æœªé€‰æ‹©",
      "æœ€ä½³",
      "ä¸‹è½½å°é¢",
      "å¯ç”¨è‡ªåŠ¨æœ¯è¯­è¡¨",
      "æå–æœ¯è¯­åç›´æ¥åº”ç”¨ï¼Œæ— éœ€äººå·¥ç¡®è®¤ã€‚æ–°æœ¯è¯­å°†è‡ªåŠ¨åˆå¹¶è‡³å½“å‰æ¿€æ´»çš„æœ¯è¯­è¡¨ï¼ˆå¦‚å½“å‰æ— æœ¯è¯­è¡¨ï¼Œåˆ™è‡ªåŠ¨æ–°å»ºï¼‰ã€‚",
      "(æ— )",
      "å¯ç”¨è¯´è¯äººåŒºåˆ†",
      "è¯†åˆ«éŸ³é¢‘æˆ–è§†é¢‘ä¸­çš„ä¸åŒè¯´è¯äººï¼Œæ‰“ä¸Šæ ‡ç­¾",
      "å¯ç”¨è¯´è¯äººé¢„åˆ†æ (å®éªŒæ€§)",
      "åœ¨ç”Ÿæˆå­—å¹•å‰é¢„å…ˆåˆ†æéŸ³é¢‘ä»¥è¯†åˆ«è¯´è¯äººæ•°é‡å’Œå£°éŸ³ç‰¹å¾ï¼Œå¯æé«˜åŒºåˆ†å‡†ç¡®åº¦ï¼Œä½†ä¼šå¢åŠ è€—æ—¶",
      "å¯¼å‡ºæ—¶åŒ…å«è¯´è¯äººåç§°",
      "åœ¨å­—å¹•æ–‡ä»¶ä¸­æ˜¾ç¤ºè¯´è¯äººï¼ˆå¦‚ï¼šç¾Šå®«å¦ƒé‚£ï¼šå¯¹è¯å†…å®¹ï¼‰",
      "ä½¿ç”¨è¯´è¯äººé¢œè‰² (ASS)",
      "ä¸ºä¸åŒè¯´è¯äººåˆ†é…ä¸åŒé¢œè‰²ï¼ˆä»… ASS æ ¼å¼æœ‰æ•ˆï¼‰",
      "è§’è‰²é£æ ¼åŒ–ç¿»è¯‘",
      "æ ¹æ®è¯´è¯äººç‰¹å¾è°ƒæ•´ç¿»è¯‘è¯­æ°”ï¼ˆæ­£å¼/å£è¯­ï¼‰",
      "éœ€è¦å¯ç”¨ã€Œè¯´è¯äººé¢„åˆ†æã€æ‰èƒ½ä½¿ç”¨æ­¤åŠŸèƒ½",
      "å¯ç”¨è§†é¢‘å‹åˆ¶",
      "é«˜æ€§èƒ½ H.264/H.265 è§†é¢‘ç¼–ç ä¸å­—å¹•å†…åµŒ"
    ],
    "src\\components\\endToEnd\\wizard\\shared\\GenreSelector.tsx": ["å†…å®¹ç±»å‹"],
    "src\\components\\editor\\SubtitleBatch.tsx": [
      "ç‰‡æ®µ {chunkIdx + 1}",
      "updateBatchComment(chunkIdx, e.target.value)}\r\n              placeholder=\"æ·»åŠ è¯´æ˜æˆ–æ³¨é‡Š...\"\r\n              className=\"w-full bg-slate-900/50 border border-slate-700/50 rounded px-2 py-1 text-xs text-amber-200 placeholder-slate-600 focus:border-amber-500/50 focus:outline-none\"\r\n            />",
      "handleBatchAction('proofread', chunkIdx)}\r\n                title=\"æ¶¦è‰²ç¿»è¯‘\"\r\n                className=\"p-2 text-slate-500 hover:text-indigo-400 hover:bg-slate-700 rounded-lg transition-colors\"\r\n              >",
      "æ·»åŠ è¯´æ˜æˆ–æ³¨é‡Š...",
      "æ¶¦è‰²ç¿»è¯‘"
    ],
    "src\\components\\editor\\SpeakerSelect.tsx": [
      "{currentSpeaker || 'é€‰æ‹©è¯´è¯äºº'}",
      "æš‚æ— è¯´è¯äºº",
      "ç®¡ç†è¯´è¯äºº",
      "é€‰æ‹©è¯´è¯äºº"
    ],
    "src\\components\\download\\UrlInput.tsx": [
      "setUrl(e.target.value)}\r\n            placeholder=\"ç²˜è´´ YouTube / Bilibili è§†é¢‘é“¾æ¥...\"\r\n            disabled={disabled}\r\n            className=\"flex-1 px-4 py-3.5 bg-white/5 border border-white/10 rounded-lg text-white text-base\r\n                          placeholder:text-white/40 transition-all\r\n                          focus:outline-none focus:border-violet-500/50 focus:ring-3 focus:ring-violet-500/15\r\n                          disabled:opacity-50 disabled:cursor-not-allowed\"\r\n          />",
      "è§£æ",
      "ç²˜è´´ YouTube / Bilibili è§†é¢‘é“¾æ¥...",
      "è§£æä¸­"
    ],
    "src\\components\\download\\QualitySelector.tsx": [
      "void;\r\n  disabled?: boolean;\r\n  label?: string;\r\n  className?: string;\r\n}\r\n\r\nexport function QualitySelector({\r\n  formats,\r\n  selectedFormat,\r\n  onSelect,\r\n  disabled,\r\n  label = 'ç”»è´¨é€‰æ‹©',\r\n  className = '',\r\n}: QualitySelectorProps) {\r\n  const formatFilesize = (bytes?: number): string => {\r\n    if (!bytes) return '';\r\n    if (bytes",
      "ç”»è´¨é€‰æ‹©"
    ],
    "src\\components\\download\\DownloadProgress.tsx": [
      "void;\r\n}\r\n\r\nexport function DownloadProgress({ progress, onCancel }: DownloadProgressProps) {\r\n  const getStageLabel = () => {\r\n    if (!progress) return 'å‡†å¤‡ä¸‹è½½ä¸­...';\r\n    switch (progress.stage) {\r\n      case 'video':\r\n        return 'æ­£åœ¨ä¸‹è½½è§†é¢‘ç”»é¢ (1/2)...';\r\n      case 'audio':\r\n        return 'æ­£åœ¨ä¸‹è½½éŸ³é¢‘å£°éŸ³ (2/2)...';\r\n      case 'merging':\r\n        return 'æ­£åœ¨åˆå¹¶æ–‡ä»¶...';\r\n      default:\r\n        return 'ä¸‹è½½ä¸­...';\r\n    }\r\n  };\r\n\r\n  return (",
      "å–æ¶ˆ",
      "å‰©ä½™ {progress.eta}",
      "æ­£åœ¨è¿æ¥æœåŠ¡å™¨...",
      "å‡†å¤‡ä¸‹è½½ä¸­...",
      "æ­£åœ¨ä¸‹è½½è§†é¢‘ç”»é¢ (1/2)...",
      "æ­£åœ¨ä¸‹è½½éŸ³é¢‘å£°éŸ³ (2/2)...",
      "æ­£åœ¨åˆå¹¶æ–‡ä»¶...",
      "ä¸‹è½½ä¸­..."
    ],
    "src\\components\\common\\ErrorBoundary.tsx": [
      "å‡ºé”™äº†",
      "åº”ç”¨ç¨‹åºé‡åˆ°æ„å¤–é”™è¯¯ã€‚æˆ‘ä»¬å·²è®°å½•æ­¤é—®é¢˜ã€‚",
      "é‡æ–°åŠ è½½åº”ç”¨"
    ]
  }
}
