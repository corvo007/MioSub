import { GoogleGenAI } from "@google/genai";
import { SubtitleItem } from "@/types/subtitle";
import { AppSettings } from "@/types/settings";
import { ChunkStatus, TokenUsage } from "@/types/api";
import { GlossaryItem, GlossaryExtractionResult, GlossaryExtractionMetadata } from "@/types/glossary";
import { decodeAudioWithRetry } from "@/services/audio/decoder";
import { formatTime, timeToSeconds } from "@/services/subtitle/time";
import { SmartSegmenter } from "@/services/audio/segmenter";
import { selectChunksByDuration } from "@/services/glossary/selector";
import { extractGlossaryFromAudio } from "./glossary";
import { GlossaryState } from "./glossary-state";
import { sliceAudioBuffer } from "@/services/audio/processor";
import { transcribeAudio } from "@/services/api/openai/transcribe";
import { blobToBase64 } from "@/services/audio/converter";
import { intelligentAudioSampling } from "@/services/audio/sampler";
import { extractSpeakerProfiles, SpeakerProfile } from "./speakerProfile";
import { getSystemInstruction, getSystemInstructionWithDiarization } from "@/services/api/gemini/prompts";
import { parseGeminiResponse } from "@/services/subtitle/parser";
import { mapInParallel, Semaphore } from "@/services/utils/concurrency";
import { logger } from "@/services/utils/logger";
import { REFINEMENT_SCHEMA, REFINEMENT_WITH_DIARIZATION_SCHEMA, SAFETY_SETTINGS } from "./schemas";
import { generateContentWithRetry, formatGeminiError } from "./client";
import { translateBatch } from "./batch";

import { getEnvVariable } from "@/services/utils/env";

export const generateSubtitles = async (
    audioSource: File | AudioBuffer,
    duration: number,
    settings: AppSettings,
    onProgress?: (update: ChunkStatus) => void,
    onIntermediateResult?: (subs: SubtitleItem[]) => void,
    onGlossaryReady?: (metadata: GlossaryExtractionMetadata) => Promise<GlossaryItem[]>,
    signal?: AbortSignal
): Promise<{ subtitles: SubtitleItem[], glossaryResults?: GlossaryExtractionResult[] }> => {

    const geminiKey = getEnvVariable('GEMINI_API_KEY') || settings.geminiKey?.trim();
    const openaiKey = getEnvVariable('OPENAI_API_KEY') || settings.openaiKey?.trim();

    if (!geminiKey) throw new Error("ç¼ºå°‘ Gemini API å¯†é’¥ã€‚");
    if (!openaiKey && !settings.useLocalWhisper) throw new Error("ç¼ºå°‘ OpenAI API å¯†é’¥ã€‚");

    const ai = new GoogleGenAI({
        apiKey: geminiKey,
        httpOptions: {
            ...(settings.geminiEndpoint ? { baseUrl: settings.geminiEndpoint } : {}),
            timeout: (settings.requestTimeout || 600) * 1000 // Convert seconds to ms, default 600s if not set (UI defaults to 600)
        }
    });

    // Token Usage Tracking
    const usageReport: Record<string, { prompt: number, output: number, total: number }> = {};
    const trackUsage = (usage: TokenUsage) => {
        const model = usage.modelName;
        if (!usageReport[model]) {
            usageReport[model] = { prompt: 0, output: 0, total: 0 };
        }
        usageReport[model].prompt += usage.promptTokens;
        usageReport[model].output += usage.candidatesTokens;
        usageReport[model].total += usage.totalTokens;
    };

    // 1. Decode Audio
    onProgress?.({ id: 'decoding', total: 1, status: 'processing', message: "æ­£åœ¨è§£ç éŸ³é¢‘..." });
    let audioBuffer: AudioBuffer;
    try {
        if (audioSource instanceof AudioBuffer) {
            audioBuffer = audioSource;
            onProgress?.({ id: 'decoding', total: 1, status: 'completed', message: `ä½¿ç”¨ç¼“å­˜éŸ³é¢‘ï¼Œæ—¶é•¿: ${formatTime(audioBuffer.duration)}` });
        } else {
            audioBuffer = await decodeAudioWithRetry(audioSource);
            onProgress?.({ id: 'decoding', total: 1, status: 'completed', message: `è§£ç å®Œæˆï¼Œæ—¶é•¿: ${formatTime(audioBuffer.duration)}` });
        }
    } catch (e) {
        logger.error("Failed to decode audio", e);
        throw new Error("éŸ³é¢‘è§£ç å¤±è´¥ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„è§†é¢‘æˆ–éŸ³é¢‘æ ¼å¼ã€‚");
    }

    const totalDuration = audioBuffer.duration;
    const chunkDuration = settings.chunkDuration || 300;
    const totalChunks = Math.ceil(totalDuration / chunkDuration);

    // Prepare chunks
    const chunksParams: { index: number; start: number; end: number }[] = [];
    let vadSegments: { start: number, end: number }[] | undefined; // Cache VAD segments

    if (settings.useSmartSplit) {
        onProgress?.({ id: 'segmenting', total: 1, status: 'processing', message: "æ­£åœ¨æ™ºèƒ½åˆ†æ®µ..." });
        const segmenter = new SmartSegmenter();
        const result = await segmenter.segmentAudio(audioBuffer, chunkDuration, signal);
        logger.info("Smart Segmentation Results", { count: result.chunks.length, chunks: result.chunks });

        result.chunks.forEach((seg, i) => {
            chunksParams.push({
                index: i + 1,
                start: seg.start,
                end: seg.end
            });
        });

        // Cache VAD segments for reuse in speaker sampling
        vadSegments = result.vadSegments;
        logger.info(`Cached ${vadSegments.length} VAD segments for speaker profile extraction`);

        onProgress?.({ id: 'segmenting', total: 1, status: 'completed', message: `æ™ºèƒ½åˆ†æ®µå®Œæˆï¼Œå…± ${result.chunks.length} ä¸ªç‰‡æ®µã€‚` });
    } else {
        // Standard fixed-size chunking
        let cursor = 0;
        for (let i = 0; i < totalChunks; i++) {
            const end = Math.min(cursor + chunkDuration, totalDuration);
            chunksParams.push({
                index: i + 1,
                start: cursor,
                end: end
            });
            cursor += chunkDuration;
        }
        logger.info("Fixed Segmentation Results", { count: chunksParams.length, chunks: chunksParams });
    }


    // PIPELINE CONCURRENCY CONFIGURATION
    // We separate the "Transcription" concurrency from the "Overall Pipeline" concurrency.
    // This allows chunks to proceed to Refinement/Translation (which use Gemini)
    // even if the Transcription slot (Local Whisper) is busy or waiting.

    // 1. Overall Pipeline Concurrency (Gemini Flash limit)
    const pipelineConcurrency = settings.concurrencyFlash || 5;

    // 2. Transcription Concurrency (Local Whisper limit or Cloud limit)
    const transcriptionLimit = settings.useLocalWhisper
        ? (settings.whisperConcurrency || 1)
        : pipelineConcurrency; // For cloud whisper, we can match pipeline concurrency

    const transcriptionSemaphore = new Semaphore(transcriptionLimit);
    const refinementSemaphore = new Semaphore(pipelineConcurrency);

    logger.info(`Pipeline Config: Overall Concurrency=${pipelineConcurrency}, Transcription Limit=${transcriptionLimit}`);


    // --- GLOSSARY EXTRACTION (Parallel) ---
    let glossaryPromise: Promise<GlossaryExtractionResult[]> | null = null;
    let glossaryChunks: { index: number; start: number; end: number }[] | undefined;

    const isDebug = window.electronAPI?.isDebug;

    if (isDebug && settings.debug?.mockGemini) {
        const mockGlossary = [{
            chunkIndex: 0,
            terms: [{ term: "Mock Term", translation: "æ¨¡æ‹Ÿæœ¯è¯­", category: "Mock Category", confidence: "high" } as any],
            confidence: "high",
            source: 'chunk'
        }];
        logger.info("âš ï¸ [MOCK] Glossary Extraction ENABLED. Returning mock data:", mockGlossary);
        glossaryPromise = Promise.resolve(mockGlossary as any);
    } else if (settings.enableAutoGlossary !== false) {
        const sampleMinutes = settings.glossarySampleMinutes || 'all';
        glossaryChunks = selectChunksByDuration(chunksParams, sampleMinutes, chunkDuration);

        logger.info(`Initiating parallel glossary extraction on ${glossaryChunks.length} chunks (Limit: ${sampleMinutes} min)`);

        // Use Pro concurrency setting for glossary (Gemini 3 Pro)
        const glossaryConcurrency = settings.concurrencyPro || 2;

        onProgress?.({ id: 'glossary', total: glossaryChunks.length, status: 'processing', message: `æ­£åœ¨æå–æœ¯è¯­ (0/${glossaryChunks.length})...` });

        glossaryPromise = extractGlossaryFromAudio(
            ai,
            audioBuffer,
            glossaryChunks,
            settings.genre,
            glossaryConcurrency,
            (completed, total) => {
                onProgress?.({
                    id: 'glossary',
                    total: total,
                    status: completed === total ? 'completed' : 'processing',
                    message: completed === total ? 'æœ¯è¯­æå–å®Œæˆã€‚' : `æ­£åœ¨æå–æœ¯è¯­ (${completed}/${total})...`
                });
            },
            signal,
            trackUsage,
            (settings.requestTimeout || 600) * 1000 // Custom timeout in milliseconds
        );
    }

    // --- GLOSSARY HANDLING (Parallel to chunk processing) ---
    // Task: Extract glossary terms and wait for user confirmation
    let glossaryHandlingPromise: Promise<GlossaryItem[]>;
    let extractedGlossaryResults: GlossaryExtractionResult[] | undefined;

    if (glossaryPromise) {
        glossaryHandlingPromise = (async () => {
            let finalGlossary = settings.glossary || [];

            try {
                logger.info("Waiting for glossary extraction...");
                onProgress?.({ id: 'glossary', total: 1, status: 'processing', message: 'æ­£åœ¨æå–æœ¯è¯­...' });

                extractedGlossaryResults = await glossaryPromise;

                // Calculate metadata for UI decision making
                const totalTerms = extractedGlossaryResults.reduce((sum, r) => sum + r.terms.length, 0);
                const hasFailures = extractedGlossaryResults.some(r => r.confidence === 'low' && r.terms.length === 0);

                if (onGlossaryReady && (totalTerms > 0 || hasFailures)) {
                    logger.info("Glossary extracted, waiting for user confirmation...", {
                        totalTerms,
                        hasFailures,
                        resultsCount: extractedGlossaryResults.length,
                        results: extractedGlossaryResults.map(r => ({ idx: r.chunkIndex, terms: r.terms.length, conf: r.confidence }))
                    });
                    onProgress?.({ id: 'glossary', total: 1, status: 'processing', message: 'ç­‰å¾…ç”¨æˆ·ç¡®è®¤...' });

                    // BLOCKING CALL (User Interaction) - Pass metadata for UI
                    logger.info("Calling onGlossaryReady with metadata...");

                    const confirmationPromise = onGlossaryReady({
                        results: extractedGlossaryResults,
                        totalTerms,
                        hasFailures,
                        glossaryChunks: glossaryChunks!
                    });

                    // Wait indefinitely for user confirmation (no timeout)
                    finalGlossary = await confirmationPromise;
                    logger.info("onGlossaryReady returned.");

                    logger.info("Glossary confirmed/updated.", { count: finalGlossary.length });
                    onProgress?.({ id: 'glossary', total: 1, status: 'completed', message: 'æœ¯è¯­è¡¨å·²åº”ç”¨ã€‚' });
                } else {
                    // No callback or truly empty results (not even failures)
                    logger.info("No glossary extraction needed", { totalTerms, hasFailures });
                    onProgress?.({ id: 'glossary', total: 1, status: 'completed', message: 'æœªå‘ç°æœ¯è¯­ã€‚' });
                }
            } catch (e: any) {
                if (e.message === 'æ“ä½œå·²å–æ¶ˆ' || e.name === 'AbortError') {
                    logger.info("Glossary extraction cancelled");
                    onProgress?.({ id: 'glossary', total: 1, status: 'completed', message: 'å·²å–æ¶ˆ' });
                } else {
                    logger.warn("Glossary extraction failed or timed out", e);
                    onProgress?.({ id: 'glossary', total: 1, status: 'error', message: 'æœ¯è¯­æå–å¤±è´¥' });
                }
            }

            return finalGlossary; // Return only the glossary, not a complex object
        })();
    } else {
        // No glossary extraction configured
        glossaryHandlingPromise = Promise.resolve(settings.glossary || []);
    }

    // Wrap glossary promise with GlossaryState for non-blocking access
    const glossaryState = new GlossaryState(glossaryHandlingPromise);
    logger.info("ğŸ”„ GlossaryState created - chunks can now access glossary independently");

    // --- SPEAKER PROFILE EXTRACTION (Parallel) ---
    let speakerProfilePromise: Promise<SpeakerProfile[]> | null = null;
    if (settings.enableDiarization) {
        logger.info("Starting parallel speaker profile extraction...");
        onProgress?.({ id: 'diarization', total: 1, status: 'processing', message: 'æ­£åœ¨åˆ†æè¯´è¯äºº...' });

        speakerProfilePromise = (async () => {
            try {
                // 1. Intelligent Sampling (returns blob and duration)
                const { blob: sampledAudioBlob, duration } = await intelligentAudioSampling(
                    audioBuffer,
                    480, // 8 minutes for comprehensive speaker coverage
                    8,
                    signal,
                    vadSegments // Pass cached VAD segments to avoid re-running VAD
                );

                // 2. Extract Profiles
                const profileSet = await extractSpeakerProfiles(
                    ai,
                    sampledAudioBlob,
                    duration,
                    settings.genre,
                    (settings.requestTimeout || 600) * 1000, // Use configured timeout
                    trackUsage,
                    signal
                );

                logger.info(`Extracted ${profileSet.profiles.length} speaker profiles`, profileSet.profiles);
                onProgress?.({ id: 'diarization', total: 1, status: 'completed', message: `å·²è¯†åˆ« ${profileSet.profiles.length} ä½è¯´è¯äºº` });

                // Swap ID with Name if available, so the AI uses the name in the output
                return profileSet.profiles.map(p => ({
                    ...p,
                    id: p.characteristics.name || p.id
                }));
            } catch (e) {
                logger.error("Speaker profile extraction failed", e);
                onProgress?.({ id: 'diarization', total: 1, status: 'error', message: 'è¯´è¯äººåˆ†æå¤±è´¥' });
                return [];
            }
        })();
    }

    // --- UNIFIED PARALLEL PIPELINE: Transcription â†’ Wait for Glossary/Profiles â†’ Refine & Translate ---
    // Each chunk proceeds independently without waiting for others
    logger.info("Starting Unified Pipeline: Each chunk will proceed independently");

    const chunkResults: SubtitleItem[][] = new Array(totalChunks).fill([]);

    // Use a high concurrency limit for the main loop (buffer)
    // The actual resource usage is controlled by semaphores inside
    // We use totalChunks to ensure all chunks can enter the "waiting room" (semaphore queue)
    // preventing the pipeline from stalling due to loop limits.
    const mainLoopConcurrency = Math.max(totalChunks, pipelineConcurrency, 20);

    await mapInParallel(chunksParams, mainLoopConcurrency, async (chunk, i) => {
        const { index, start, end } = chunk;

        try {
            // ===== STEP 1: TRANSCRIPTION =====
            onProgress?.({ id: index, total: totalChunks, status: 'processing', stage: 'transcribing', message: 'ç­‰å¾…è½¬å½•...' });

            let rawSegments: SubtitleItem[] = [];

            // Acquire Transcription Semaphore
            await transcriptionSemaphore.acquire();
            try {
                if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');

                onProgress?.({ id: index, total: totalChunks, status: 'processing', stage: 'transcribing', message: 'æ­£åœ¨è½¬å½•...' });
                logger.debug(`[Chunk ${index}] Starting transcription...`);

                const shouldMockTranscription = isDebug && (settings.useLocalWhisper
                    ? settings.debug?.mockLocalWhisper
                    : settings.debug?.mockOpenAI);

                if (shouldMockTranscription) {
                    const mockTranscription = [{
                        id: 0,
                        startTime: "00:00:00,000",
                        endTime: formatTime(end - start),
                        original: `[Mock] Transcription for Chunk ${index}`,
                        translated: ""
                    }];
                    logger.info(`âš ï¸ [MOCK] Transcription ENABLED for Chunk ${index}. Returning mock data:`, mockTranscription);
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    rawSegments = mockTranscription;
                } else {
                    const wavBlob = await sliceAudioBuffer(audioBuffer, start, end);
                    rawSegments = await transcribeAudio(
                        wavBlob,
                        openaiKey,
                        settings.transcriptionModel,
                        settings.openaiEndpoint,
                        (settings.requestTimeout || 600) * 1000,
                        settings.useLocalWhisper,
                        settings.whisperModelPath,
                        settings.whisperThreads,
                        signal,
                        settings.debug?.whisperPath
                    );
                }
            } finally {
                transcriptionSemaphore.release();
            }

            logger.debug(`[Chunk ${index}] Transcription complete. Segments: ${rawSegments.length}`);

            // Skip if no segments
            if (rawSegments.length === 0) {
                logger.warn(`[Chunk ${index}] No speech detected, skipping`);
                chunkResults[i] = [];
                onProgress?.({ id: index, total: totalChunks, status: 'completed', message: 'å®Œæˆï¼ˆæ— å†…å®¹ï¼‰' });
                return;
            }

            // ===== STEP 2: WAIT FOR GLOSSARY (Non-blocking for other chunks) =====
            onProgress?.({ id: index, total: totalChunks, status: 'processing', stage: 'waiting_glossary', message: 'ç­‰å¾…æœ¯è¯­è¡¨...' });
            logger.debug(`[Chunk ${index}] Waiting for glossary confirmation...`);

            if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');

            const finalGlossary = await glossaryState.get();

            if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');

            const chunkSettings = { ...settings, glossary: finalGlossary };

            logger.debug(`[Chunk ${index}] Glossary ready (${finalGlossary.length} terms), proceeding to refinement`);

            // Wait for speaker profiles if diarization is enabled (Before acquiring semaphore)
            let speakerProfiles: SpeakerProfile[] | undefined;
            if (speakerProfilePromise) {
                onProgress?.({ id: index, total: totalChunks, status: 'processing', stage: 'waiting_speakers', message: 'ç­‰å¾…è¯´è¯äººåˆ†æ...' });
                try {
                    speakerProfiles = await speakerProfilePromise;
                } catch (e) {
                    logger.warn("Failed to get speaker profiles, proceeding without them", e);
                }
            }

            // ===== STEP 3: REFINEMENT =====
            // Acquire Refinement Semaphore (Gemini API limit)
            await refinementSemaphore.acquire();
            try {
                if (signal?.aborted) throw new Error('æ“ä½œå·²å–æ¶ˆ');

                // Re-slice audio for Gemini (Refine needs audio)
                const refineWavBlob = await sliceAudioBuffer(audioBuffer, start, end);
                const base64Audio = await blobToBase64(refineWavBlob);

                let refinedSegments: SubtitleItem[] = [];
                onProgress?.({ id: index, total: totalChunks, status: 'processing', stage: 'refining', message: 'æ­£åœ¨æ ¡å¯¹æ—¶é—´è½´...' });

                const refineSystemInstruction = getSystemInstructionWithDiarization(
                    chunkSettings.genre,
                    undefined,
                    'refinement',
                    chunkSettings.glossary,
                    chunkSettings.enableDiarization,
                    speakerProfiles
                );
                // For refinement, only show original terms (without translations) to prevent language mixing
                const glossaryInfo = chunkSettings.glossary && chunkSettings.glossary.length > 0
                    ? `\n\nKEY TERMINOLOGY (Listen for these terms in the audio and transcribe them accurately in the ORIGINAL LANGUAGE):\n${chunkSettings.glossary.map(g => `- ${g.term}${g.notes ? ` (${g.notes})` : ''}`).join('\n')}`
                    : '';

                const refinePrompt = `
            TRANSCRIPTION REFINEMENT TASK
            Context: ${chunkSettings.genre}

            TASK: Refine the raw OpenAI Whisper transcription by listening to the audio and correcting errors.

            RULES (Priority Order):

            [P1 - ACCURACY] Audio-Based Correction
            â†’ Listen carefully to the attached audio
            â†’ Fix misrecognized words and phrases in 'text'
            â†’ Verify timing accuracy of 'start' and 'end' timestamps
            ${glossaryInfo ? `â†’ Pay special attention to key terminology listed below` : ''}

            [P2 - READABILITY] Segment Splitting
            â†’ SPLIT any segment longer than 4 seconds OR >25 characters
            â†’ When splitting: distribute timing based on actual audio speech
            â†’ Ensure splits occur at natural speech breaks
            
            [P3 - CLEANING] Remove Non-Speech Elements
            â†’ Remove filler words (uh, um, å‘ƒ, å—¯, etc.)
            â†’ Remove stuttering and false starts
            â†’ Keep natural speech flow

            [P4 - OUTPUT] Format Requirements
            â†’ Return timestamps in HH:MM:SS,mmm format
            â†’ Timestamps must be relative to the provided audio (starting at 00:00:00,000)
            â†’ Ensure all required fields are present
            ${chunkSettings.enableDiarization ? `â†’ INCLUDE "speaker" field for every segment (e.g., "Speaker 1")` : ''}

            FINAL VERIFICATION:
            âœ“ Long segments (>4s or >25 chars) properly split
            âœ“ Timestamps are relative to chunk start
            âœ“ Terminology from glossary is used correctly
            ${glossaryInfo ? `âœ“ Checked against ${chunkSettings.glossary?.length} glossary terms` : ''}

            Input Transcription (JSON):
            ${JSON.stringify(rawSegments.map(s => ({ start: s.startTime, end: s.endTime, text: s.original })))}
            `;

                try {
                    if (isDebug && settings.debug?.mockGemini) {
                        logger.info(`âš ï¸ [MOCK] Refinement ENABLED for Chunk ${index}. Returning raw segments as refined.`);
                        await new Promise(resolve => setTimeout(resolve, 500));
                        refinedSegments = [...rawSegments];
                    } else {
                        const refineResponse = await generateContentWithRetry(ai, {
                            model: 'gemini-2.5-flash',
                            contents: {
                                parts: [
                                    { inlineData: { mimeType: "audio/wav", data: base64Audio } },
                                    { text: refinePrompt }
                                ]
                            },
                            config: {
                                responseMimeType: "application/json",
                                responseSchema: chunkSettings.enableDiarization ? REFINEMENT_WITH_DIARIZATION_SCHEMA : REFINEMENT_SCHEMA,
                                systemInstruction: refineSystemInstruction,
                                safetySettings: SAFETY_SETTINGS,
                                maxOutputTokens: 65536,
                            }
                        }, 3, signal, trackUsage, (settings.requestTimeout || 600) * 1000);

                        refinedSegments = parseGeminiResponse(refineResponse.text, chunkDuration);
                    }

                    if (refinedSegments.length === 0) {
                        refinedSegments = [...rawSegments];
                    }
                    logger.debug(`[Chunk ${index}] Refinement complete. Segments: ${refinedSegments.length}`);
                    if (refinedSegments.length > 0 && chunkSettings.enableDiarization) {
                        logger.debug(`[Chunk ${index}] Refinement first segment speaker: ${refinedSegments[0].speaker}`);
                    }
                } catch (e) {
                    logger.error(`åˆ†æ®µ ${index} æ—¶é—´è½´å¤±è´¥ï¼Œå°†å›é€€åˆ°åŸå§‹ç»“æœã€‚`, formatGeminiError(e));
                    refinedSegments = [...rawSegments];
                }

                // ===== STEP 4: TRANSLATION =====
                let finalChunkSubs: SubtitleItem[] = [];
                if (refinedSegments.length > 0) {
                    onProgress?.({ id: index, total: totalChunks, status: 'processing', stage: 'translating', message: 'æ­£åœ¨ç¿»è¯‘...' });

                    const toTranslate = refinedSegments.map((seg, idx) => ({
                        id: idx + 1,
                        original: seg.original,
                        start: seg.startTime,
                        end: seg.endTime,
                        speaker: seg.speaker
                    }));

                    const translateSystemInstruction = getSystemInstruction(chunkSettings.genre, chunkSettings.customTranslationPrompt, 'translation', chunkSettings.glossary);

                    let translatedItems: any[] = [];
                    if (isDebug && settings.debug?.mockGemini) {
                        logger.info(`âš ï¸ [MOCK] Translation ENABLED for Chunk ${index}. Generating mock translations.`);
                        await new Promise(resolve => setTimeout(resolve, 500));
                        translatedItems = toTranslate.map(t => ({
                            ...t,
                            translated: `[Mock] Translated: ${t.original}`
                        }));
                        logger.info(`âš ï¸ [MOCK] Translation Result for Chunk ${index}:`, translatedItems);
                    } else {
                        translatedItems = await translateBatch(
                            ai,
                            toTranslate,
                            translateSystemInstruction,
                            1, // Internal concurrency (we're already in refinementSemaphore)
                            chunkSettings.translationBatchSize || 20,
                            (update) => onProgress?.({ id: index, total: totalChunks, status: 'processing', stage: 'translating', ...update }),
                            signal,
                            trackUsage,
                            (settings.requestTimeout || 600) * 1000 // Custom timeout in milliseconds
                        );
                    }
                    logger.debug(`[Chunk ${index}] Translation complete. Items: ${translatedItems.length}`);
                    if (translatedItems.length > 0 && chunkSettings.enableDiarization) {
                        logger.debug(`[Chunk ${index}] Translation first segment speaker: ${translatedItems[0].speaker}`);
                    }

                    finalChunkSubs = translatedItems.map(item => ({
                        id: 0, // Placeholder, will re-index later
                        startTime: formatTime(timeToSeconds(item.start) + start),
                        endTime: formatTime(timeToSeconds(item.end) + start),
                        original: item.original,
                        translated: item.translated,
                        speaker: item.speaker
                    }));
                }

                chunkResults[i] = finalChunkSubs;

                // Update Intermediate Result
                const currentAll = chunkResults.flat().map((s, idx) => ({ ...s, id: idx + 1 }));
                onIntermediateResult?.(currentAll);

                onProgress?.({ id: index, total: totalChunks, status: 'completed', message: 'å®Œæˆ' });

            } finally {
                refinementSemaphore.release();
            }

        } catch (e) {
            logger.error(`Chunk ${index} failed`, e);
            onProgress?.({ id: index, total: totalChunks, status: 'error', message: 'å¤±è´¥' });
        }
    });

    const finalSubtitles = chunkResults.flat().map((s, idx) => ({ ...s, id: idx + 1 }));

    // Log Token Usage Report
    let reportLog = "\nğŸ“Š Token Usage Report:\n----------------------------------------\n";
    let grandTotal = 0;
    for (const [model, usage] of Object.entries(usageReport)) {
        reportLog += `Model: ${model}\n`;
        reportLog += `  - Prompt Tokens: ${usage.prompt.toLocaleString()}\n`;
        reportLog += `  - Output Tokens: ${usage.output.toLocaleString()}\n`;
        reportLog += `  - Total: ${usage.total.toLocaleString()}\n`;
        reportLog += `----------------------------------------\n`;
        grandTotal += usage.total;
    }
    reportLog += `Grand Total: ${grandTotal.toLocaleString()}\n`;
    logger.info(reportLog);

    return { subtitles: finalSubtitles, glossaryResults: extractedGlossaryResults };
};
